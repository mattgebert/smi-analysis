{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMI Data Reduction for Grazing-Incidence Angle-of-Incidence Scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "##### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Prevent numpy from using multiple threads when using emcee multiprocessing.\n",
    "# These packages should all be installed if the procedure was followed\n",
    "%matplotlib widget\n",
    "import matplotlib, matplotlib.pyplot as plt, matplotlib.colors as mplc, matplotlib.cm as cm\n",
    "matplotlib.interactive(True)\n",
    "plt.ion()\n",
    "from smi_analysis import SMI_beamline\n",
    "import numpy as np, numpy.typing as npt\n",
    "import pandas as pd\n",
    "import fabio\n",
    "import logging\n",
    "import scipy.constants as const\n",
    "import time\n",
    "import corner\n",
    "from typing import Literal, Callable\n",
    "import emcee\n",
    "import tqdm, tqdm.notebook # For progress bars\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "\n",
    "# Setup options\n",
    "fabio.TiffIO.logger.setLevel(logging.ERROR)\n",
    "pd.set_option(\"display.width\", 1000) #display large filenames\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "en2wav = lambda en: const.h * const.c / (const.e * en)\n",
    "\"\"\"Function to convert energy (eV) to wavelength\"\"\"\n",
    "en2wav(2.45e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimental configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry: Literal['Transmission'] | Literal['Reflection'] = 'Reflection'\n",
    "\"\"\"The measurement geometry\"\"\"\n",
    "energy: float = 2.45e3\n",
    "\"\"\"The energy (keV) at which the scan is performed\"\"\"\n",
    "wavelength: float = en2wav(energy)\n",
    "\"\"\"The wavelength corresponding to `energy`.\"\"\"\n",
    "beamstop_type: Literal[\"pindiode\"] | Literal['rod'] = 'pindiode'\n",
    "\"\"\"The beamstop type\"\"\"\n",
    "incident_angle = np.deg2rad(0)\n",
    "\"\"\"The default incident angle (varies)\"\"\"\n",
    "\n",
    "#WAXS\n",
    "detector_waxs: Literal['Pilatus900kw'] | Literal['Pilatus1m'] = 'Pilatus900kw'\n",
    "\"\"\"Type of WAXS/SAXS detector\"\"\"\n",
    "sdd_waxs: float | int = 280 # In mm\n",
    "\"\"\"Sample to detector distance in millimeters\"\"\"\n",
    "center_waxs: tuple[int|float, int|float] = [97, 1255.9]\n",
    "\"\"\"Coordinates of the beam centre at 0 degrees, for the middle detector strip\"\"\"\n",
    "bs_pos_waxs: list[tuple[int, int]] = [[(195+17) + 98, 1187]] #, [0, 0], [0, 0]\n",
    "\"\"\"The position of the center of the beam stop for each detector angle; [0,0] implies not measured. \n",
    "This coordinate is relative to the stitch of the 3 detector strips.\"\"\"\n",
    "detector_angles: list[int | float] | npt.NDArray[np.float64 | np.int_] = np.deg2rad(np.array([0]) - 0.06) #0.06 is the correction for the WAXS 0 deg detector position\n",
    "\"\"\"The angles of the detector in radians. \n",
    "May need to include corrections (-0.06 degs at 0, -0.36 at 20 deg) for position offsets.\"\"\"\n",
    "\n",
    "\n",
    "display(pd.DataFrame([\n",
    "    (\"Geometry\", geometry),\n",
    "    (\"Energy (keV)\", energy),\n",
    "    (\"Wavelength (nm)\", wavelength * 1e9),\n",
    "    (\"Sample to Detector Distance (mm)\", sdd_waxs),\n",
    "    (\"Beamstop Type\", beamstop_type),\n",
    "    (\"Incident Angle (deg)\", np.rad2deg(incident_angle)),\n",
    "    (\"Detector Type\", detector_waxs),\n",
    "    (\"Center Coords\", center_waxs),\n",
    "    (\"Beamstop Coords\", bs_pos_waxs),\n",
    "    (\"Detector Angles\", detector_angles)\n",
    "], columns=[\"Parameter\", \"Value\"]))\n",
    "\n",
    "#Test the configuration can be loaded!\n",
    "SMI_waxs = SMI_beamline.SMI_geometry(geometry = geometry,\n",
    "                                     wav = wavelength,\n",
    "                                     sdd = sdd_waxs,\n",
    "                                     alphai = incident_angle,\n",
    "                                     detector = detector_waxs,\n",
    "                                     center = center_waxs,\n",
    "                                     bs_pos = bs_pos_waxs,\n",
    "                                     bs_kind = beamstop_type,\n",
    "                                     det_angles=detector_angles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatfield Data\n",
    "Data to normalise the detector pixels and remove background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Past beamline data for flat fielding (normalizing default pixel intensities)\n",
    "# Note this is done at 2478eV, not all energies.\n",
    "\n",
    "# # 2024 Cycle 2 Flatfielding\n",
    "# CYCLE_FLAT = '2024_3'\n",
    "# PROPOSAL_FLAT= '314483-Freychet-Flatfielding'\n",
    "# FLAT_FILE = 'GF_flatfield_Sedge_2450uhighg1600_WZY11_wa30deg_2478eV_20s_id701601_000000_WAXS.tif'\n",
    "\n",
    "# 2024 Cycle 3 Flatfielding\n",
    "PROPOSAL_FLAT= '314483_Freychet_08'\n",
    "FLAT_FILE = 'GF_GF_flatfield_Sedge_2450uhighg1600_Y2_06_2477.00eV_wa20deg_id807229_000000_WAXS.tif'\n",
    "\n",
    "# Compile and load the flatfield path\n",
    "FLAT_DIR_PATH = f'D:/Datasets/2024-09 SMI/{PROPOSAL_FLAT}/900KW/'\n",
    "flatfield: npt.NDArray = np.rot90(fabio.open(os.path.join(FLAT_DIR_PATH, FLAT_FILE)).data, 1)\n",
    "\n",
    "fig,ax = plt.subplots(2,1, sharex=True, sharey=True, figsize=(8,5))\n",
    "p = 99.9\n",
    "percentile = np.percentile(flatfield, p) #99.9th percentile\n",
    "ax[0].imshow(np.rot90(flatfield, 3), vmin=0, vmax=percentile, interpolation=None)\n",
    "ax[0].set_title(\"Flatfielding Data\")\n",
    "\n",
    "erronous =  (flatfield > percentile) * 1.0\n",
    "ax[1].imshow(np.rot90(erronous,3), vmin=0, vmax=np.max(erronous)/5, interpolation=None)\n",
    "ax[1].set_title(f\"{p}th percentile pixels\")\n",
    "fig.tight_layout()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra functions for SMI Beamline Masking\n",
    "\n",
    "##### Detector Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_detector_mask_to_array(\n",
    "    mask: npt.NDArray = np.zeros((1475, 195), dtype=bool)\n",
    ") -> npt.NDArray[np.bool]:\n",
    "    \"\"\"Sets an array mask for bad pixels; should only be applied to the middle column array\"\"\"\n",
    "    mask[1254:1256, 47] = True\n",
    "    mask[979:1050, 0:100] = True\n",
    "    mask[967, 67] = True\n",
    "    mask[490:555, 100:] = True\n",
    "    mask[1231:1233, 174] = True\n",
    "    mask[1414: 1416, 179] = True\n",
    "    mask[858:860, 5] = True\n",
    "    mask[414, 6] = True\n",
    "    mask[394, 138] = True\n",
    "    mask[364:366, 41] = True\n",
    "    mask[364:366, 96] = True\n",
    "    mask[304:306, 96:98] =  True\n",
    "    mask[988, 188:194] = True\n",
    "    mask[:, 1] = True\n",
    "    mask[473, 20] = True\n",
    "    mask[98, 5] = True\n",
    "    mask[141, 111] = True\n",
    "    mask[240:300, 0:50] = True\n",
    "    mask[300:425, 125:] = True\n",
    "    mask[181:183, 97:99] = True\n",
    "    mask[553:555, 99:100] = True\n",
    "    return mask\n",
    "\n",
    "def apply_boundary_mask_to_array(\n",
    "    mask: npt.NDArray = np.zeros((1475, 195), dtype=bool)\n",
    ") -> npt.NDArray[np.bool]:\n",
    "    \"\"\"Sets an array mask for the boundary pixels; should only be applied to the middle column array\"\"\"\n",
    "    mask[0, :] = True\n",
    "    mask[-1, :] = True\n",
    "    mask[:, 0] = True\n",
    "    mask[:, -1] = True\n",
    "    return mask\n",
    "\n",
    "def apply_detector_mask(geom: SMI_beamline.SMI_geometry) -> None:\n",
    "    \"\"\"Applies a pre-defined mask for the bad pixels in the SMI beamline\"\"\"\n",
    "    for i, mask in enumerate(geom.masks):\n",
    "        # Dead pixels in the 2nd detector strip.\n",
    "        if i%3 == 1: # For multiple WAXS images, always masks the 2nd strip.\n",
    "            apply_detector_mask_to_array(mask)\n",
    "        # Add a mask on the boundary\n",
    "        apply_boundary_mask_to_array(mask)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flatield Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLATFIELD_PERCENTILE = 99.5\n",
    "\"\"\"The percentile of the flatfield data to mask.\"\"\"\n",
    "SAMPLE_PERCENTILE = 99.99\n",
    "\"\"\"The percentile of the real data to mask.\"\"\"\n",
    "\n",
    "# For flatfielding, ignore/mask reigons between detector pixels.\n",
    "FLATFIELD_SLICES = [slice(0, 195),  # Ign. flatfield above first frame\n",
    "                    slice(211, 406),# Ign. flatfield outside middle  \n",
    "                    slice(-195, None)] # Ign. flatfield below first frame\n",
    "            \n",
    "def flatfield_mask(flatfield: npt.NDArray = flatfield, \n",
    "                   percentile: float = FLATFIELD_PERCENTILE, \n",
    "                   min: float | int = 0) -> npt.NDArray[np.bool]:\n",
    "        \"\"\"\n",
    "        Returns a mask of flatfield data as a boolean numpy array.\n",
    "        \n",
    "        Masks pixels above the `percentile` (by default 99.9)\n",
    "        and values less than `min` (by default 1).\n",
    "        \"\"\"\n",
    "        # Calculate the 99.9th percentile of the total flatfield data\n",
    "        p = np.percentile(flatfield, percentile) #99.9th percentile\n",
    "        erronous =  flatfield > p\n",
    "        # Also mask negative and zero pixels\n",
    "        negative = flatfield < min\n",
    "        # Return the overlap of the erronous and negative masks.\n",
    "        return erronous | negative\n",
    "\n",
    "def apply_flatfield(geom: SMI_beamline.SMI_geometry, \n",
    "                    flatfield: npt.NDArray, \n",
    "                    flat_percentile: float | int = FLATFIELD_PERCENTILE,\n",
    "                    img_percentile: float | int = SAMPLE_PERCENTILE,\n",
    "                    min : float | int = 1) -> None:\n",
    "    \"\"\"Applies a pre-defined flatfield mask and normalisation for the SMI beamline object\"\"\"\n",
    "    flatmask = flatfield_mask(flatfield=flatfield, percentile=flat_percentile, min = min)\n",
    "    for i, (mask, img) in enumerate(zip(geom.masks, geom.imgs)):\n",
    "        fmask_i = flatmask[:, FLATFIELD_SLICES[i % 3]]\n",
    "        # Apply the masking values\n",
    "        masking_values = np.where(fmask_i == True)\n",
    "        mask[masking_values] = True\n",
    "        # Apply the normalisation \n",
    "        flat = flatfield[:, FLATFIELD_SLICES[i%3]] # Get flatfield panel\n",
    "        flat[flat < 0] = 10000 # avoid negative values - note these are already masked\n",
    "        # Multiply img by max of flatfield, then divide by flatfield to keep integer precision of img.\n",
    "        temp = ((img * np.max(flatfield[:, FLATFIELD_SLICES[i%3]] * ~mask))\n",
    "                / (flat * ~mask +1)) * ~mask # Avoid divide by zero, and zero mask values.\n",
    "        # This line creates a runtime error due to casting... img is int32, flat is float64\n",
    "        img[:] = temp.astype(np.int32)\n",
    "    \n",
    "        # Repeat mask for very large values erronously normalised\n",
    "        mask2 = flatfield_mask(flatfield=img, percentile=img_percentile, min = 0) # only consider positive values.\n",
    "        masking_values2 = np.where(mask2 == True)\n",
    "        mask[masking_values2] = True\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the stages of Flatfield masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, figsize=(16,6), dpi=50, sharex=True, sharey=True)\n",
    "\n",
    "# Detector image is (619, 1475) #619 is 195 * 3 + 2 * 17\n",
    "# Show the detector masking\n",
    "grid = np.zeros((195, 1475), dtype=bool)\n",
    "\n",
    "\n",
    "det_mask = np.c_[apply_boundary_mask_to_array(grid.copy().T),\n",
    "                 np.zeros((17, 1475), dtype=bool).T,\n",
    "                 apply_detector_mask_to_array(apply_boundary_mask_to_array(grid.copy().T)), # 195 pixels\n",
    "                 np.zeros((17, 1475), dtype=bool).T,\n",
    "                apply_boundary_mask_to_array(grid.copy().T)]\n",
    "    \n",
    "\n",
    "det_im = ax[0][0].imshow(np.rot90(det_mask,3), interpolation='nearest') # required to prevent interpolation\n",
    "ax[0][0].set_title(\"Pre-defined Detector Mask (boolean)\")\n",
    "\n",
    "# Show the flatfield masking\n",
    "ff_masked = flatfield_mask(flatfield)\n",
    "ff_masked_im = ax[0][1].imshow(np.rot90(ff_masked,3), interpolation='nearest') # required to prevent interpolation\n",
    "ax[0][1].set_title(\"Calculated Flatfield Mask (boolean)\")\n",
    "\n",
    "# Show the flatfield image\n",
    "ff_im = ax[1][0].imshow(np.rot90(flatfield,3), vmin = 0, vmax = np.percentile(flatfield, FLATFIELD_PERCENTILE))\n",
    "plt.colorbar(ff_im)\n",
    "ax[1][0].set_title(\"Flatfield Raw Data\")\n",
    "\n",
    "# Show the masks on the flatfield image\n",
    "joined_mask = ff_masked | det_mask\n",
    "cmap = mplc.LinearSegmentedColormap.from_list(\"Mask\", [(256,0,0,0),(256,0,0,256)], N=2)\n",
    "ff_im_masked = ax[1][1].imshow(np.rot90(flatfield,3), interpolation='nearest',\n",
    "                               vmin = 0, vmax = np.percentile(flatfield, FLATFIELD_PERCENTILE))\n",
    "ax[1][1].imshow(np.rot90(joined_mask,3), cmap=cmap, interpolation='nearest')\n",
    "ax[1][1].set_title(\"Flatfield Data Masked\")\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "\n",
    "##### Locate the files on your computer and define the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CYCLE: str = '2024_3' #YYYY_[1-3]\n",
    "PROPOSAL_ID = '316022_McNeil_04' #PPPPPP_[Name]_[#]\n",
    "## ----------- Path to the raw data -----------\n",
    "\n",
    "# RAW_PATH = f'D:/Datasets/2024-09 SMI/{CYCLE}/{PROPOSAL_ID}/900KW/'\n",
    "RAW_DIR = f'D:/Datasets/2024-09 SMI/{PROPOSAL_ID}/900KW/'\n",
    "display(pd.DataFrame(os.listdir(RAW_DIR), columns=[\"Filename\"])) #use tail or head to display a subset\n",
    "\n",
    "## ----------- Create/select the results directory -----------\n",
    "RESULT_DIR = f'D:/Datasets/2024-09 SMI/{PROPOSAL_ID}/angle_scan_results/'\n",
    "created = False\n",
    "for i in range(len(RESULT_DIR.split(\"/\"))):\n",
    "    if not os.path.isdir(\"/\".join(RESULT_DIR.split(\"/\")[:i+1])) and not (i==0 and RESULT_DIR[0]==\"/\"):\n",
    "        os.mkdir(\"/\".join(RESULT_DIR.split(\"/\")[:i+1]))\n",
    "        created = True\n",
    "if not created:\n",
    "    print(\"Results path exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Organise files into sample names and data for each detector angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_flags: list[str] = ['wide', '2450', \"A1\"]\n",
    "\"\"\"The strings you want included in the files processed.\"\"\"\n",
    "\n",
    "# Find all samples\n",
    "samples: list[str] = []\n",
    "\"\"\"String names of the unique samples matching patterns in `filename_flags`\"\"\"\n",
    "for file in sorted(os.listdir(RAW_DIR)):\n",
    "     # Define the flags for the files you want to process, by filtering the filename.\n",
    "     if all([flag in file for flag in filename_flags]):\n",
    "        # Find the angle of incidence:\n",
    "        idx = file.find('_ai')\n",
    "        sample_substring = file[:idx+1]\n",
    "        # If sample substring not in list of samples, add it!\n",
    "        if sample_substring not in samples:\n",
    "            samples.append(sample_substring)\n",
    "\n",
    "# Find all TIF image measurements\n",
    "datasets: list[list[str]] = [[] for _ in samples]\n",
    "count = 0\n",
    "\"\"\"A list for each sample, corresponding to the raw data filenames. \n",
    "Each sample list will be an index of anles of incidence.\"\"\"\n",
    "for i, sample in enumerate(samples):\n",
    "        for j, file in enumerate(sorted(os.listdir(RAW_DIR))):\n",
    "                if all([flag in file for flag in filename_flags + [sample, '.tif']]):\n",
    "                        datasets[i].append(file)\n",
    "\n",
    "# Display the number of files for each sample:\n",
    "display(\n",
    "      pd.DataFrame([\n",
    "            (sample, len(datasets[i]))\n",
    "            for i, sample in enumerate(samples)\n",
    "      ], columns=[\"Sample Name\", \"Number of Files\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction\n",
    "### Run the first sample to check everything is working and define the Q fit ranges\n",
    "\n",
    "##### First check sample image exists and looks reasonable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files: list[int] = [0, 40, 80, -2]\n",
    "\"\"\"The index of each sample's test file\"\"\"\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for test_file in test_files:\n",
    "        print(sample, \"|\", datasets[i][test_file].replace(sample, \"\"))\n",
    "        fig,ax = plt.subplots(1,2, figsize=(8,1.5), sharex=True, sharey=True)\n",
    "        fname = datasets[i][test_file]\n",
    "        # fig.suptitle(sample)\n",
    "        # Use \n",
    "        img=fabio.open(os.path.join(RAW_DIR, fname)).data\n",
    "        mappable = ax[0].imshow(img, vmin=0, vmax=np.percentile(img,99))\n",
    "        ax[0].set_title(\"Raw Data\")\n",
    "        plt.colorbar(mappable)\n",
    "        \n",
    "        # Show the masks on the flatfield image\n",
    "        cmap = mplc.LinearSegmentedColormap.from_list(\"Mask\", [(256,0,0,0),(256,0,0,256)], N=2)\n",
    "        ff_im_masked = ax[1].imshow(img, interpolation='nearest',\n",
    "                                vmin = 0, vmax = np.percentile(img, 99))\n",
    "        ax[1].imshow(np.rot90(joined_mask,3), cmap=cmap, interpolation='nearest')\n",
    "        ax[1].set_title(\"Masked by flatfield and detector\")\n",
    "        plt.colorbar(ff_im_masked)\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import data via SMI beamline and observe the application of internal masking and flatfielding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Tried digging into SMI_beamline > SMI_geometry > stitching_data > stitch.stitching, to understand how the mask is stitched. \n",
    "## Too difficult to follow, for some reason mask is not really applied well in the first sector of the detector.\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for test_file in test_files:\n",
    "        # Setup a figure and open the file\n",
    "        fig,ax = plt.subplots(1,3, figsize=(12,2.5), sharex=True, sharey=True)\n",
    "        fname = datasets[i][test_file]\n",
    "        \n",
    "        # Collect the metadata\n",
    "        en_idx = fname.find('eV_')\n",
    "        en = float(fname[en_idx-7:en_idx])\n",
    "        ai_idx = fname.find(\"_ai\")\n",
    "        ai = float(fname[ai_idx+3:ai_idx+8])\n",
    "        display(pd.DataFrame([\n",
    "                (fname, en, ai)\n",
    "                ], columns=[\"Filename\", \"Energy (eV)\", \"Incident Angle (deg)\"]))\n",
    "        \n",
    "        # Update the geometry\n",
    "        SMI_waxs.alphai = np.deg2rad(ai)\n",
    "        SMI_waxs.wav = en2wav(en)\n",
    "        \n",
    "        # Reset the masks\n",
    "        for mask in SMI_waxs.masks:\n",
    "            mask[:,:] = False\n",
    "        \n",
    "        # Plot the unmodified data\n",
    "        SMI_waxs.open_data(RAW_DIR, [fname])\n",
    "        SMI_waxs.stitching_data(interp_factor=2, flag_scale=False)\n",
    "        # mp = ax[0].imshow(SMI_waxs.img_st,\n",
    "        #         extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "        mp = ax[0].imshow(np.rot90(SMI_waxs.img_st.T, 2),\n",
    "                extent=[SMI_waxs.qz[0], SMI_waxs.qz[-1], SMI_waxs.qp[0], SMI_waxs.qp[-1]], \n",
    "                # vmin=0,\n",
    "                vmax=np.percentile(SMI_waxs.img_st, 99)\n",
    "        )\n",
    "        plt.colorbar(mp)\n",
    "        ax[0].set_title(\"Stitched Data (Raw)\")\n",
    "        \n",
    "        # Plot the flatfield / masked normalized data\n",
    "        SMI_waxs.open_data(RAW_DIR, [fname])\n",
    "        apply_detector_mask(SMI_waxs)\n",
    "        apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "        apply_detector_mask(SMI_waxs)\n",
    "        SMI_waxs.stitching_data(interp_factor=1, flag_scale=False)\n",
    "        # mp = ax[1].imshow(SMI_waxs.img_st,\n",
    "                # extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "        mp = ax[1].imshow(np.rot90(SMI_waxs.img_st.T,2),\n",
    "                extent=[SMI_waxs.qz[0], SMI_waxs.qz[-1], SMI_waxs.qp[0], SMI_waxs.qp[-1]], \n",
    "                # vmin=np.percentile(SMI_waxs.img_st, 1.0), \n",
    "                vmax=np.percentile(SMI_waxs.img_st, 99.0)\n",
    "        )\n",
    "        plt.colorbar(mp)\n",
    "        ax[1].set_title(\"Stitched (Norm. + Custom Masked)\")\n",
    "        \n",
    "        # Show the total mask over the image\n",
    "        cmap = mplc.LinearSegmentedColormap.from_list(\"Mask\", [(256,0,0,0),(256,0,0,256)], N=2)\n",
    "        mp = ax[2].imshow(np.rot90(SMI_waxs.img_st.T,2),\n",
    "                extent=[SMI_waxs.qz[0], SMI_waxs.qz[-1], SMI_waxs.qp[0], SMI_waxs.qp[-1]], \n",
    "                # vmin=np.percentile(SMI_waxs.img_st, 1.0), \n",
    "                vmax=np.percentile(SMI_waxs.img_st, 99.0)\n",
    "        )\n",
    "        ax[2].imshow(~np.rot90(SMI_waxs.mask_st != 0, 1),\n",
    "                extent=[SMI_waxs.qz[0], SMI_waxs.qz[-1], SMI_waxs.qp[0], SMI_waxs.qp[-1]],\n",
    "                cmap=cmap\n",
    "        )\n",
    "        plt.colorbar(mp)\n",
    "        ax[2].set_title(\"Detector mask + custom mask\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate a ROI to check the beam-centre \n",
    "###### Also check that the beamcentre is correct by symmetry (unless sample has anisotropic behavour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the radial angles\n",
    "AZIMUTHAL_WIDTH = 10\n",
    "\"\"\"The +- azimuthal width of the orthogonal range\"\"\"\n",
    "AZIMUTHAL_INPLANE = 10\n",
    "\"\"\"The azimuthal angle for the in-plane scattering\"\"\"\n",
    "AZIMUTHAL_OUTOFPLANE = 80\n",
    "\"\"\"The azimuthal angle for the out-of-plane averaging\"\"\"\n",
    "RADIAL_WIDTH = 35\n",
    "\"\"\"The +- azimuthal width for the radial averaging\"\"\"\n",
    "AZIMUTHAL_RADIAL = 45\n",
    "\"\"\"The azimuthal angle for the radial averaging\"\"\"\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for test_file in test_files:\n",
    "        # Setup a figure and open the file\n",
    "        fname = datasets[i][test_file]\n",
    "        \n",
    "        # Collect the metadata\n",
    "        en_idx = fname.find('eV_')\n",
    "        en = float(fname[en_idx-7:en_idx])\n",
    "        ai_idx = fname.find(\"_ai\")\n",
    "        ai = float(fname[ai_idx+3:ai_idx+8])\n",
    "        display(pd.DataFrame([\n",
    "                (fname, en, ai)\n",
    "                ], columns=[\"Filename\", \"Energy (eV)\", \"Incident Angle (deg)\"]))\n",
    "        \n",
    "        # Update the geometry\n",
    "        SMI_waxs.alphai = np.deg2rad(ai)\n",
    "        SMI_waxs.wav = en2wav(en)\n",
    "\n",
    "        # Show the angles on a plot\n",
    "        fig,ax = plt.subplots(1,1, figsize=(8,4), sharex=True, sharey=True)\n",
    "        ax.set_ylim(SMI_waxs.qz[0], SMI_waxs.qz[-1])\n",
    "        ax.set_xlim(SMI_waxs.qp[0], SMI_waxs.qp[-1])\n",
    "\n",
    "        SMI_waxs.open_data(RAW_DIR, [fname])\n",
    "        apply_detector_mask(SMI_waxs)\n",
    "        apply_flatfield(SMI_waxs, flatfield)\n",
    "        SMI_waxs.stitching_data(interp_factor=3, flag_scale=False)\n",
    "        # mp = ax[1].imshow(SMI_waxs.img_st,\n",
    "                # extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "        mp = ax.imshow(SMI_waxs.img_st,\n",
    "                extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "                vmin=np.percentile(SMI_waxs.img_st, 1.0), \n",
    "                vmax=np.percentile(SMI_waxs.img_st, 99.0)\n",
    "        )\n",
    "        plt.colorbar(mp)\n",
    "\n",
    "        # Plot the azimuthal and radial angles\n",
    "        colors = ['r', 'orange', 'white'][::-1]\n",
    "        for angle, width in zip([AZIMUTHAL_INPLANE, AZIMUTHAL_OUTOFPLANE, AZIMUTHAL_RADIAL], [AZIMUTHAL_WIDTH, AZIMUTHAL_WIDTH, RADIAL_WIDTH]):\n",
    "                # Generate a set of x points to plot lines of.\n",
    "                q_x = np.linspace(0, SMI_waxs.qp[-1], 100)\n",
    "                # Calculate the x and y gradients for the lines\n",
    "                m1 = np.tan(np.deg2rad(angle - width)) if angle - width != 90 else np.inf\n",
    "                m2 = np.tan(np.deg2rad(angle + width)) if angle + width != 90 else np.inf\n",
    "                # Calculate the x & y values for the lines\n",
    "                q_x1 = q_x if m1 != np.inf else np.zeros(100)\n",
    "                q_x2 = q_x if m2 != np.inf else np.zeros(100)\n",
    "                y1 = m1 * q_x if m1 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                y2 = m2 * q_x if m2 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                # Plot the lines\n",
    "                color = colors.pop()\n",
    "                ax.plot(q_x1, y1, color=color, linestyle='-', label=f\"{angle} deg\")\n",
    "                ax.plot(q_x2, y2, color=color, linestyle='-')\n",
    "                # If gradient is inf, calculate an alternative fill between\n",
    "                if m2 == np.inf:\n",
    "                        ax.fill_betweenx(y1, q_x1, q_x2, color=color, alpha=0.1)\n",
    "                else:\n",
    "                        ax.fill_between(q_x, y1, y2, color=color, alpha=0.1)\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "        # ax.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the azimuthal/radial averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPOINTS_RADIAL_AVE: int = 2000 # Use a number the is consistent with the pixel density?\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for test_file in test_files:\n",
    "        # Setup a figure and open the file\n",
    "        fname = datasets[i][test_file]\n",
    "        \n",
    "        # Collect the metadata\n",
    "        en_idx = fname.find('eV_')\n",
    "        en = float(fname[en_idx-7:en_idx])\n",
    "        ai_idx = fname.find(\"_ai\")\n",
    "        ai = float(fname[ai_idx+3:ai_idx+8])\n",
    "        display(pd.DataFrame([\n",
    "                (fname, en, ai)\n",
    "                ], columns=[\"Filename\", \"Energy (eV)\", \"Incident Angle (deg)\"]))\n",
    "        \n",
    "        # Update the geometry\n",
    "        SMI_waxs.alphai = np.deg2rad(ai)\n",
    "        SMI_waxs.wav = en2wav(en)\n",
    "\n",
    "        # Open and stitch the data\n",
    "        SMI_waxs.open_data(RAW_DIR, [fname])\n",
    "        apply_detector_mask(SMI_waxs)\n",
    "        apply_flatfield(SMI_waxs, flatfield)\n",
    "        SMI_waxs.stitching_data(interp_factor=3, flag_scale=False)\n",
    "\n",
    "        # Generate radial averages\n",
    "        fig,ax = plt.subplots(2,4, figsize=(12,5), sharex=True, height_ratios=[3,1])\n",
    "        fig.suptitle(f\"{sample}\\n{en:0.2f} eV, {ai:0.3f} deg\")\n",
    "        ax[0][0].set_xscale(\"log\")\n",
    "\n",
    "        # In plane and out of plane\n",
    "        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                azimuth_range=[90 - (AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH) , 90 - (AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                npt = NPOINTS_RADIAL_AVE)\n",
    "        q0_IP, I0_IP, I0_IP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                azimuth_range=[90 - (AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH), 90 - (AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                npt = NPOINTS_RADIAL_AVE)\n",
    "        q0_OOP, I0_OOP, I0_OOP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "        # Repeat IP and OOP for the consistency checking\n",
    "        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                azimuth_range=[-90+(AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH), -90+(AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                npt = NPOINTS_RADIAL_AVE)\n",
    "        q0_IP2, I0_IP2, I0_IP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                azimuth_range=[-90+(AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH) , -90+(AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                npt = NPOINTS_RADIAL_AVE)\n",
    "        q0_OOP2, I0_OOP2, I0_OOP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "        # Radial averaging\n",
    "        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                azimuth_range=[AZIMUTHAL_RADIAL - RADIAL_WIDTH , AZIMUTHAL_RADIAL + RADIAL_WIDTH], \n",
    "                                npt = NPOINTS_RADIAL_AVE)\n",
    "        q0_R, I0_R, I0_R_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "        # Repeat radial averaging for consistency checking\n",
    "        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                azimuth_range=[180-AZIMUTHAL_RADIAL - RADIAL_WIDTH , 180-AZIMUTHAL_RADIAL + RADIAL_WIDTH], \n",
    "                                npt = NPOINTS_RADIAL_AVE)\n",
    "        q0_R2, I0_R2, I0_R2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "        # Labels\n",
    "        ax[1][1].set_xlabel(\"$q$ ($Å^{-1}$)\")\n",
    "        ax[0][0].set_ylabel(\"Intensity (a.u.)\")\n",
    "        ax[1][0].set_ylabel(\"Error (Poisson)\")\n",
    "        ax[0][0].set_title(\"Radial Averaging\")\n",
    "        ax[0][1].set_title(\"In-plane Averaging\")\n",
    "        ax[0][2].set_title(\"Out-of-plane Averaging\")\n",
    "        # Plot the radial averages\n",
    "        ax[0][0].plot(q0_R, I0_R, label=\"Radial\")\n",
    "        ax[0][0].fill_between(q0_R, I0_R - I0_R_err, I0_R + I0_R_err, alpha=0.5)\n",
    "        ax[0][0].plot(q0_R2, I0_R2, label=\"Radial (180)\")\n",
    "        ax[0][0].fill_between(q0_R2, I0_R2 - I0_R2_err, I0_R2 + I0_R2_err, alpha=0.5)\n",
    "        # Plot the in-plane and out-of-plane averages\n",
    "        ax[0][1].plot(q0_IP, I0_IP, label=\"In-plane\")\n",
    "        ax[0][1].fill_between(q0_IP, I0_IP - I0_IP_err, I0_IP + I0_IP_err, alpha=0.5)\n",
    "        ax[0][1].plot(q0_IP2, I0_IP2, label=\"In-plane (180)\")\n",
    "        ax[0][1].fill_between(q0_IP2, I0_IP2 - I0_IP2_err, I0_IP2 + I0_IP2_err, alpha=0.5)\n",
    "        ax[0][2].plot(q0_OOP, I0_OOP, label=\"Out-of-plane\")\n",
    "        ax[0][2].fill_between(q0_OOP, I0_OOP - I0_OOP_err, I0_OOP + I0_OOP_err, alpha=0.5)\n",
    "        ax[0][2].plot(q0_OOP2, I0_OOP2, label=\"Out-of-plane (180)\")\n",
    "        ax[0][2].fill_between(q0_OOP2, I0_OOP2 - I0_OOP2_err, I0_OOP2 + I0_OOP2_err, alpha=0.5)\n",
    "        # Overlap the in-plane and out-of-plane averages to check for consistency\n",
    "        ax[0][3].plot(q0_IP, I0_IP, label=\"In-plane\")\n",
    "        ax[0][3].fill_between(q0_IP, I0_IP - I0_IP_err, I0_IP + I0_IP_err, alpha=0.5)\n",
    "        ax[0][3].plot(q0_OOP, I0_OOP, label=\"Out-of-plane\")\n",
    "        ax[0][3].fill_between(q0_OOP, I0_OOP - I0_OOP_err, I0_OOP + I0_OOP_err, alpha=0.5)\n",
    "        ax[0][3].plot(q0_IP2, I0_IP2, label=\"In-plane (180)\")\n",
    "        ax[0][3].fill_between(q0_IP2, I0_IP2 - I0_IP2_err, I0_IP2 + I0_IP2_err, alpha=0.5)\n",
    "        ax[0][3].plot(q0_OOP2, I0_OOP2, label=\"Out-of-plane (180)\")\n",
    "        ax[0][3].fill_between(q0_OOP2, I0_OOP2 - I0_OOP2_err, I0_OOP2 + I0_OOP2_err, alpha=0.5)\n",
    "\n",
    "        # Plot the errors\n",
    "        ax[1][0].plot(q0_R, I0_R_err, label=\"Radial\")\n",
    "        ax[1][0].plot(q0_R2, I0_R2_err, label=\"Radial (180)\")\n",
    "        ax[1][1].plot(q0_IP, I0_IP_err, label=\"In-plane\")\n",
    "        ax[1][1].plot(q0_IP2, I0_IP2_err, label=\"In-plane (180)\")\n",
    "        ax[1][2].plot(q0_OOP, I0_OOP_err, label=\"Out-of-plane\")\n",
    "        ax[1][2].plot(q0_OOP2, I0_OOP2_err, label=\"Out-of-plane (180)\")\n",
    "        ax[1][3].plot(q0_IP, I0_IP_err, label=\"In-plane\")\n",
    "        ax[1][3].plot(q0_OOP, I0_OOP_err, label=\"Out-of-plane\")\n",
    "        ax[1][3].plot(q0_IP2, I0_IP2_err, label=\"In-plane (180)\")\n",
    "        ax[1][3].plot(q0_OOP2, I0_OOP2_err, label=\"Out-of-plane (180)\")\n",
    "\n",
    "        for a in ax[0]:\n",
    "            a.legend()\n",
    "        for a in ax.flatten():\n",
    "            a.set_yscale(\"log\")\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        # plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If happy with line profiles above, then generate the line profiles for test_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDE: bool = False\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\"\n",
    "with tqdm.notebook.tqdm(total=len(samples), position=1, leave=True, desc=\"Samples\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        # For each file in the sample\n",
    "        with tqdm.notebook.tqdm(total=len(test_files), position=0, leave=False, desc=\"Files\") as pbar2:\n",
    "            for test_file in test_files:\n",
    "                ## Create the results directory for the sample\n",
    "                sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "                if not os.path.isdir(sample_dir):\n",
    "                    os.mkdir(sample_dir)\n",
    "                    \n",
    "                ## Create the directories for the images and line profiles\n",
    "                giwaxs_img_dir = os.path.join(sample_dir, \"giwaxs_flatfielded_images\")\n",
    "                if not os.path.isdir(giwaxs_img_dir):\n",
    "                    os.mkdir(giwaxs_img_dir)\n",
    "                line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "                if not os.path.isdir(line_profiles_dir):\n",
    "                    os.mkdir(line_profiles_dir)\n",
    "            \n",
    "            \n",
    "                for j, fname in enumerate(datasets[i][test_file:test_file+1]):\n",
    "                    # Collect the metadata\n",
    "                    en_idx = fname.find('eV_')\n",
    "                    en = float(fname[en_idx-7:en_idx])\n",
    "                    ai_idx = fname.find(\"_ai\")\n",
    "                    ai = float(fname[ai_idx+3:ai_idx+8])\n",
    "                    \n",
    "                    # Generate paths for the output files\n",
    "                    path_det_img = os.path.join(giwaxs_img_dir, f\"{sample}_giwaxs_{en:0.2f}eV_{ai:0.3f}deg.png\")\n",
    "                    path_OOP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_OOP.txt\")\n",
    "                    path_IP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_IP.txt\")\n",
    "                    path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_R.txt\")\n",
    "                    path_det_line_profiles_img = os.path.join(sample_dir, f\"{sample}_line_profile_angles_test.png\")\n",
    "                    \n",
    "                    # Do not override the files if they \"all\" exist already. Override partial file sets though.\n",
    "                    if (not OVERRIDE \n",
    "                        and os.path.isfile(path_det_img) \n",
    "                        and os.path.isfile(path_OOP) \n",
    "                        and os.path.isfile(path_IP) \n",
    "                        and os.path.isfile(path_R)\n",
    "                        and (j != 0 or os.path.isfile(path_det_line_profiles_img))):\n",
    "                        pbar.total -= 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Update the geometry\n",
    "                    SMI_waxs.alphai = np.deg2rad(ai)\n",
    "                    SMI_waxs.wav = en2wav(en)\n",
    "                    \n",
    "                    # Plot the flatfield / masked normalized data\n",
    "                    SMI_waxs.open_data(RAW_DIR, [fname])\n",
    "                    apply_detector_mask(SMI_waxs)\n",
    "                    apply_flatfield(SMI_waxs, flatfield)\n",
    "                    SMI_waxs.stitching_data(interp_factor=3, flag_scale=False)\n",
    "                    \n",
    "                    # Setup a figure and open the file\n",
    "                    fig,ax = plt.subplots(1,1, figsize=(7, 10), dpi=300)\n",
    "                    mp = ax.imshow(SMI_waxs.img_st,\n",
    "                            extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]],\n",
    "                            vmin=np.percentile(SMI_waxs.img_st, 1.0), \n",
    "                            vmax=np.percentile(SMI_waxs.img_st, 99.0) # Avoid extremities\n",
    "                    )\n",
    "                    plt.colorbar(mp)\n",
    "                    ax.set_title(f\"{sample}\\n{en} eV - {ai} deg\")\n",
    "                    # Don't save the image, as we use a different display format later.\n",
    "                    # fig.savefig(path_det_img)\n",
    "                    \n",
    "                    \n",
    "                    if j==0:\n",
    "                        # Plot the azimuthal and radial angles\n",
    "                        colors = ['r', 'orange', 'white'][::-1]\n",
    "                        for angle, width in zip([AZIMUTHAL_INPLANE, AZIMUTHAL_OUTOFPLANE, AZIMUTHAL_RADIAL], [AZIMUTHAL_WIDTH, AZIMUTHAL_WIDTH, RADIAL_WIDTH]):\n",
    "                            # Generate a set of x points to plot lines of.\n",
    "                            q_x = np.linspace(0, SMI_waxs.qp[-1], 100)\n",
    "                            # Calculate the x and y gradients for the lines\n",
    "                            m1 = np.tan(np.deg2rad(angle - width)) if angle - width != 90 else np.inf\n",
    "                            m2 = np.tan(np.deg2rad(angle + width)) if angle + width != 90 else np.inf\n",
    "                            # Calculate the x & y values for the lines\n",
    "                            q_x1 = q_x if m1 != np.inf else np.zeros(100)\n",
    "                            q_x2 = q_x if m2 != np.inf else np.zeros(100)\n",
    "                            y1 = m1 * q_x if m1 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                            y2 = m2 * q_x if m2 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                            # Plot the lines\n",
    "                            color = colors.pop()\n",
    "                            ax.plot(q_x1, y1, color=color, linestyle='-', label=f\"{angle} deg\")\n",
    "                            ax.plot(q_x2, y2, color=color, linestyle='-')\n",
    "                            # If gradient is inf, calculate an alternative fill between\n",
    "                            if m2 == np.inf:\n",
    "                                    ax.fill_betweenx(y1, q_x1, q_x2, color=color, alpha=0.1)\n",
    "                            else:\n",
    "                                    ax.fill_between(q_x, y1, y2, color=color, alpha=0.1)\n",
    "                        ax.set_xlim(*SMI_waxs.qp)\n",
    "                        ax.set_ylim(*SMI_waxs.qz)\n",
    "                        ax.legend()\n",
    "                        fig.savefig(path_det_line_profiles_img, dpi=300)\n",
    "                    plt.close() # Save memory\n",
    "                    \n",
    "                    # Perform the radial/azimuthal averaging\n",
    "                    # In plane and out of plane\n",
    "                    SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                            azimuth_range=[90 - (AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH) , 90 - (AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                            npt = NPOINTS_RADIAL_AVE)\n",
    "                    q0_IP, I0_IP, I0_IP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                    SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                            azimuth_range=[90 - (AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH), 90 - (AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                            npt = NPOINTS_RADIAL_AVE)\n",
    "                    q0_OOP, I0_OOP, I0_OOP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                    # Repeat IP and OOP for the consistency checking\n",
    "                    SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                            azimuth_range=[-90+(AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH), -90+(AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                            npt = NPOINTS_RADIAL_AVE)\n",
    "                    q0_IP2, I0_IP2, I0_IP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                    SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                            azimuth_range=[-90+(AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH) , -90+(AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                            npt = NPOINTS_RADIAL_AVE)\n",
    "                    q0_OOP2, I0_OOP2, I0_OOP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                    # Radial averaging\n",
    "                    SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                            azimuth_range=[AZIMUTHAL_RADIAL - RADIAL_WIDTH , AZIMUTHAL_RADIAL + RADIAL_WIDTH], \n",
    "                                            npt = NPOINTS_RADIAL_AVE)\n",
    "                    q0_R, I0_R, I0_R_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                    # Repeat radial averaging for consistency checking\n",
    "                    SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                            azimuth_range=[180-AZIMUTHAL_RADIAL - RADIAL_WIDTH , 180-AZIMUTHAL_RADIAL + RADIAL_WIDTH], \n",
    "                                            npt = NPOINTS_RADIAL_AVE)\n",
    "                    q0_R2, I0_R2, I0_R2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "                    \n",
    "                    # Save the line profiles \n",
    "                    header = (\"Main Data\\t\\tMirror-Y axis Data\\t\\n\" \n",
    "                            + \"q (Å^-1)\\tI (a.u.)\\tI_err (a.u.)\\tq (Å^-1)\\tI (a.u.)\\tI_err (a.u.)\\n\")\n",
    "                    delim = \"\\t\"\n",
    "                    kwargs = {\"header\": header, \"delimiter\": delim}\n",
    "                    np.savetxt(path_OOP, np.array([q0_OOP, I0_OOP, I0_OOP_err, q0_OOP2, I0_OOP2, I0_OOP2_err]).T, **kwargs)\n",
    "                    np.savetxt(path_IP, np.array([q0_IP, I0_IP, I0_IP_err, q0_IP2, I0_IP2, I0_IP2_err]).T, **kwargs)\n",
    "                    np.savetxt(path_R, np.array([q0_R, I0_R, I0_R_err, q0_R2, I0_R2, I0_R2_err]).T, **kwargs)\n",
    "                    \n",
    "                    # Update the progress bars\n",
    "                    pbar2.update(1)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Fitting\n",
    "##### Define q regions for truncation, where summation can be performed to observe changes in amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the q-range for the peaks\n",
    "VERT_PEAKS: list[tuple[float, float]] = [(0.23, 0.5),\n",
    "                                    # (0.5, 0.75), etc.\n",
    "                                    ]\n",
    "\"\"\"A list of tuples defining the q-range fitting for each peak of interest in the out-of-plane direction\"\"\"\n",
    "HOR_PEAKS: list[tuple[float, float]] = []\n",
    "\"\"\"A list of tuples defining the q-range fitting for each peak of interest in the in-plane direction\"\"\"\n",
    "RAD_PEAKS: list[tuple[float, float]] = []\n",
    "\"\"\"A list of tuples defining the q-range fitting for each peak of interest across the azimuthal\"\"\"\n",
    "\n",
    "VERT_BEAMSTOP: tuple[float, float] = (0.13, 0.15)\n",
    "\"\"\"A q-range specifying a q-range to collect the change in beamstop intensity\"\"\"\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for test_file in test_files:\n",
    "        # Setup a figure and open the file\n",
    "        fname = datasets[i][test_file]\n",
    "        print(fname)\n",
    "        # Collect the metadata\n",
    "        en_idx = fname.find('eV_')\n",
    "        en = float(fname[en_idx-7:en_idx])\n",
    "        ai_idx = fname.find(\"_ai\")\n",
    "        ai = float(fname[ai_idx+3:ai_idx+8])\n",
    "        \n",
    "        # Collect the line profiles from disk\n",
    "        sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "        line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "        if not os.path.isdir(line_profiles_dir):\n",
    "            os.mkdir(line_profiles_dir)\n",
    "        path_OOP = os.path.normpath(os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_OOP.txt\"))\n",
    "        path_IP = os.path.normpath(os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_IP.txt\"))\n",
    "        path_R = os.path.normpath(os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_R.txt\"))\n",
    "        \n",
    "        data_OOP = np.loadtxt(path_OOP, delimiter=\"\\t\", unpack=True, skiprows=2, dtype=float)\n",
    "        q0_OOP, I0_OOP, I0_OOP_err = data_OOP[0], data_OOP[1], data_OOP[2]\n",
    "        data_IP = np.loadtxt(path_IP, delimiter=\"\\t\", unpack=True, skiprows=2, dtype=float)\n",
    "        q0_IP, I0_IP, I0_IP_err = data_IP[0], data_IP[1], data_IP[2]\n",
    "        data_R = np.loadtxt(path_R, delimiter=\"\\t\", unpack=True, skiprows=2, dtype=float)\n",
    "        q0_R, I0_R, I0_R_err = data_R[0], data_R[1], data_R[2]\n",
    "\n",
    "        # Find the q-range to the region of interest\n",
    "        if len(VERT_PEAKS) > 0:\n",
    "            idx_OOP = np.where((q0_OOP > np.min(VERT_PEAKS)) & (q0_OOP < np.max(VERT_PEAKS)))\n",
    "        else:\n",
    "            idx_OOP = np.arange(len(q0_OOP))\n",
    "        if len(HOR_PEAKS) > 0:\n",
    "            idx_IP = np.where((q0_IP > np.min(HOR_PEAKS)) & (q0_IP < np.max(HOR_PEAKS)))\n",
    "        else:\n",
    "            idx_IP = np.arange(len(q0_IP))\n",
    "        if len(RAD_PEAKS) > 0:\n",
    "            idx_R = np.where((q0_R > np.min(RAD_PEAKS)) & (q0_R < np.max(RAD_PEAKS)))\n",
    "        else:\n",
    "            idx_R = np.arange(len(q0_R))\n",
    "        # Collect the beamstop region\n",
    "        idx_BS = np.where((q0_OOP > np.min(VERT_BEAMSTOP)) & (q0_OOP < np.max(VERT_BEAMSTOP)))\n",
    "            \n",
    "        # Truncate the data to the region of interest\n",
    "        q0_OOP_TR, I0_OOP_TR, I0_OOP_err_TR = q0_OOP[idx_OOP], I0_OOP[idx_OOP], I0_OOP_err[idx_OOP]\n",
    "        q0_IP_TR, I0_IP_TR, I0_IP_err_TR = q0_IP[idx_IP], I0_IP[idx_IP], I0_IP_err[idx_IP]\n",
    "        q0_R_TR, I0_R_TR, I0_R_err_TR = q0_R[idx_R], I0_R[idx_R], I0_R_err[idx_R]\n",
    "\n",
    "        # Plot the data\n",
    "        fig, ax = plt.subplots(max([len(VERT_PEAKS), len(HOR_PEAKS), len(RAD_PEAKS)]), 4, figsize=(12,4))\n",
    "        if len(ax.shape) == 1:\n",
    "            ax = ax.reshape(1,4)\n",
    "        for i, peak in enumerate(VERT_PEAKS):\n",
    "            idx = (q0_OOP_TR > peak[0]) & (q0_OOP_TR < peak[1])\n",
    "            ax[i][0].plot(q0_OOP_TR[idx], I0_OOP_TR[idx], label=\"Out of plane\")\n",
    "            ax[i][0].fill_between(q0_OOP_TR[idx], (I0_OOP_TR - I0_OOP_err_TR)[idx], (I0_OOP_TR + I0_OOP_err_TR)[idx], alpha=0.5)\n",
    "        for i, peak in enumerate(RAD_PEAKS):\n",
    "            idx = (q0_R_TR > peak[0]) & (q0_R_TR < peak[1])\n",
    "            ax[i][1].plot(q0_R_TR[idx], I0_R_TR[idx], label=\"Radial\")\n",
    "            ax[i][1].fill_between(q0_R_TR[idx], (I0_R_TR - I0_R_err_TR)[idx], (I0_R_TR + I0_R_err_TR)[idx], alpha=0.5)\n",
    "        for i, peak in enumerate(HOR_PEAKS):\n",
    "            idx = (q0_IP_TR > peak[0]) & (q0_IP_TR < peak[1])\n",
    "            ax[i][2].plot(q0_IP_TR[idx], I0_IP_TR[idx], label=\"In plane\")\n",
    "            ax[i][2].fill_between(q0_IP_TR[idx], (I0_IP_TR - I0_IP_err_TR)[idx], (I0_IP_TR + I0_IP_err_TR)[idx], alpha=0.5)\n",
    "        ax[0][0].set_title(\"Out of plane\")\n",
    "        ax[0][1].set_title(\"Radial\")\n",
    "        ax[0][2].set_title(\"In plane\")\n",
    "\n",
    "        for a in ax.flatten():\n",
    "            # a.set_xscale(\"log\")\n",
    "            a.set_yscale(\"log\")\n",
    "        # Plot the area under the beamstop region\n",
    "        ax[0][3].plot(q0_OOP[idx_BS], I0_OOP[idx_BS], label=\"Beamstop\")\n",
    "        ax[0][3].fill_between(q0_OOP[idx_BS], I0_OOP[idx_BS] - I0_OOP_err[idx_BS], I0_OOP[idx_BS] + I0_OOP_err[idx_BS], alpha=0.5)\n",
    "        ax[0][3].set_title(\"Beamstop\")\n",
    "        fig.suptitle(sample)\n",
    "        # plt.close()\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attempt some fitting of the peaks\n",
    "###### Define the peakfit functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_law = lambda x, a=1, b=-1, c=0: a * (x-c)**b\n",
    "\"\"\"A power law function for fitting the data\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "    a : float\n",
    "        The amplitude of the power law, by default 1\n",
    "    b : float\n",
    "        The power of the power law, by default -1\n",
    "    c : float\n",
    "        The translational offset of the power law, by default 0\n",
    "\"\"\"\n",
    "\n",
    "lorentz = lambda x, a=1, b=1, c=1: a * c / ((x - b)**2 + c**2)\n",
    "\"\"\"A Lorentzian function for fitting the data\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "    a : float\n",
    "        The amplitude of the Lorentzian, by default 1\n",
    "    b : float\n",
    "        The peak position of the Lorentzian, by default 1\n",
    "    c : float\n",
    "        The width of the Lorentzian, by default 1\n",
    "\"\"\"\n",
    "\n",
    "gauss = lambda x, a=1, b=1, c=1: a * np.exp(-((x - b)**2) / (2 * c**2))\n",
    "\"\"\"A Gaussian function for fitting the data\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "    a : float\n",
    "        The amplitude of the Gaussian, by default 1\n",
    "    b : float\n",
    "        The peak position of the Gaussian, by default 1\n",
    "    c : float\n",
    "        The width of the Gaussian, by default 1\n",
    "\"\"\"\n",
    "# # Without power law centre:\n",
    "fit_fn_lor = lambda x, a,b,c,d,e,f: np.sqrt((lorentz(x,a,b,c) + power_law(x,d,e))**2 + f**2)\n",
    "fit_fn_lor_log = lambda x, a,b,c,d,e,f: np.log(fit_fn_lor(x,a,b,c,d,e,f))\n",
    "fit_fn_gau = lambda x, a,b,c,d,e,f: np.sqrt((gauss(x,a,b,c) + power_law(x,d,e))**2 + f**2)\n",
    "fit_fn_gau_log = lambda x, a,b,c,d,e,f: np.log(fit_fn_gau(x,a,b,c,d,e,f))\n",
    "\n",
    "# fit_fn_lor_sin = lambda x, a,b,c,d,e,f,g,h,i: np.sqrt((lorentz(x,a,b,c) + power_law(x,d,e) + h*np.sin(x*2*np.pi*f - g*np.pi/180)**2)**2 + i**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(any([\"A1_04\" in file for  file in datasets[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a reasonable guess close to the optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial guesses for the peaks\n",
    "VERT_GUESSES: list[tuple] = [\n",
    "    (100,\t0.37,\t0.017,\t50,\t-2.6,\t3000),\n",
    "]\n",
    "\"\"\"A list of tuples defining the initial guesses for each out-of-plane peak of interest\"\"\"\n",
    "HOR_GUESSES: list[tuple] = []\n",
    "\"\"\"A list of tuples defining the initial guesses for each in-plane peak of interest\"\"\"\n",
    "RAD_GUESSES: list[tuple] = []\n",
    "\"\"\"A list of tuples defining the initial guesses for each radial peak of interest\"\"\"\n",
    "\n",
    "# Define the parameter labels\n",
    "VERT_LABELS: list[list[str]] = [\n",
    "    # [\"Gauss Amp.\", \"Gauss Centre\", \"Gauss Width\", \"Powerlaw Amplitude\", \"Powerlaw Power\", \"Offset\"]\n",
    "    [\"Lorentz Amp.\", \"Lorentz Centre\", \"Lorentz Width\", \"Powerlaw Amplitude\", \"Powerlaw Power\", \"Offset\"]\n",
    "    # [\"Lorentz Amp.\", \"Lorentz Centre\", \"Lorentz Width\", \"Powerlaw Amplitude\", \"Powerlaw Power\", \"Sin Freq.\", \"Sin Phase\", \"Sin Amp.\", \"Offset\"]\n",
    "]\n",
    "\"\"\"A list of lists defining the labels for the fitting parameters for each out-of-plane peak\"\"\"\n",
    "HOR_LABELS: list[list[str]] = []\n",
    "\"\"\"A list of lists defining the labels for the fitting parameters for each in-plane peak\"\"\"\n",
    "RAD_LABELS: list[list[str]] = []\n",
    "\"\"\"A list of lists defining the labels for the fitting parameters for each radial peak\"\"\"\n",
    "\n",
    "# Fitting functions\n",
    "# VERT_FIT_FNS: list[Callable] = [fit_fn_lor_sin]\n",
    "VERT_FIT_FNS: list[Callable] = [fit_fn_lor]\n",
    "# VERT_FIT_FNS: list[Callable] = [fit_fn_gau]\n",
    "\"\"\"A list of fitting functions for the out-of-plane peaks\"\"\"\n",
    "HOR_FIT_FNS: list[Callable] = []\n",
    "\"\"\"A list of fitting functions for the in-plane peaks\"\"\"\n",
    "RAD_FIT_FNS: list[Callable] = []\n",
    "\"\"\"A list of fitting functions for the radial peaks\"\"\"\n",
    "\n",
    "#Bounds\n",
    "# VERT_LB : list[tuple | None] = [(0, 0, 0, 0, -np.inf, 0, -180, 0, 0)]\n",
    "VERT_LB : list[tuple | None] = [(0, 0.36, 0.001, 0, -np.inf, 0)]\n",
    "\"\"\"List of tuples defining the lower bounds for the fitting parameters for each out-of-plane peak\"\"\"\n",
    "# VERT_UB : list[tuple | None] = [(np.inf, np.inf, np.inf, np.inf, -1, np.inf, 180, np.inf, np.inf)]\n",
    "VERT_UB : list[tuple | None] = [(np.inf, 0.4, 0.04, np.inf, -1, np.inf)]\n",
    "\"\"\"List of tuples defining the upper bounds for the fitting parameters for each out-of-plane peak\"\"\"\n",
    "VERT_BOUNDS : list[tuple[tuple | None, tuple | None] | None] = [(VERT_LB[i], VERT_UB[i]) for i in range(len(VERT_LB))]\n",
    "\"\"\"List of tuples of lower-bound/upper-bound tuples, defining the bounds for the fitting parameters for each out-of-plane peak\"\"\"\n",
    "HOR_LB : list[tuple | None] = []\n",
    "\"\"\"List of tuples defining the lower bounds for the fitting parameters for each in-plane peak\"\"\"\n",
    "HOR_UB : list[tuple | None] = []\n",
    "\"\"\"List of tuples defining the upper bounds for the fitting parameters for each in-plane peak\"\"\"\n",
    "HOR_BOUNDS : list[tuple[tuple | None, tuple | None] | None] = [(HOR_LB[i], HOR_UB[i]) for i in range(len(HOR_LB))]\n",
    "\"\"\"List of tuples of lower-bound/upper-bound tuples, defining the bounds for the fitting parameters for each in-plane peak\"\"\"\n",
    "RAD_LB : list[tuple | None] = []\n",
    "\"\"\"List of tuples defining the lower bounds for the fitting parameters for each radial peak\"\"\"\n",
    "RAD_UB : list[tuple | None] = []\n",
    "\"\"\"List of tuples defining the upper bounds for the fitting parameters for each radial peak\"\"\"\n",
    "RAD_BOUNDS : list[tuple[tuple | None, tuple | None] | None] = [(RAD_LB[i], RAD_UB[i]) for i in range(len(RAD_LB))]\n",
    "\"\"\"List of tuples of lower-bound/upper-bound tuples, defining the bounds for the fitting parameters for each radial peak\"\"\"\n",
    "\n",
    "# Convert (None, None) tuples to None for the bounds\n",
    "VERT_BOUNDS = [None if bounds == (None, None) else bounds for bounds in VERT_BOUNDS]\n",
    "HOR_BOUNDS = [None if bounds == (None, None) else bounds for bounds in HOR_BOUNDS]\n",
    "RAD_BOUNDS = [None if bounds == (None, None) else bounds for bounds in RAD_BOUNDS]\n",
    "\n",
    "# Check the number of guesses matches the number of peaks\n",
    "assert len(VERT_PEAKS) == len(VERT_GUESSES) == len(VERT_BOUNDS) == len(VERT_LABELS), \"The number of guesses must match the number of peaks and bounds.\"\n",
    "assert len(HOR_PEAKS) == len(HOR_GUESSES) == len(HOR_BOUNDS) == len(HOR_LABELS), \"The number of guesses must match the number of peaks and bounds.\"\n",
    "assert len(RAD_PEAKS) == len(RAD_GUESSES) == len(RAD_BOUNDS) == len(RAD_LABELS), \"The number of guesses must match the number of peaks and bounds.\"\n",
    "\n",
    "# Pack the fitting functions, guesses and bounds into a single iterable\n",
    "FIT_REGISTER: list[tuple[Literal[\"OOP\"] | Literal[\"IP\"] | Literal[\"RAD\"],\n",
    "                         tuple[float, float],\n",
    "                         Callable, \n",
    "                         tuple, \n",
    "                         tuple[tuple | None, tuple | None] | None,\n",
    "                         list[str]]\n",
    "                   ] = []\n",
    "\"\"\"A list of tuples defining:\n",
    "    1. The peak type (e.g. 'OOP', 'IP', 'RAD')\n",
    "    2. The peak q-range\n",
    "    3. The fitting function\n",
    "    4. The initial guesses tuple\n",
    "    5. The bounds tuple (lower|None, upper|None) | None matching length of guesses tuple.\n",
    "    6. The list of parameter labels (corresponding to 4,5).\n",
    "\"\"\"\n",
    "for i, peak in enumerate(VERT_PEAKS):\n",
    "    FIT_REGISTER.append((\"OOP\", peak, VERT_FIT_FNS[i], VERT_GUESSES[i], VERT_BOUNDS[i], VERT_LABELS[i]))\n",
    "for i, peak in enumerate(HOR_PEAKS):\n",
    "    FIT_REGISTER.append((\"IP\", peak, HOR_FIT_FNS[i], HOR_GUESSES[i], HOR_BOUNDS[i], HOR_LABELS[i]))\n",
    "for i, peak in enumerate(RAD_PEAKS):\n",
    "    FIT_REGISTER.append((\"RAD\", peak, RAD_FIT_FNS[i], RAD_GUESSES[i], RAD_BOUNDS[i], RAD_LABELS[i]))\n",
    "\n",
    "# Display the initial guesses over the data\n",
    "FIT_N = max([len(VERT_PEAKS), len(HOR_PEAKS), len(RAD_PEAKS)])\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for test_file in test_files:\n",
    "        # Setup a figure and open the file\n",
    "        fname = datasets[i][test_file]\n",
    "        print(fname)\n",
    "        # Collect the metadata\n",
    "        en_idx = fname.find('eV_')\n",
    "        en = float(fname[en_idx-7:en_idx])\n",
    "        ai_idx = fname.find(\"_ai\")\n",
    "        ai = float(fname[ai_idx+3:ai_idx+8])\n",
    "        \n",
    "        # Collect the line profiles from disk\n",
    "        sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "        line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "        if not os.path.isdir(line_profiles_dir):\n",
    "            os.mkdir(line_profiles_dir)\n",
    "        path_OOP = os.path.normpath(os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_OOP.txt\"))\n",
    "        path_IP = os.path.normpath(os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_IP.txt\"))\n",
    "        path_R = os.path.normpath(os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_R.txt\"))\n",
    "        \n",
    "        data_OOP = np.loadtxt(path_OOP, delimiter=\"\\t\", unpack=True, skiprows=2, dtype=float)\n",
    "        q0_OOP, I0_OOP, I0_OOP_err = data_OOP[0], data_OOP[1], data_OOP[2]\n",
    "        data_IP = np.loadtxt(path_IP, delimiter=\"\\t\", unpack=True, skiprows=2, dtype=float)\n",
    "        q0_IP, I0_IP, I0_IP_err = data_IP[0], data_IP[1], data_IP[2]\n",
    "        data_R = np.loadtxt(path_R, delimiter=\"\\t\", unpack=True, skiprows=2, dtype=float)\n",
    "        q0_R, I0_R, I0_R_err = data_R[0], data_R[1], data_R[2]\n",
    "\n",
    "        fig, ax = plt.subplots(FIT_N, 3, figsize=(12,3*FIT_N))\n",
    "        if len(ax.shape) == 1:\n",
    "            ax = ax.reshape(1,3)\n",
    "\n",
    "        counts = [0, 0, 0] # Count the number of peaks for each direction\n",
    "        for k, fit in enumerate(FIT_REGISTER):\n",
    "            peak_type, peak_range, fit_fn, guess, bounds, labels = fit\n",
    "            match peak_type:\n",
    "                case \"OOP\":\n",
    "                    j = 0\n",
    "                    q, I, I_err = q0_OOP, I0_OOP, I0_OOP_err\n",
    "                case \"IP\":\n",
    "                    j = 1\n",
    "                    q, I, I_err = q0_IP, I0_IP, I0_IP_err\n",
    "                case \"RAD\":\n",
    "                    j = 2\n",
    "                    q, I, I_err = q0_R, I0_R, I0_R_err\n",
    "            idxs = np.where((q > peak_range[0]) & (q < peak_range[1]))\n",
    "            q = q[idxs]\n",
    "            I = I[idxs]\n",
    "            I_err = I_err[idxs]\n",
    "            \n",
    "            ax[j][counts[j]].set_title(f\"{peak_type} Peak #{k}\")\n",
    "            ax[j][counts[j]].plot(q, I, label=f\"{peak_type} Data\")\n",
    "            ax[j][counts[j]].fill_between(q, I - I_err, I + I_err, alpha=0.5)\n",
    "            ax[j][counts[j]].plot(q, fit_fn(q, *VERT_GUESSES[k]), label=\"Initial Guess\")\n",
    "            popt, pcov = curve_fit(fit_fn, q, I, p0=guess, sigma=I_err, maxfev=100000, bounds=bounds)\n",
    "            ax[j][counts[j]].plot(q, fit_fn(q, *popt), label=\"Fitted\")\n",
    "            ax[j][counts[j]].legend()\n",
    "            display(pd.DataFrame(\n",
    "                [popt, np.sqrt(np.diag(pcov))],\n",
    "                columns=labels,\n",
    "                index=[f\"{peak_type} Peak #{k} Fit\", f\"{peak_type} Peak #{k} Error\"]\n",
    "            ))\n",
    "            counts[j] += 1\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Monte-Carlo Markov Chains to see if the Least Squares solution is really a good fit of random variables\n",
    "##### Define the likelihood, prior and probability functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fns(fit_fn: Callable, bounds: tuple[tuple | None, tuple | None] | None) -> tuple[Callable, Callable, Callable]:\n",
    "    \"\"\"Genereate the log-likelihood, log-prior and log-probability functions for the fitting\"\"\"\n",
    "    def log_likelihood(theta, x, y, yerr, fit_fn: Callable = fit_fn) -> float:\n",
    "        model_y = fit_fn(x, *theta)\n",
    "        sigma2 = yerr**2\n",
    "        return -0.5 * np.sum((y - model_y)**2 / sigma2 )\n",
    "    \n",
    "    def log_prior(theta, bounds: tuple[tuple | None, tuple | None] | None = bounds) -> float:\n",
    "        \"\"\"Define the log-prior function for the fitting\"\"\"\n",
    "        if bounds is None:\n",
    "            return 0.0\n",
    "        lb, ub = bounds\n",
    "        # Return -np.inf if out of bounds.\n",
    "        if lb is not None:\n",
    "            for i in range(len(theta)):\n",
    "                if lb[i] is not None:\n",
    "                    if lb[i] > theta[i]:\n",
    "                        return -np.inf\n",
    "        if not ub is None:\n",
    "            for i in range(len(theta)):\n",
    "                if ub[i] is not None:\n",
    "                    if ub[i] < theta[i]:\n",
    "                        return -np.inf\n",
    "        return 0.0\n",
    "    \n",
    "    def log_probability(theta, x, y, yerr, lprior: Callable = log_prior, llikelihood: Callable = log_likelihood) -> float:\n",
    "        \"\"\"Define the log-probability function for the fitting\"\"\"\n",
    "        lp = lprior(theta)\n",
    "        if not np.isfinite(lp):\n",
    "            return -np.inf\n",
    "        return lp + llikelihood(theta, x, y, yerr)\n",
    "    \n",
    "    return log_likelihood, log_prior, log_probability\n",
    "\n",
    "LOG_LIKELIHOOD_FNS: list[Callable] = []\n",
    "\"\"\"A list of log-likelihood functions for each peak. By default a Gaussian log-likelihood function is used for each parameter, but can be redefined.\"\"\"\n",
    "LOG_PRIOR_FNS: list[Callable] = []\n",
    "\"\"\"A list of log-prior functions for each peak. By default no prior is defined, but can be redefined.\"\"\"\n",
    "LOG_PROBABILITY_FNS: list[Callable] = []\n",
    "\n",
    "for i, fit in enumerate(FIT_REGISTER):\n",
    "    peak_type, peak_range, fit_fn, guess, bounds, labels = fit\n",
    "    log_likelihood, log_prior, log_probability = gen_fns(fit_fn, bounds)\n",
    "    LOG_LIKELIHOOD_FNS.append(log_likelihood)\n",
    "    LOG_PRIOR_FNS.append(log_prior)\n",
    "    LOG_PROBABILITY_FNS.append(log_probability)\n",
    "\n",
    "# Test the log_probability function - should be non-infinite if working.\n",
    "LOG_PROBABILITY_FNS[0](\n",
    "    theta=VERT_GUESSES[0],\n",
    "    x=q0_OOP_TR, \n",
    "    y=I0_OOP_TR, \n",
    "    yerr=np.sqrt(I0_OOP_TR),\n",
    "    lprior=LOG_PRIOR_FNS[0],\n",
    "    llikelihood=LOG_LIKELIHOOD_FNS[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use emcee sampler to run N walkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the data from the first vert peak\n",
    "qmin, qmax = VERT_PEAKS[0]\n",
    "idxs = np.where((q0_OOP_TR > qmin) & (q0_OOP_TR < qmax))\n",
    "x = q0_OOP_TR[idxs]\n",
    "y = I0_OOP_TR[idxs]\n",
    "yerr = I0_OOP_err_TR[idxs]\n",
    "popt, pcov = curve_fit(VERT_FIT_FNS[0], x, y, sigma=yerr, p0=VERT_GUESSES[0], maxfev=10000, bounds=VERT_BOUNDS[0])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Use the model on the first vert peak.\n",
    "N = 200\n",
    "pos = popt * np.ones((N, len(popt))) * (1 + (1.0e-2 * np.random.randn(N, len(popt)) - 5.0e-3))\n",
    "# Check each parameter is within bounds\n",
    "for i in range(len(VERT_BOUNDS[0][0])):\n",
    "    lb, ub = VERT_BOUNDS[0][0][i], VERT_BOUNDS[0][1][i]\n",
    "    if lb is not None:\n",
    "        pos[:,i] = np.clip(pos[:,i], lb, np.inf)\n",
    "    if ub is not None:\n",
    "        pos[:,i] = np.clip(pos[:,i], -np.inf, ub)\n",
    "nwalkers, ndim = pos.shape\n",
    "\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers, ndim, LOG_PROBABILITY_FNS[0], args=(x, y, yerr, LOG_PRIOR_FNS[0], LOG_LIKELIHOOD_FNS[0])\n",
    ")\n",
    "M = 2000\n",
    "sampler.run_mcmc(pos, M, progress=True)\n",
    "max_tau = None\n",
    "while max_tau is None:\n",
    "    try:\n",
    "        tau = sampler.get_autocorr_time()\n",
    "        max_tau = np.max(tau)\n",
    "    except emcee.autocorr.AutocorrError as e:\n",
    "        tau_estimate = int(np.max(e.tau))\n",
    "        if tau_estimate * 50 < 150000:\n",
    "            print(f\"Estimated max tau: {tau_estimate}, running for {50*tau_estimate} more steps.\")\n",
    "            sampler.run_mcmc(None, tau_estimate * 50, progress=False)\n",
    "        else:\n",
    "            print(f\"Tau estimate too high ({tau_estimate}), to run Tau*50 steps, exiting...\")\n",
    "            break\n",
    "    \n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start:.1f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display the chain output and estimate where the burn-in period (tau) is.\n",
    "fig, axes = plt.subplots(len(VERT_LABELS[0]), figsize=(10, 2*len(VERT_LABELS[0])), sharex=True)\n",
    "MC_samples = sampler.get_chain()\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(MC_samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(MC_samples))\n",
    "    ax.set_ylabel(VERT_LABELS[0][i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "axes[-1].set_xlabel(\"step number\")\n",
    "tau_max = np.max(tau)\n",
    "fig.suptitle(f\"Chain Output - Corr time = {tau_max:0.2f} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_MC_samples = sampler.get_chain(discard=int(tau_max * 3), thin=5, flat=True)\n",
    "fig = corner.corner(\n",
    "    flat_MC_samples, labels=VERT_LABELS[0], truths=popt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result of the Least Squares fit and the MCMC fit\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "q = q0_OOP_TR[idxs]\n",
    "ax[0].plot(q0_OOP_TR[idxs], I0_OOP_TR[idxs], label=\"Data\")\n",
    "ax[0].fill_between(q, I0_OOP_TR[idxs] - I0_OOP_err_TR[idxs], I0_OOP_TR[idxs] + I0_OOP_err_TR[idxs], alpha=0.5)\n",
    "ax[0].plot(q, FIT_REGISTER[0][2](q, *popt), label=\"Least Squares Fit\")\n",
    "ax[0].fill_between(q, FIT_REGISTER[0][2](q, *popt - np.sqrt(np.diag(pcov))), FIT_REGISTER[0][2](q, *popt + np.sqrt(np.diag(pcov))), alpha=0.1)\n",
    "ax[0].legend()\n",
    "                   \n",
    "# Plot the MCMC fit\n",
    "MC_samples = sampler.get_chain(discard=200, thin=5, flat=True)\n",
    "percentiles = [np.percentile(MC_samples[:, i], [16, 50, 84]) for i in range(MC_samples.shape[1])] #Sampling at 1 sigma percentiles\n",
    "values = [p[1] for p in percentiles]\n",
    "error_bounds = np.array([[p[0] - p[1], p[2] - p[1]] for p in percentiles])\n",
    "\n",
    "\n",
    "inds = np.random.randint(len(MC_samples), size=100)\n",
    "for ind in inds:\n",
    "    sample = MC_samples[ind]\n",
    "    ax[1].plot(q, FIT_REGISTER[0][2](q, *sample), \"C1\", alpha=0.1, label=\"MCMC Fit\" if ind == inds[0] else None)\n",
    "ax[1].plot(q0_OOP_TR[idxs], I0_OOP_TR[idxs], label=\"Data\")\n",
    "ax[1].fill_between(q, I0_OOP_TR[idxs] - I0_OOP_err_TR[idxs], I0_OOP_TR[idxs] + I0_OOP_err_TR[idxs], alpha=0.5)\n",
    "ax[1].plot(q, FIT_REGISTER[0][2](q, *values), 'black', label=\"MCMC Fit\")\n",
    "ax[1].plot(q, FIT_REGISTER[0][2](q, *popt), label=\"Least Squares Fit\", c='green')\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "display(pd.DataFrame(\n",
    "    [popt, np.sqrt(np.diag(pcov)), values, error_bounds[:,0], error_bounds[:,1]],\n",
    "    columns=VERT_LABELS[0],\n",
    "    index=[f\"Peak #{i} Fit\", f\"Peak #{i} Error\", \"MCMC Fit\", \"MCMC LB Error\", \"MCMC UB Error\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform reduction and analysis across all samples\n",
    "### Detector images and consequent line profile reduction\n",
    "##### Create a plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aoi_video_figure(geom: SMI_beamline.SMI_geometry, \n",
    "                              dataset_minimum:float, \n",
    "                              dataset_maximum: float, \n",
    "                              ai: float) -> plt.Figure:\n",
    "    SMI_waxs = geom\n",
    "    fig = plt.figure(figsize = (4,5), dpi=150)\n",
    "    gs = fig.add_gridspec(2,2, width_ratios=[1,0.05], height_ratios=[1, 0.05])\n",
    "    ax = fig.add_subplot(gs[0,0])\n",
    "    ax_cmap = fig.add_subplot(gs[0,1])\n",
    "    ax_ai = fig.add_subplot(gs[1,0])\n",
    "\n",
    "    cmap = plt.cm.get_cmap('terrain')\n",
    "    norm = mplc.LogNorm(vmin=dataset_minimum, vmax=dataset_maximum, clip=True)\n",
    "    mp = ax.matshow(SMI_waxs.img_st,\n",
    "            extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]],\n",
    "            cmap = cmap,\n",
    "            norm=norm\n",
    "    )\n",
    "    colorbar = fig.colorbar(mp, cax=ax_cmap, orientation='vertical', location=\"right\", fraction = 0.05)\n",
    "    colorbar.ax.set_ylabel(\"Intensity (a.u.)\", fontsize=4)\n",
    "    colorbar.ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax2 = ax_ai\n",
    "    ax2.yaxis.set_ticks([])\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.set_xscale(\"log\")\n",
    "    ax2.set_xlim(0.1, 10)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=4)\n",
    "    ax2.plot([ai, ai], [0, 1], 'r')\n",
    "    ax2.set_xlabel(f\"Angle of Incidence     {ai:0.3f}°\", loc=\"left\", fontsize=4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "if \"dataset_minimum\" in locals():\n",
    "    generate_aoi_video_figure(SMI_waxs, 1, dataset_maximum, ai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reduce the lineprofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDE: bool = False\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\"\n",
    "\n",
    "plt.ioff()\n",
    "matplotlib.interactive(False)\n",
    "with tqdm.notebook.tqdm(total=len(samples), position=0, leave=True, desc=\"All Samples\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        ## Create the results directory for the sample\n",
    "        sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "        if not os.path.isdir(sample_dir):\n",
    "            os.mkdir(sample_dir)\n",
    "            \n",
    "        ## Create the directories for the images and line profiles\n",
    "        giwaxs_img_dir = os.path.join(sample_dir, \"giwaxs_flatfielded_images\")\n",
    "        if not os.path.isdir(giwaxs_img_dir):\n",
    "            os.mkdir(giwaxs_img_dir)\n",
    "        line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "        if not os.path.isdir(line_profiles_dir):\n",
    "            os.mkdir(line_profiles_dir)\n",
    "        \n",
    "        dataset_maximum = None\n",
    "        dataset_minimum = None\n",
    "        with tqdm.notebook.tqdm(total=len(datasets[i]), position=1, leave=False, desc=\"Finding dataset maximum / minimum\") as pbar2:\n",
    "            for j, fname in enumerate(datasets[i]):\n",
    "                # Collect the metadata\n",
    "                en_idx = fname.find('eV_')\n",
    "                en = float(fname[en_idx-7:en_idx])\n",
    "                ai_idx = fname.find(\"_ai\")\n",
    "                ai = float(fname[ai_idx+3:ai_idx+8])\n",
    "                \n",
    "                # Update the geometry\n",
    "                SMI_waxs.alphai = np.deg2rad(ai)\n",
    "                SMI_waxs.wav = en2wav(en)\n",
    "                \n",
    "                # Flatfield / masked normalized data\n",
    "                SMI_waxs.open_data(RAW_DIR, [fname])\n",
    "                apply_detector_mask(SMI_waxs)\n",
    "                apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "                SMI_waxs.stitching_data(interp_factor=3, flag_scale=False)\n",
    "                \n",
    "                # Collect the maximum and minimum values\n",
    "                if dataset_maximum is None:\n",
    "                    dataset_maximum = np.max(SMI_waxs.img_st)\n",
    "                else:\n",
    "                    dataset_maximum = np.max([dataset_maximum, np.max(SMI_waxs.img_st)])\n",
    "                    \n",
    "                if dataset_minimum is None:\n",
    "                    dataset_minimum = np.min(SMI_waxs.img_st[SMI_waxs.img_st > 0])\n",
    "                else:\n",
    "                    min_val = np.min(SMI_waxs.img_st[SMI_waxs.img_st > 0])\n",
    "                    dataset_minimum = np.min([dataset_minimum, min_val])\n",
    "                pbar2.update(1)\n",
    "        \n",
    "        cmap = plt.cm.get_cmap('viridis')\n",
    "        norm = mplc.LogNorm(vmin=dataset_minimum, vmax=dataset_maximum)\n",
    "        \n",
    "        with tqdm.notebook.tqdm(total=len(datasets[i]), position=1, leave=False, desc=\"Sample dataset\") as pbar2:\n",
    "            # For each file in the sample\n",
    "            for j, fname in enumerate(datasets[i]):\n",
    "                # Collect the metadata\n",
    "                en_idx = fname.find('eV_')\n",
    "                en = float(fname[en_idx-7:en_idx])\n",
    "                ai_idx = fname.find(\"_ai\")\n",
    "                ai = float(fname[ai_idx+3:ai_idx+8])\n",
    "                \n",
    "                # Generate paths for the output files\n",
    "                path_det_img = os.path.join(giwaxs_img_dir, f\"{sample}_giwaxs_{en:0.2f}eV_{ai:0.3f}deg.png\")\n",
    "                path_OOP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_OOP.txt\")\n",
    "                path_IP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_IP.txt\")\n",
    "                path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_R.txt\")\n",
    "                path_det_line_profiles_img = os.path.join(sample_dir, f\"{sample}_line_profile_angles.png\")\n",
    "                \n",
    "                # Do not override the files if they \"all\" exist already. Override partial file sets though.\n",
    "                if (not OVERRIDE \n",
    "                    and os.path.isfile(path_det_img) \n",
    "                    and os.path.isfile(path_OOP) \n",
    "                    and os.path.isfile(path_IP) \n",
    "                    and os.path.isfile(path_R)\n",
    "                    and (j != 0 or os.path.isfile(path_det_line_profiles_img))):\n",
    "                    pbar2.total -= 1 # Reduce the total count\n",
    "                    continue\n",
    "                \n",
    "                # Update the geometry\n",
    "                SMI_waxs.alphai = np.deg2rad(ai)\n",
    "                SMI_waxs.wav = en2wav(en)\n",
    "                \n",
    "                # flatfield / masked normalized data\n",
    "                SMI_waxs.open_data(RAW_DIR, [fname])\n",
    "                apply_detector_mask(SMI_waxs)\n",
    "                apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "                SMI_waxs.stitching_data(interp_factor=3, flag_scale=False)\n",
    "                \n",
    "                # Setup a figure and open the file\n",
    "                fig = generate_aoi_video_figure(SMI_waxs, 1, dataset_maximum, ai)\n",
    "                fig.savefig(path_det_img)\n",
    "                ax = fig.get_axes()[0]\n",
    "                \n",
    "                if j==0:\n",
    "                    # Plot the azimuthal and radial angles\n",
    "                    colors = ['r', 'orange', 'white'][::-1]\n",
    "                    for angle, width in zip([AZIMUTHAL_INPLANE, AZIMUTHAL_OUTOFPLANE, AZIMUTHAL_RADIAL], [AZIMUTHAL_WIDTH, AZIMUTHAL_WIDTH, RADIAL_WIDTH]):\n",
    "                        # Generate a set of x points to plot lines of.\n",
    "                        q_x = np.linspace(0, SMI_waxs.qp[-1], 100)\n",
    "                        # Calculate the x and y gradients for the lines\n",
    "                        m1 = np.tan(np.deg2rad(angle - width)) if angle - width != 90 else np.inf\n",
    "                        m2 = np.tan(np.deg2rad(angle + width)) if angle + width != 90 else np.inf\n",
    "                        # Calculate the x & y values for the lines\n",
    "                        q_x1 = q_x if m1 != np.inf else np.zeros(100)\n",
    "                        q_x2 = q_x if m2 != np.inf else np.zeros(100)\n",
    "                        y1 = m1 * q_x if m1 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                        y2 = m2 * q_x if m2 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                        # Plot the lines\n",
    "                        color = colors.pop()\n",
    "                        ax.plot(q_x1, y1, color=color, linestyle='-', label=f\"{angle} deg\")\n",
    "                        ax.plot(q_x2, y2, color=color, linestyle='-')\n",
    "                        # If gradient is inf, calculate an alternative fill between\n",
    "                        if m2 == np.inf:\n",
    "                                ax.fill_betweenx(y1, q_x1, q_x2, color=color, alpha=0.1)\n",
    "                        else:\n",
    "                                ax.fill_between(q_x, y1, y2, color=color, alpha=0.1)\n",
    "                    ax.set_xlim(*SMI_waxs.qp)\n",
    "                    ax.set_ylim(*SMI_waxs.qz)\n",
    "                    ax.legend()\n",
    "                    fig.savefig(path_det_line_profiles_img, dpi=300)\n",
    "                plt.close(fig) # Save memory\n",
    "                \n",
    "                # Perform the radial/azimuthal averaging\n",
    "                # In plane and out of plane\n",
    "                SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                        azimuth_range=[90 - (AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH) , 90 - (AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                        npt = NPOINTS_RADIAL_AVE)\n",
    "                q0_IP, I0_IP, I0_IP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                        azimuth_range=[90 - (AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH), 90 - (AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                        npt = NPOINTS_RADIAL_AVE)\n",
    "                q0_OOP, I0_OOP, I0_OOP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                # Repeat IP and OOP for the consistency checking\n",
    "                SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                        azimuth_range=[-90+(AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH), -90+(AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                        npt = NPOINTS_RADIAL_AVE)\n",
    "                q0_IP2, I0_IP2, I0_IP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                        azimuth_range=[-90+(AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH) , -90+(AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                        npt = NPOINTS_RADIAL_AVE)\n",
    "                q0_OOP2, I0_OOP2, I0_OOP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                # Radial averaging\n",
    "                SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                        azimuth_range=[AZIMUTHAL_RADIAL - RADIAL_WIDTH , AZIMUTHAL_RADIAL + RADIAL_WIDTH], \n",
    "                                        npt = NPOINTS_RADIAL_AVE)\n",
    "                q0_R, I0_R, I0_R_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                # Repeat radial averaging for consistency checking\n",
    "                SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                        azimuth_range=[180-AZIMUTHAL_RADIAL - RADIAL_WIDTH , 180-AZIMUTHAL_RADIAL + RADIAL_WIDTH], \n",
    "                                        npt = NPOINTS_RADIAL_AVE)\n",
    "                q0_R2, I0_R2, I0_R2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "                \n",
    "                # Save the line profiles \n",
    "                header = (\"Main Data\\t\\tMirror-Y axis Data\\t\\n\" \n",
    "                        + \"q (Å^-1)\\tI (a.u.)\\tI_err (a.u.)\\tq (Å^-1)\\tI (a.u.)\\tI_err (a.u.)\\n\")\n",
    "                delim = \"\\t\"\n",
    "                kwargs = {\"header\": header, \"delimiter\": delim}\n",
    "                np.savetxt(path_OOP, np.array([q0_OOP, I0_OOP, I0_OOP_err, q0_OOP2, I0_OOP2, I0_OOP2_err]).T, **kwargs)\n",
    "                np.savetxt(path_IP, np.array([q0_IP, I0_IP, I0_IP_err, q0_IP2, I0_IP2, I0_IP2_err]).T, **kwargs)\n",
    "                np.savetxt(path_R, np.array([q0_R, I0_R, I0_R_err, q0_R2, I0_R2, I0_R2_err]).T, **kwargs)\n",
    "                \n",
    "                pbar2.update(1)\n",
    "                plt.close(\"all\")\n",
    "                # break # Only do the first file for now.\n",
    "        pbar.update(1)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Render a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDE: bool = True\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\"\n",
    "\n",
    "import subprocess\n",
    "with tqdm.notebook.tqdm(total=len(samples), position=0, leave=True, desc=\"All Samples\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        ## REQUIRES GLOB SUPPORT BY LIBAVFILTER, NOT INCLUDED IN BUILD BINARIES AVAILALBLE ONLINE.\n",
    "        # img_path_regex = os.path.normpath(os.path.join(giwaxs_img_dir, f\"*.png\"))\n",
    "        # ffmpeg_cmd = f'ffmpeg -framerate 30 -pattern_type glob -i \"{img_path_regex}\" -c:v libx264 -pix_fmt yuv420p \"{vid_path_output}\"'\n",
    "        \n",
    "        ## Results directory for the sample\n",
    "        sample_dir = os.path.join(RESULT_DIR, sample)    \n",
    "        ## Images directory\n",
    "        giwaxs_img_dir = os.path.join(sample_dir, \"giwaxs_flatfielded_images\")\n",
    "        img_list = os.listdir(giwaxs_img_dir)\n",
    "        path_img_list =  os.path.normpath(os.path.join(giwaxs_img_dir, \"img_list.txt\"))\n",
    "        with open(path_img_list, 'w') as f:\n",
    "            for img in img_list:\n",
    "                if not \".png\" in img:\n",
    "                    continue\n",
    "                # f.write(f\"file '{os.path.normpath(os.path.join(giwaxs_img_dir,img))}' duration 0.033333\\n\")\n",
    "                f.write(f\"file '{img}'\\n outpoint 0.1\\n\")\n",
    "        vid_path_output = os.path.normpath(os.path.join(sample_dir, f\"{sample}_angle_of_incidence.mp4\"))\n",
    "        \n",
    "        ## Check if path exists\n",
    "        if os.path.exists(vid_path_output) and not OVERRIDE:\n",
    "            print(\"Video already exists, skipping...\")\n",
    "            pbar.update(1)\n",
    "        else:\n",
    "            if os.path.exists(vid_path_output):\n",
    "                os.remove(vid_path_output)\n",
    "            ## Video generation\n",
    "            ffmpeg_cmd = ['ffmpeg', \n",
    "                        '-f', 'concat',\n",
    "                        #   '-safe', '0'\n",
    "                        '-i', '\"' + path_img_list.replace(\"\\\\\", \"/\")  + '\"',\n",
    "                        '-framerate', '30',\n",
    "                        '-c:v', 'libx264',\n",
    "                        '-shortest',\n",
    "                        '-r', '30',\n",
    "                        '-pix_fmt', 'yuv420p',\n",
    "                        #   '-movflags', 'faststart',\n",
    "                        #   '-vf', 'format=yuv420p',\n",
    "                        '\"' + vid_path_output + '\"']\n",
    "            a = subprocess.run(args=\" \".join(ffmpeg_cmd), capture_output=True)\n",
    "            # Check for errors!\n",
    "            display(a.stderr.splitlines())\n",
    "        pbar.update(1)\n",
    "        if i != len(samples) - 1:\n",
    "            display(\"--------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the video to check for any inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "for i, sample in enumerate(samples):\n",
    "    ## Results directory for the sample\n",
    "    sample_dir = os.path.join(RESULT_DIR, sample)    \n",
    "    vid_path_output = os.path.normpath(os.path.join(sample_dir, f\"{sample}_angle_of_incidence.mp4\"))\n",
    "    v = Video(vid_path_output, embed=True)\n",
    "    display(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Fitting and MC Sampling\n",
    "##### Reset magic and matplotlib to avoid memory leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ipython causes memory leaks when using interactive matplotlib and generating lots of graphs.\n",
    "# https://github.com/ipython/ipython/issues/7270#issuecomment-355276432\n",
    "%matplotlib inline\n",
    "matplotlib.interactive(False)\n",
    "plt.ioff()\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Least squares fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDE: bool = False\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\"\n",
    "\n",
    "USE_GUESS_FIRST: bool = True\n",
    "\"\"\"Whether to use the initial guess for the least squares fit instead of the previous fit result.\"\"\"\n",
    "\n",
    "dataset_fits: list[list[np.ndarray]] = [[np.zeros(\n",
    "                                            (len(datasets[i]), 1 + len(FIT_REGISTER[j][3]) * 2)\n",
    "                                        )\n",
    "                                        for j in range(len(FIT_REGISTER))]\n",
    "                                        for i in range(len(samples))]\n",
    "\"\"\"For each sample, for each peak, the parameters: the angle of incidence and a set of the least squares popt values and errors\"\"\"\n",
    "\n",
    "dataset_beamstop_area: list[np.ndarray] = [np.zeros((len(datasets[i]), 3)) for i in range(len(samples))]\n",
    "\"\"\"For each sample, the angle of incidence, the beamstop area and error\"\"\"\n",
    "\n",
    "with tqdm.notebook.tqdm(total=len(samples), position=2, leave=True, desc=\"Sample\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        # Setup sample dependent variables\n",
    "        sample_fits = dataset_fits[i]\n",
    "        \n",
    "        # Create the results directory for the sample\n",
    "        sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "        if not os.path.isdir(sample_dir):\n",
    "            os.mkdir(sample_dir)\n",
    "        fits_img_dir = os.path.join(sample_dir, \"ls_fit_images\")\n",
    "        if not os.path.isdir(fits_img_dir):\n",
    "            os.mkdir(fits_img_dir)\n",
    "            \n",
    "        # Setup files to store results as they are generated\n",
    "        paths_ls_fits: list[str] = [os.path.join(sample_dir, f\"{sample}-{d}-{q0}_to_{q1}-ls_fits.txt\") for d, (q0,q1), _, _, _, _ in FIT_REGISTER]\n",
    "        paths_beamstop_area: str = os.path.join(sample_dir, f\"{sample}-beamstop_integrated-area.txt\")\n",
    "        \n",
    "        # First line header\n",
    "        header_ls = [(\"\\t\".join([\"Angle of Incidence\"] + [\"Fit Params\"]*len(labels) + [\"Fit Errors\"]*len(labels) + [\"Filename\"])) \n",
    "                    + (\"\\n\" + \"\\t\".join([\"Angle of Incidence\"] + labels + [label + \"_unc\" for label in labels] + [\"Filename\"]))\n",
    "                    for _, _, _, _, _, labels in FIT_REGISTER ]\n",
    "        header_beamstop = \"Angle of Incidence\\tBeamstop Area\\tBeamstop Area Error\\tFilename\"\n",
    "        \n",
    "        # Load the angles of incidence already processed    \n",
    "        prev_ls_datasets: list[dict[str, np.ndarray]] = [{} for _ in range(len(FIT_REGISTER))]\n",
    "        \"\"\"For each peak, a dictionary containing the previous least squares fits for each file.\"\"\"\n",
    "        prev_beamstop_area: dict[str, np.ndarray] = {}\n",
    "        \"\"\"A dictionary containing the previous beamstop area for each file.\"\"\"\n",
    "        \n",
    "        # Iterate over each peak\n",
    "        with tqdm.notebook.tqdm(total=len(datasets[i]), position=1, leave=False, desc=\"Sample Scans\") as pbar2:\n",
    "            for j in range(len(FIT_REGISTER)):\n",
    "                # Get the paths for the peak fit files\n",
    "                path_ls_fits = paths_ls_fits[j]\n",
    "                # Check if the file exists already, and load previous/existing data.\n",
    "                if os.path.isfile(path_ls_fits) and not OVERRIDE:\n",
    "                    # Ignore the UserWarning from numpy for empty files - taken care of below.\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                        prev_ls_fits = np.loadtxt(path_ls_fits, delimiter=\"\\t\", skiprows=2, dtype=str)\n",
    "                    \n",
    "                    if len(prev_ls_fits) == 0:\n",
    "                        # No data in the file, skip loading.\n",
    "                        continue\n",
    "                    if len(prev_ls_fits.shape) == 1:\n",
    "                        prev_ls_fits = prev_ls_fits.reshape(1, -1)\n",
    "                    # Collect common fits and load into memory\n",
    "                    for fname in prev_ls_fits[:, -1]:\n",
    "                        # Check if the file has been MCMC fitted as well, and is loadable data:\n",
    "                        if fname in datasets[i]:\n",
    "                            idx = np.where(prev_ls_fits[:, -1] == fname)[0][0]\n",
    "                            prev_ls_datasets[j][fname] = prev_ls_fits[idx, :-1].astype(float)\n",
    "                    print(f\"Loaded {len(prev_ls_datasets[j])} previous fits for peak {j}.\")\n",
    "            \n",
    "            # Load the beamstop area data\n",
    "            if os.path.isfile(paths_beamstop_area) and not OVERRIDE:\n",
    "                # Ignore the UserWarning from numpy for empty files - taken care of below.\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                    prev_beamstop_data = np.loadtxt(paths_beamstop_area, delimiter=\"\\t\", skiprows=1, dtype=str)\n",
    "                    \n",
    "                if len(prev_beamstop_data) == 0:\n",
    "                    # No data in the file, skip loading.\n",
    "                    pass\n",
    "                else:\n",
    "                    if len(prev_beamstop_data.shape) == 1:\n",
    "                        prev_beamstop_data = prev_beamstop_data.reshape(1, -1)\n",
    "                    for fname in prev_beamstop_data[:, -1]:\n",
    "                        if fname in datasets[i]:\n",
    "                            idx = np.where(prev_beamstop_data[:, -1] == fname)[0][0]\n",
    "                            prev_beamstop_area[fname] = prev_beamstop_data[idx, :-1].astype(float)\n",
    "                    print(f\"Loaded {len(prev_beamstop_area)} previous beamstop areas.\")\n",
    "            \n",
    "            # Begin writing the files\n",
    "            open_ls_fits = [open(paths_ls_fits[j], \"w\") for j in range(len(FIT_REGISTER))]\n",
    "            open_beamstop_area = open(paths_beamstop_area, \"w\")\n",
    "            try: # Ensure the files are closed\n",
    "                # Write the headers and previous data\n",
    "                for j in range(len(FIT_REGISTER)):\n",
    "                    open_ls_fits[j].write(header_ls[j])\n",
    "                    for fname in prev_ls_datasets[j].keys():\n",
    "                        open_ls_fits[j].write(\"\\n\" + \"\\t\".join([str(val) for val in prev_ls_datasets[j][fname]] + [fname]))\n",
    "                \n",
    "                # Write the beamstop area header and previous data\n",
    "                open_beamstop_area.write(header_beamstop)\n",
    "                for fname in prev_beamstop_area.keys():\n",
    "                    open_beamstop_area.write(\"\\n\" + \"\\t\".join([str(val) for val in prev_beamstop_area[fname]] + [fname]))\n",
    "                        \n",
    "                # Loop over all files in the sample dataset\n",
    "                for j, fname in enumerate(datasets[i]):\n",
    "                    # Check if the file has already been processed\n",
    "                    #-----------------------------------------------\n",
    "                    if not OVERRIDE and all(\n",
    "                        [fname in prev_ls_dataset.keys()\n",
    "                        for prev_ls_dataset in prev_ls_datasets]\n",
    "                        ):\n",
    "                        print(f\"File data already exists for all peaks, skipping `{fname}`.\")\n",
    "                        # Reload the existing data and skip.\n",
    "                        for k in range(len(FIT_REGISTER)):\n",
    "                            dataset_fits[i][k][j,:] = prev_ls_datasets[k][fname]\n",
    "                        if fname in prev_beamstop_area:\n",
    "                            dataset_beamstop_area[i][j,:] = prev_beamstop_area[fname]\n",
    "                        pbar.total -= 1\n",
    "                        pbar2.total -= 1\n",
    "                        continue\n",
    "                    \n",
    "                    ### Otherwise, load the data and perform the fits\n",
    "                    # Collect the metadata\n",
    "                    en_idx = fname.find('eV_')\n",
    "                    en = float(fname[en_idx-7:en_idx])\n",
    "                    ai_idx = fname.find(\"_ai\")\n",
    "                    ai = float(fname[ai_idx+3:ai_idx+8])\n",
    "                    \n",
    "                    # Setup the file dependent output paths\n",
    "                    path_det_line_profiles_img = os.path.join(sample_dir, f\"{sample}_line_profile_angles.png\")\n",
    "                    \n",
    "                    # Collect the line profiles from disk\n",
    "                    line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "                    if not os.path.isdir(line_profiles_dir):\n",
    "                        os.mkdir(line_profiles_dir)\n",
    "                    path_OOP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_OOP.txt\")\n",
    "                    path_IP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_IP.txt\")\n",
    "                    path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_R.txt\")\n",
    "                    \n",
    "                    q0_OOP, I0_OOP, I0_OOP_err, _, _, _ = np.loadtxt(path_OOP, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                    q0_IP, I0_IP, I0_IP_err, _, _, _ = np.loadtxt(path_IP, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                    q0_R, I0_R, I0_R_err, _, _, _ = np.loadtxt(path_R, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                    \n",
    "                    # Beamstop region of interest:\n",
    "                    BS_idx = np.where((q0_OOP > VERT_BEAMSTOP[0]) & (q0_OOP < VERT_BEAMSTOP[1]))\n",
    "                    q0_BS, I0_BS, I0_BS_err = q0_OOP[BS_idx], I0_OOP[BS_idx], np.sqrt(I0_OOP[BS_idx])\n",
    "                    beamstop_data = [ai, np.trapezoid(I0_BS, q0_BS), np.trapezoid(np.sqrt(I0_BS), q0_BS)]\n",
    "                    dataset_beamstop_area[i][j, :] = beamstop_data\n",
    "                    open_beamstop_area.write(\"\\n\" + \"\\t\".join([str(param) for param in (beamstop_data + [fname])]))\n",
    "                    \n",
    "                    # Perform a fit to the data for each listed peak.\n",
    "                    for k, (d, peak, fit_fn, guess, bounds, labels) in enumerate(FIT_REGISTER):\n",
    "                        # Setup the data\n",
    "                        if d == \"OOP\":\n",
    "                            q0, I0, I0_err = q0_OOP, I0_OOP, I0_OOP_err\n",
    "                        elif d == \"IP\":\n",
    "                            q0, I0, I0_err = q0_IP, I0_IP, I0_IP_err\n",
    "                        elif d == \"RAD\":\n",
    "                            q0, I0, I0_err = q0_R, I0_R, I0_R_err\n",
    "                        \n",
    "                        # Remove rows with NaN or zero values and collect the fit subset data\n",
    "                        idx = np.where((~np.isnan(I0)) & (I0 != 0) & (q0 > peak[0]) & (q0 < peak[1]))\n",
    "                        q, I, I_err = q0[idx], I0[idx], I0_err[idx]\n",
    "                            \n",
    "                        # Define the output paths\n",
    "                        fits_img_peak_dir = os.path.join(fits_img_dir, f\"{d}-{peak[0]}_to_{peak[1]}_perÅ\")\n",
    "                        if not os.path.isdir(fits_img_peak_dir):\n",
    "                            os.mkdir(fits_img_peak_dir)\n",
    "                        path_fit_img = os.path.join(fits_img_peak_dir, f\"{sample}_fit-{d}-peak_{peak[0]}-{peak[1]}_perÅ-{en:0.2f}eV-{ai:0.3f}deg.png\")\n",
    "                        \n",
    "                        # Perform a least squres fit\n",
    "                        if USE_GUESS_FIRST:\n",
    "                            try: \n",
    "                                popt, pcov = curve_fit(fit_fn, q, I, sigma=I_err, p0=guess, maxfev=100000, bounds=bounds)\n",
    "                            except RuntimeError as e:\n",
    "                                if j > 0:\n",
    "                                    print(\"Re-attempting fit again with previous fit instead of initial guess.\")\n",
    "                                    popt, pcov = curve_fit(fit_fn, q, I, sigma=I_err, p0=dataset_fits[i][k][j-1, 1:1+len(guess)], maxfev=100000, bounds=bounds)\n",
    "                                else:\n",
    "                                    raise e\n",
    "                        else:\n",
    "                            try:\n",
    "                                if j > 0:\n",
    "                                    popt, pcov = curve_fit(fit_fn, q, I, sigma=I_err, p0=dataset_fits[i][k][j-1, 1:1+len(guess)], maxfev=100000, bounds=bounds)\n",
    "                                else:\n",
    "                                    popt, pcov = curve_fit(fit_fn, q, I, sigma=I_err, p0=guess, maxfev=100000, bounds=bounds)\n",
    "                            except RuntimeError as e:\n",
    "                                print(\"Re-attempting fit again with initial guess instead of previous fit.\")\n",
    "                                popt, pcov = curve_fit(fit_fn, q, I, sigma=I_err, p0=guess, maxfev=100000, bounds=bounds)\n",
    "                        data_line = np.r_[[ai], popt, np.sqrt(np.diag(pcov))]\n",
    "                        dataset_fits[i][k][j, :] = data_line\n",
    "                        open_ls_fits[k].write(\"\\n\" + \"\\t\".join([str(val) for val in data_line] + [fname])) #Save the data\n",
    "\n",
    "                        # Plot the fit\n",
    "                        fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "                        ax.plot(q, I, label=\"Data\")\n",
    "                        ax.fill_between(q, I - I_err, I + I_err, alpha=0.2)\n",
    "                        ax.plot(q, fit_fn(q, *popt), label=\"Least Squares Fit\")\n",
    "                        # ax.fill_between(q, fit_fn(q, *popt - np.sqrt(np.diag(pcov))), fit_fn(q, *popt + np.sqrt(np.diag(pcov))), alpha=0.1)\n",
    "                        ax.set_ylim(0, 1.1 * np.max(I))\n",
    "                        ax.set_title(f\"{sample}\\n{en} eV - {ai} deg - Peak {peak[0]}-{peak[1]}\")\n",
    "                        fig.savefig(path_fit_img)\n",
    "                        fig.tight_layout()\n",
    "                        plt.close() # Save memory\n",
    "                    plt.close(\"all\")\n",
    "                    pbar2.update(1)\n",
    "                    \n",
    "                # Close the files\n",
    "                for f1 in open_ls_fits:\n",
    "                    f1.close()\n",
    "                open_beamstop_area.close()\n",
    "                \n",
    "                # Generate final graphics for the angle of incidence fits to each peak.\n",
    "                #-----------------------------------------------------------------------\n",
    "                for k, (d, peak, fit_fn, guess, bounds, labels) in enumerate(FIT_REGISTER):\n",
    "                    path_ls_summary = os.path.join(sample_dir, f\"{sample}_ls_fits_summary.png\")\n",
    "                \n",
    "                    # Load the data\n",
    "                    ls_fits = dataset_fits[i][k]\n",
    "                    # Re-index the data\n",
    "                    angles = ls_fits[:, 0]\n",
    "                    ls_popt = ls_fits[:, 1:1+len(labels)]\n",
    "                    ls_errors = ls_fits[:, 1+len(labels):]\n",
    "                    \n",
    "                    # Plot the LS data\n",
    "                    fig, ax = plt.subplots(len(labels), 1, figsize=(8, 2*len(labels)))\n",
    "                    for z in range(len(labels)):\n",
    "                        ax[z].plot(angles, ls_popt[:, z], label=labels[z])\n",
    "                        ax[z].fill_between(angles, ls_popt[:, z] - ls_errors[:, z], ls_popt[:, z] + ls_errors[:, z], alpha=0.1)\n",
    "                        ax[z].set_ylabel(labels[z])\n",
    "                    fig.suptitle(f\"{sample}\\n{d} Peak {peak[0]:0.2f} to {peak[1]:0.2f} - Least Squares Fit Summary\")\n",
    "                    fig.savefig(path_ls_summary)\n",
    "                    fig.tight_layout()\n",
    "                    plt.close()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample}: {e}\")\n",
    "                # Close the files\n",
    "                for f1 in open_ls_fits:\n",
    "                    f1.close()\n",
    "                open_beamstop_area.close()\n",
    "                raise e\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional close command in case files are still open\n",
    "for f1 in open_ls_fits:\n",
    "    f1.close()\n",
    "open_beamstop_area.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCMC_SAMPLING : bool = False\n",
    "\"\"\"Whether to use MCMC sampling to fit the data or not\"\"\"\n",
    "N: int = 200\n",
    "\"\"\"The number of walkers to use in the MCMC sampling\"\"\"\n",
    "M = 1000\n",
    "\"\"\"The default number of steps to use in the MCMC sampling\"\"\"\n",
    "\n",
    "dataset_MC_fits: list[list[np.ndarray]] = [[np.zeros(\n",
    "                                                (len(datasets[i]), 1 + len(FIT_REGISTER[j][3]) * 3)\n",
    "                                            )\n",
    "                                            for j in range(len(FIT_REGISTER))]\n",
    "                                            for i in range(len(samples))]\n",
    "\"\"\"For each sample, for each peak, the parameters: the angle of incidence and a set of the MCMC opt values and lb/ub\"\"\"\n",
    "\n",
    "\n",
    "OVERRIDE: bool = False\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\"\n",
    "\n",
    "with tqdm.notebook.tqdm(total=np.sum([len(datasets[i]) for i in range(len(datasets))]), position=2, leave=True, desc=\"Sample\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        # Setup sample dependent variables\n",
    "        sample_fits = dataset_fits[i]\n",
    "        sample_MC_fits = dataset_MC_fits[i]\n",
    "        \n",
    "        # Create the results directory for the sample\n",
    "        sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "        if not os.path.isdir(sample_dir):\n",
    "            os.mkdir(sample_dir)\n",
    "        mcmc_img_dir = os.path.join(sample_dir, \"mcmc_fit_images\")\n",
    "        if not os.path.isdir(mcmc_img_dir):\n",
    "            os.mkdir(mcmc_img_dir)\n",
    "            \n",
    "        # Setup files to store results as they are generated\n",
    "        paths_mcmc_fits: list[str] = [os.path.join(sample_dir, f\"{sample}-{d}-{q0}_to_{q1}-mcmc_fits.txt\") for d, (q0,q1), _, _, _, _ in FIT_REGISTER]\n",
    "        \n",
    "        # First line header\n",
    "        header_mcmc = [(\"\\t\".join([\"Angle of Incidence\"] + [\"MC Fit Params\"]*len(labels) + [\"MC LB Errors\"]*len(labels) + [\"MC UB Errors\"]*len(labels) + [\"Filename\"]))\n",
    "                    + (\"\\n\" + \"\\t\".join([\"Angle of Incidence\"] + labels + [label + \"_unc_lb\" for label in labels] + [label + \"_unc_ub\" for label in labels] + [\"Filename\"]))\n",
    "                    for _, _, _, _, _, labels in FIT_REGISTER]\n",
    "                \n",
    "        # Load the angles of incidence already processed    \n",
    "        prev_mcmc_datasets: list[dict[str, np.ndarray]] = [{} for _ in range(len(FIT_REGISTER))]\n",
    "        \"\"\"For each peak, a dictionary containing the previous mcmc fits for each file.\"\"\"\n",
    "        \n",
    "        # Iterate over each peak\n",
    "        with tqdm.notebook.tqdm(total=len(datasets[i]), position=1, leave=False, desc=\"Sample Scans\") as pbar2:\n",
    "            for j in range(len(FIT_REGISTER)):\n",
    "                # Get the paths for the peak fit files\n",
    "                path_mcmc_fits = paths_mcmc_fits[j]\n",
    "                # Check if the file exists already, and load previous/existing data.\n",
    "                if os.path.isfile(path_mcmc_fits) and not OVERRIDE:\n",
    "                    # Ignore the UserWarning from numpy for empty files - taken care of below.\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                        prev_mcmc_fits = np.loadtxt(path_mcmc_fits, delimiter=\"\\t\", skiprows=2, dtype=str)\n",
    "                    \n",
    "                    if len(prev_mcmc_fits) == 0:\n",
    "                        # No data in the file, skip loading.\n",
    "                        continue\n",
    "                    if len(prev_mcmc_fits.shape) == 1:\n",
    "                        prev_mcmc_fits = prev_mcmc_fits.reshape(1, -1)\n",
    "                    # Collect common fits and load into memory\n",
    "                    for fname in prev_mcmc_fits[:, -1]:\n",
    "                        # Check if the file has been MCMC fitted as well, and is loadable data:\n",
    "                        if fname in datasets[i]:\n",
    "                            idx = np.where(prev_mcmc_fits[:, -1] == fname)[0][0]\n",
    "                            prev_mcmc_datasets[j][fname] = prev_mcmc_fits[idx, :-1].astype(float)\n",
    "                    print(f\"Loaded {len(prev_mcmc_datasets[j])} previous fits for peak {j}.\")\n",
    "            \n",
    "            \n",
    "            # Begin writing the files\n",
    "            open_mcmc_fits = [open(paths_mcmc_fits[j], \"w\") for j in range(len(FIT_REGISTER))]\n",
    "            try:\n",
    "                # Write the headers and previous data\n",
    "                for j in range(len(FIT_REGISTER)):\n",
    "                    open_mcmc_fits[j].write(header_mcmc[j])\n",
    "                    for fname in prev_mcmc_datasets[j].keys():\n",
    "                        open_mcmc_fits[j].write(\"\\n\" + \"\\t\".join([str(val) for val in prev_mcmc_datasets[j][fname]] + [fname]))\n",
    "                        \n",
    "                # Loop over all files in the sample dataset\n",
    "                for j, fname in enumerate(datasets[i]):\n",
    "                    # Check if the file has already been processed\n",
    "                    #-----------------------------------------------\n",
    "                    if not OVERRIDE and all(\n",
    "                        [fname in prev_mcmc_dataset.keys()\n",
    "                        for prev_mcmc_dataset in prev_mcmc_datasets]\n",
    "                        ):\n",
    "                        print(f\"File data already exists for all peaks, skipping `{fname}`.\")\n",
    "                        # Reload the existing data and skip.\n",
    "                        for k in range(len(FIT_REGISTER)):\n",
    "                            dataset_MC_fits[i][k][j,:] = prev_mcmc_datasets[k][fname]\n",
    "                        \n",
    "                        pbar.total -= 1\n",
    "                        pbar2.total -= 1\n",
    "                        continue\n",
    "                    \n",
    "                    ### Otherwise, load the data and perform the fits\n",
    "                    # Collect the metadata\n",
    "                    en_idx = fname.find('eV_')\n",
    "                    en = float(fname[en_idx-7:en_idx])\n",
    "                    ai_idx = fname.find(\"_ai\")\n",
    "                    ai = float(fname[ai_idx+3:ai_idx+8])\n",
    "                    \n",
    "                    # Collect the line profiles from disk\n",
    "                    line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "                    path_OOP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_OOP.txt\")\n",
    "                    path_IP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_IP.txt\")\n",
    "                    path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_R.txt\")\n",
    "                    \n",
    "                    q0_OOP, I0_OOP, I0_OOP_err, _, _, _ = np.loadtxt(path_OOP, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                    q0_IP, I0_IP, I0_IP_err, _, _, _ = np.loadtxt(path_IP, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                    q0_R, I0_R, I0_R_err, _, _, _ = np.loadtxt(path_R, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                    \n",
    "                    # Perform a fit to the data for each listed peak.\n",
    "                    for k, (d, peak, fit_fn, guess, bounds, labels) in enumerate(FIT_REGISTER):\n",
    "                        # Setup the data\n",
    "                        if d == \"OOP\":\n",
    "                            q0, I0, I0_err = q0_OOP, I0_OOP, I0_OOP_err\n",
    "                        elif d == \"IP\":\n",
    "                            q0, I0, I0_err = q0_IP, I0_IP, I0_IP_err\n",
    "                        elif d == \"RAD\":\n",
    "                            q0, I0, I0_err = q0_R, I0_R, I0_R_err\n",
    "                        \n",
    "                        # Remove rows with NaN or zero values and collect the fit subset data\n",
    "                        idx = np.where((~np.isnan(I0)) & (I0 != 0) & (q0 > peak[0]) & (q0 < peak[1]))\n",
    "                        q, I, I_err = q0[idx], I0[idx], I0_err[idx]\n",
    "                            \n",
    "                        # Define the output paths\n",
    "                        mcmc_img_peak_dir = os.path.join(mcmc_img_dir, f\"{d}-{peak[0]}_to_{peak[1]}_perÅ\")\n",
    "                        if not os.path.isdir(mcmc_img_peak_dir):\n",
    "                            os.mkdir(mcmc_img_peak_dir)\n",
    "                        path_mc_chain = os.path.join(mcmc_img_peak_dir, f\"mcmc_chain-{sample}-peak_{peak[0]}-{peak[1]}_perÅ-{en:0.2f}eV-{ai:0.3f}deg-{d}.png\")\n",
    "                        path_mc_corner_plot = os.path.join(mcmc_img_peak_dir, f\"corner_plot-{sample}-peak_{peak[0]}-{peak[1]}_perÅ-{en:0.2f}eV-{ai:0.3f}deg-{d}.png\")\n",
    "                        \n",
    "                        # Obtain the least squres fit\n",
    "                        popt = dataset_fits[i][k][j, 1:1+len(guess)]\n",
    "\n",
    "                        # # Plot the fit\n",
    "                        # fig, ax = plt.subplots(1, 1, figsize=(4, 2.5))\n",
    "                        # ax.plot(q, I, label=\"Data\")\n",
    "                        # ax.fill_between(q, I - I_err, I + I_err, alpha=0.2)\n",
    "                        # ax.plot(q, fit_fn(q, *popt), label=\"Least Squares Fit\")\n",
    "                        # # ax.fill_between(q, fit_fn(q, *popt - np.sqrt(np.diag(pcov))), fit_fn(q, *popt + np.sqrt(np.diag(pcov))), alpha=0.1)\n",
    "                        # ax.set_ylim(0, 1.1 * np.max(I))\n",
    "                        # ax.set_title(f\"{sample}\\n{en} eV - {ai} deg - Peak {peak[0]}-{peak[1]}\")\n",
    "                        # fig.savefig(path_fit_img)\n",
    "                        # plt.close() # Save memory\n",
    "                        \n",
    "                        # Create the MCMC fitter\n",
    "                        init_position = (popt * np.ones((N, len(popt))) # Add some small random magnitude variance on each parameter.\n",
    "                                        * (1 + (1.0e-2 * np.random.randn(N, len(popt)) - 5.0e-3))) \n",
    "                        for b in range(len(bounds[0])):\n",
    "                            lb, ub = bounds[0][b], bounds[1][b]\n",
    "                            if lb is not None:\n",
    "                                init_position[:,b] = np.clip(init_position[:,b], lb, np.inf)\n",
    "                            if ub is not None:\n",
    "                                init_position[:,b] = np.clip(init_position[:,b], -np.inf, ub)\n",
    "                                \n",
    "                        nwalkers, ndim = init_position.shape\n",
    "                        sampler = emcee.EnsembleSampler(\n",
    "                            nwalkers, ndim, LOG_PROBABILITY_FNS[k], args=(q, I, I_err, LOG_PRIOR_FNS[k], LOG_LIKELIHOOD_FNS[k])\n",
    "                        )\n",
    "                        # Run for M steps\n",
    "                        sampler.run_mcmc(init_position, M, progress=True, progress_kwargs={\"leave\": False, \"position\":0})\n",
    "                        # Check the autocorrelation time, and run for more steps if necessary.\n",
    "                        max_tau = None\n",
    "                        sample_attempts = 1\n",
    "                        while max_tau is None:\n",
    "                            try:\n",
    "                                tau = sampler.get_autocorr_time()\n",
    "                                max_tau = np.nanmax(tau)\n",
    "                            except emcee.autocorr.AutocorrError as e:\n",
    "                                if np.isnan(e.tau).all():\n",
    "                                    print(\"Nan tau, running for 1000 more steps.\")\n",
    "                                    sampler.run_mcmc(None, 1000, progress=False, progress_kwargs={\"leave\": False, \"position\":0})\n",
    "                                else:\n",
    "                                    tau_estimate = int(np.nanmax(e.tau))\n",
    "                                    if tau_estimate * 50 < 150000:\n",
    "                                        print(f\"Estimated max tau: {tau_estimate}, running for {50*tau_estimate} more steps.\")\n",
    "                                        sampler.run_mcmc(None, tau_estimate * 50, progress=False, progress_kwargs={\"leave\": False, \"position\":0})\n",
    "                                    elif len(sampler.get_chain()) > 2 * tau_estimate:\n",
    "                                        max_tau = tau_estimate\n",
    "                                    else:\n",
    "                                        print(\"Tau estimate unreasonably large.\")\n",
    "                                        sample_attempts = 11\n",
    "                            sample_attempts += 1\n",
    "                            if sample_attempts > 10: # Break if the sample attempts exceed 10\n",
    "                                break\n",
    "                            \n",
    "                        # Skip the MCMC fit if the sample attempts exceed 10.\n",
    "                        if sample_attempts > 10:\n",
    "                            print(\"Skipping MCMC fit, using nans.\")\n",
    "                            dataset_MC_fits[i][k][j,:] = np.full((1 + len(labels)*3, ), np.nan)\n",
    "                            pbar.update(1)\n",
    "                            pbar2.update(1)\n",
    "                            continue\n",
    "                        \n",
    "                        # Create a default max_tau value if it is not found.\n",
    "                        if np.isnan(max_tau):\n",
    "                            max_tau = 0\n",
    "                            \n",
    "                        # Get the samples and discard some for the burn-in period\n",
    "                        MC_samples = sampler.get_chain()\n",
    "                        flat_MC_samples = sampler.get_chain(discard=int(max_tau * 3), thin=5, flat=True)\n",
    "\n",
    "                        # Save the MCMC fit results\n",
    "                        percentiles = [np.percentile(MC_samples[:, :, z], [16, 50, 84]) for z in range(MC_samples.shape[2])] #Sampling at 1 sigma percentiles\n",
    "                        values = np.array([p[1] for p in percentiles])\n",
    "                        gauss_error = np.array([[p[0] - p[1], p[2] - p[1]] for p in percentiles])\n",
    "                        \n",
    "                        mc_data_line = np.r_[[ai], values, gauss_error[:,0], gauss_error[:,1]]\n",
    "                        # Save the MC data to the open file\n",
    "                        open_mcmc_fits[k].write(\"\\n\" + \"\\t\".join([str(val) for val in mc_data_line] + [fname]))\n",
    "                        dataset_MC_fits[i][k][j, :] = mc_data_line\n",
    "                        \n",
    "                        # Plot the chain output\n",
    "                        fig, axes = plt.subplots(len(labels), figsize=(10, 2*len(labels)), sharex=True)\n",
    "                        for z in range(ndim):\n",
    "                            ax = axes[z]\n",
    "                            ax.plot(MC_samples[:, :, z], \"k\", alpha=0.3)\n",
    "                            ax.set_xlim(0, len(MC_samples))\n",
    "                            ax.set_ylabel(labels[z])\n",
    "                            ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "                        axes[-1].set_xlabel(\"step number\")\n",
    "                        fig.suptitle(f\"Chain Output - Corr time = {max_tau:0.2f} steps\")\n",
    "                        fig.savefig(path_mc_chain)\n",
    "                        plt.close()\n",
    "                    \n",
    "                        # Create a corner plot        \n",
    "                        fig = corner.corner(\n",
    "                            flat_MC_samples, labels=labels, truths=popt\n",
    "                        )\n",
    "                        fig.savefig(path_mc_corner_plot)\n",
    "                        plt.close()\n",
    "                    plt.close(\"all\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                    pbar2.update(1)\n",
    "                    \n",
    "                # Close the files\n",
    "                for f1, f2 in zip(open_ls_fits, open_mcmc_fits):\n",
    "                    f1.close()\n",
    "                    f2.close()\n",
    "                open_beamstop_area.close()\n",
    "                \n",
    "                # Generate final graphics for the angle of incidence fits to each peak.\n",
    "                #-----------------------------------------------------------------------\n",
    "                for k, (d, peak, fit_fn, guess, bounds, labels) in enumerate(FIT_REGISTER):\n",
    "                    path_ls_summary = os.path.join(sample_dir, f\"{sample}-peak_{peak[0]}-{peak[1]}_perÅ-ls_fits_summary.png\")\n",
    "                    path_mcmc_summary = os.path.join(sample_dir, f\"{sample}-peak_{peak[0]}-{peak[1]}_perÅ-mcmc_fits_summary.png\")\n",
    "                    path_overlap_summary = os.path.join(sample_dir, f\"{sample}-peak_{peak[0]}-{peak[1]}_perÅ-overlap_fits_summary.png\")\n",
    "                \n",
    "                    # Load the data\n",
    "                    ls_fits = dataset_fits[i][k]\n",
    "                    MC_fits = dataset_MC_fits[i][k]\n",
    "                    # Re-index the data\n",
    "                    angles = ls_fits[:, 0]\n",
    "                    ls_popt = ls_fits[:, 1:1+len(labels)]\n",
    "                    ls_errors = ls_fits[:, 1+len(labels):]\n",
    "                    MC_popt = MC_fits[:, 1:1+len(labels)]\n",
    "                    MC_lb_errors = MC_fits[:, 1+len(labels):1+2*len(labels)]\n",
    "                    MC_ub_errors = MC_fits[:, 1+2*len(labels):]\n",
    "                    \n",
    "                    # Plot the MCMC data\n",
    "                    fig, ax = plt.subplots(len(labels), 1, figsize=(8, 2*len(labels)))\n",
    "                    for z in range(len(labels)):\n",
    "                        ax[z].plot(angles, MC_popt[:, z], label=labels[z])\n",
    "                        ax[z].fill_between(angles, MC_popt[:, z] - MC_lb_errors[:, z], MC_popt[:, z] + MC_ub_errors[:, z], alpha=0.1)\n",
    "                        ax[z].set_ylabel(labels[z])\n",
    "                    fig.suptitle(f\"{sample}\\n{d} Peak {peak[0]:0.2f} to {peak[1]:0.2f} - MCMC Fit Summary\")\n",
    "                    fig.savefig(path_mcmc_summary)\n",
    "                    plt.close()\n",
    "                    \n",
    "                    # Plot the overlap data\n",
    "                    fig, ax = plt.subplots(len(labels), 1, figsize=(8, 2*len(labels)))\n",
    "                    for z in range(len(labels)):\n",
    "                        ax[z].plot(angles, ls_popt[:, z], label=\"Least Squares Fit\")\n",
    "                        ax[z].fill_between(angles, ls_popt[:, z] - ls_errors[:, z], ls_popt[:, z] + ls_errors[:, z], alpha=0.1)\n",
    "                        ax[z].plot(angles, MC_popt[:, z], label=\"MCMC Fit\")\n",
    "                        ax[z].fill_between(angles, MC_popt[:, z] - MC_lb_errors[:, z], MC_popt[:, z] + MC_ub_errors[:, z], alpha=0.1)\n",
    "                        ax[z].set_ylabel(labels[z])\n",
    "                        ax[z].legend()\n",
    "                    fig.suptitle(f\"{sample}\\n{d} Peak {peak[0]:0.2f} to {peak[1]:0.2f} - MCMC/LS Fit Overlap Summary\")\n",
    "                    fig.savefig(path_overlap_summary)\n",
    "                    plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample}: {e}\")\n",
    "                # Close the files\n",
    "                for f2 in open_mcmc_fits:\n",
    "                    f2.close()\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f2 in open_mcmc_fits:\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\"Stop Here.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
