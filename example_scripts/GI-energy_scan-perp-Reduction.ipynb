{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMI Data Reduction for Grazing-Incidence Energy Scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "##### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Prevent numpy from using multiple threads when using emcee multiprocessing.\n",
    "# These packages should all be installed if the procedure was followed\n",
    "%matplotlib inline\n",
    "import matplotlib, matplotlib.pyplot as plt, matplotlib.colors as mplc\n",
    "matplotlib.interactive(True)\n",
    "plt.ion()\n",
    "plt.ioff()\n",
    "from smi_analysis import SMI_beamline\n",
    "import numpy as np, numpy.typing as npt\n",
    "import pandas as pd\n",
    "import fabio\n",
    "import logging\n",
    "import scipy.constants as const\n",
    "import time\n",
    "import corner\n",
    "from typing import Literal, Callable\n",
    "import emcee\n",
    "import tqdm, tqdm.notebook # For progress bars\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "\n",
    "# Setup options\n",
    "fabio.TiffIO.logger.setLevel(logging.ERROR)\n",
    "pd.set_option(\"display.width\", 1000) #display large filenames\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "en2wav = lambda en: const.h * const.c / (const.e * en)\n",
    "\"\"\"Function to convert energy (eV) to wavelength\"\"\"\n",
    "en2wav(2.45e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE = \"C:/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimental configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry: Literal['Transmission'] | Literal['Reflection'] = 'Reflection'\n",
    "\"\"\"The measurement geometry\"\"\"\n",
    "energy: float = 2.45e3\n",
    "\"\"\"The energy (keV) at which the scan is performed\"\"\"\n",
    "wavelength: float = en2wav(energy)\n",
    "\"\"\"The wavelength corresponding to `energy`.\"\"\" \n",
    "beamstop_type: Literal[\"pindiode\"] | Literal['rod'] | None = None # Angle selected away from beam so no need for beamstop.\n",
    "\"\"\"The beamstop type\"\"\"\n",
    "incident_angle = 0\n",
    "\"\"\"The default incident angle (varies) in degrees\"\"\"\n",
    "\n",
    "#WAXS\n",
    "detector_waxs: Literal['Pilatus900kw'] | Literal['Pilatus1m'] = 'Pilatus900kw'\n",
    "\"\"\"Type of WAXS/SAXS detector\"\"\"\n",
    "sdd_waxs: float | int = 280 # In mm\n",
    "\"\"\"Sample to detector distance in millimeters\"\"\"\n",
    "center_waxs: tuple[int|float, int|float] = (#208, 97)\n",
    "                                            223, 97)\n",
    "\"\"\"Coordinates of the beam centre at 0 degrees, for the middle detector strip\"\"\"\n",
    "bs_pos_waxs: list[tuple[int, int]] = [(0, 0),\n",
    "                                      (0, 0), \n",
    "                                      (0, 0)\n",
    "                                     ]\n",
    "\"\"\"The position of the center of the beam stop for each detector angle; [0,0] implies not measured. \n",
    "This coordinate is relative to the stitch of the 3 detector strips.\"\"\"\n",
    "detector_angles: list[int | float] | npt.NDArray[np.float64 | np.int_] = np.deg2rad(np.array([17.7]) - 0.06) #0.06 is the correction for the WAXS 0 deg detector position\n",
    "\"\"\"The angles of the detector in radians. \n",
    "May need to include corrections (-0.06 degs at 0, -0.36 at 20 deg) for position offsets.\"\"\"\n",
    "\n",
    "\n",
    "display(pd.DataFrame([\n",
    "    (\"Geometry\", geometry),\n",
    "    (\"Energy (keV)\", energy),\n",
    "    (\"Wavelength (nm)\", wavelength * 1e9),\n",
    "    (\"Sample to Detector Distance (mm)\", sdd_waxs),\n",
    "    (\"Beamstop Type\", beamstop_type),\n",
    "    (\"Incident Angle (deg)\", incident_angle),\n",
    "    (\"Detector Type\", detector_waxs),\n",
    "    (\"Center Coords\", center_waxs),\n",
    "    (\"Beamstop Coords\", bs_pos_waxs),\n",
    "    (\"Detector Angles\", detector_angles)\n",
    "], columns=[\"Parameter\", \"Value\"]))\n",
    "\n",
    "#Test the configuration can be loaded!\n",
    "SMI_waxs = SMI_beamline.SMI_geometry(geometry = geometry,\n",
    "                                     wav = wavelength,\n",
    "                                     sdd = sdd_waxs,\n",
    "                                     alphai = incident_angle,\n",
    "                                     detector = detector_waxs,\n",
    "                                     center = center_waxs,\n",
    "                                     bs_pos = bs_pos_waxs,\n",
    "                                     bs_kind = beamstop_type,\n",
    "                                     det_angles=detector_angles)\n",
    "\n",
    "\n",
    "angles = [detector_angles[0] - np.deg2rad(7.47), detector_angles[0], detector_angles[0] + np.deg2rad(7.47)]\n",
    "SMI_waxs.calculate_integrator_gi(angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatfield Data\n",
    "Data to normalise the detector pixels and remove background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Past beamline data for flat fielding (normalizing default pixel intensities)\n",
    "# Note this is done at 2478eV, not all energies.\n",
    "\n",
    "# # 2024 Cycle 2 Flatfielding\n",
    "# CYCLE_FLAT = '2024_3'\n",
    "# PROPOSAL_FLAT= '314483-Freychet-Flatfielding'\n",
    "# FLAT_FILE = 'GF_flatfield_Sedge_2450uhighg1600_WZY11_wa30deg_2478eV_20s_id701601_000000_WAXS.tif'\n",
    "\n",
    "# 2024 Cycle 3 Flatfielding\n",
    "PROPOSAL_FLAT= '314483_Freychet_08'\n",
    "FLAT_FILE = 'GF_GF_flatfield_Sedge_2450uhighg1600_Y2_06_2477.00eV_wa20deg_id807229_000000_WAXS.tif'\n",
    "\n",
    "# Compile and load the flatfield path\n",
    "# FLAT_DIR_PATH = f'D:/Datasets/2024-09 SMI/{PROPOSAL_FLAT}/900KW/'\n",
    "FLAT_DIR_PATH = f'{DRIVE}Datasets/2024-09 SMI/{PROPOSAL_FLAT}/900KW/'\n",
    "flatfield: npt.NDArray = np.rot90(fabio.open(os.path.join(FLAT_DIR_PATH, FLAT_FILE)).data, 1)\n",
    "\n",
    "fig,ax = plt.subplots(2,1, sharex=True, sharey=True, figsize=(8,5))\n",
    "p = 99.9\n",
    "percentile = np.percentile(flatfield, p) #99.9th percentile\n",
    "ax[0].imshow(np.rot90(flatfield, 3), vmin=0, vmax=percentile, interpolation=None)\n",
    "ax[0].set_title(\"Flatfielding Data\")\n",
    "\n",
    "erronous =  (flatfield > percentile) * 1.0\n",
    "ax[1].imshow(np.rot90(erronous,3), vmin=0, vmax=np.max(erronous)/5, interpolation=None)\n",
    "ax[1].set_title(f\"{p}th percentile pixels\")\n",
    "fig.tight_layout()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra functions for SMI Beamline Masking\n",
    "\n",
    "##### Detector Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_detector_mask_to_array(\n",
    "    mask: npt.NDArray = np.zeros((1475, 195), dtype=bool)\n",
    ") -> npt.NDArray[np.bool]:\n",
    "    \"\"\"Sets an array mask for bad pixels; should only be applied to the middle column array\"\"\"\n",
    "    mask[1254:1256, 47] = True\n",
    "    mask[979:1050, 0:100] = True\n",
    "    mask[967, 67] = True\n",
    "    mask[490:555, 100:] = True\n",
    "    mask[1231:1233, 174] = True\n",
    "    mask[1414: 1416, 179] = True\n",
    "    mask[858:860, 5] = True\n",
    "    mask[414, 6] = True\n",
    "    mask[394, 138] = True\n",
    "    mask[364:366, 41] = True\n",
    "    mask[364:366, 96] = True\n",
    "    mask[304:306, 96:98] =  True\n",
    "    mask[988, 188:194] = True\n",
    "    mask[:, 1] = True\n",
    "    mask[473, 20] = True\n",
    "    mask[98, 5] = True\n",
    "    mask[141, 111] = True\n",
    "    mask[240:300, 0:50] = True\n",
    "    mask[300:425, 125:] = True\n",
    "    mask[181:183, 97:99] = True\n",
    "    mask[553:555, 99:100] = True\n",
    "    return mask\n",
    "\n",
    "def apply_boundary_mask_to_array(\n",
    "    mask: npt.NDArray = np.zeros((1475, 195), dtype=bool)\n",
    ") -> npt.NDArray[np.bool]:\n",
    "    \"\"\"Sets an array mask for the boundary pixels; should only be applied to the middle column array\"\"\"\n",
    "    mask[0, :] = True\n",
    "    mask[-1, :] = True\n",
    "    mask[:, 0] = True\n",
    "    mask[:, -1] = True\n",
    "    return mask\n",
    "\n",
    "def apply_detector_mask(geom: SMI_beamline.SMI_geometry) -> None:\n",
    "    \"\"\"Applies a pre-defined mask for the bad pixels in the SMI beamline\"\"\"\n",
    "    for i, mask in enumerate(geom.masks):\n",
    "        # Dead pixels in the 2nd detector strip.\n",
    "        if i%3 == 1: # For multiple WAXS images, always masks the 2nd strip.\n",
    "            apply_detector_mask_to_array(mask)\n",
    "        # Add a mask on the boundary\n",
    "        apply_boundary_mask_to_array(mask)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flatield Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLATFIELD_PERCENTILE = 99.5\n",
    "\"\"\"The percentile of the flatfield data to mask.\"\"\"\n",
    "# SAMPLE_PERCENTILE = 100.00\n",
    "SAMPLE_PERCENTILE = 100.00\n",
    "\"\"\"The percentile of the real data to mask.\"\"\"\n",
    "\n",
    "# For flatfielding, ignore/mask reigons between detector pixels.\n",
    "FLATFIELD_SLICES = [slice(0, 195),  # Ign. flatfield above first frame\n",
    "                    slice(211, 406),# Ign. flatfield outside middle  \n",
    "                    slice(-195, None)] # Ign. flatfield below first frame\n",
    "            \n",
    "def flatfield_mask(flatfield: npt.NDArray = flatfield, \n",
    "                   percentile: float = FLATFIELD_PERCENTILE, \n",
    "                   min: float | int = 0) -> npt.NDArray[np.bool]:\n",
    "        \"\"\"\n",
    "        Returns a mask of flatfield data as a boolean numpy array.\n",
    "        \n",
    "        Masks pixels above the `percentile` (by default 99.9)\n",
    "        and values less than `min` (by default 1).\n",
    "        \"\"\"\n",
    "        # Calculate the 99.9th percentile of the total flatfield data\n",
    "        p = np.percentile(flatfield, percentile) #99.9th percentile\n",
    "        erronous =  flatfield > p\n",
    "        # Also mask pixels well away from the standard deviation\n",
    "        mean = np.mean(flatfield)\n",
    "        std = np.std(flatfield)\n",
    "        erronous |= flatfield > mean + 5*std\n",
    "        \n",
    "        # Also mask negative and zero pixels\n",
    "        negative = flatfield < min\n",
    "        # Return the overlap of the erronous and negative masks.\n",
    "        return erronous | negative\n",
    "\n",
    "def apply_flatfield(geom: SMI_beamline.SMI_geometry, \n",
    "                    flatfield: npt.NDArray, \n",
    "                    flat_percentile: float | int = FLATFIELD_PERCENTILE,\n",
    "                    img_percentile: float | int = SAMPLE_PERCENTILE,\n",
    "                    min : float | int = 1,\n",
    "                    outliers:bool = False) -> None:\n",
    "    \"\"\"Applies a pre-defined flatfield mask and normalisation for the SMI beamline object\"\"\"\n",
    "    flatmask = flatfield_mask(flatfield=flatfield, percentile=flat_percentile, min = min)\n",
    "    for i, (mask, img) in enumerate(zip(geom.masks, geom.imgs)):\n",
    "        fmask_i = flatmask[:, FLATFIELD_SLICES[i % 3]]\n",
    "        # Apply the masking values\n",
    "        masking_values = np.where(fmask_i == True)\n",
    "        mask[masking_values] = True\n",
    "        # Apply the normalisation \n",
    "        flat = flatfield[:, FLATFIELD_SLICES[i%3]] # Get flatfield panel\n",
    "        flat[flat < 0] = 10000 # avoid negative values - note these are already masked\n",
    "        # Multiply img by max of flatfield, then divide by flatfield to keep integer precision of img.\n",
    "        temp = ((img * np.max(flatfield[:, FLATFIELD_SLICES[i%3]] * ~mask))\n",
    "                / (flat * ~mask +1)) * ~mask # Avoid divide by zero, and zero mask values.\n",
    "        # This line creates a runtime error due to casting... img is int32, flat is float64\n",
    "        img[:] = temp.astype(np.int32)\n",
    "    \n",
    "        # Repeat mask for very large values erronously normalised\n",
    "        if not img_percentile is None and img_percentile < 100:\n",
    "            mask2 = flatfield_mask(flatfield=img, percentile=img_percentile, min = 0) # only consider positive values.\n",
    "            masking_values2 = np.where(mask2 == True)\n",
    "            mask[masking_values2] = True\n",
    "        \n",
    "        # Mask outliers in the data\n",
    "        if outliers:\n",
    "            img_mean = np.mean(img[~mask])\n",
    "            img_std = np.std(img[~mask])\n",
    "            mask[img > img_mean + 10*img_std] = True\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the stages of Flatfield masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, figsize=(16,6), dpi=50, sharex=True, sharey=True)\n",
    "\n",
    "# Detector image is (619, 1475) #619 is 195 * 3 + 2 * 17\n",
    "# Show the detector masking\n",
    "grid = np.zeros((195, 1475), dtype=bool)\n",
    "\n",
    "\n",
    "det_mask = np.c_[apply_boundary_mask_to_array(grid.copy().T),\n",
    "                 np.zeros((17, 1475), dtype=bool).T,\n",
    "                 apply_detector_mask_to_array(apply_boundary_mask_to_array(grid.copy().T)), # 195 pixels\n",
    "                 np.zeros((17, 1475), dtype=bool).T,\n",
    "                apply_boundary_mask_to_array(grid.copy().T)]\n",
    "    \n",
    "\n",
    "det_im = ax[0][0].imshow(np.rot90(det_mask,3), interpolation='nearest') # required to prevent interpolation\n",
    "ax[0][0].set_title(\"Pre-defined Detector Mask (boolean)\")\n",
    "\n",
    "# Show the flatfield masking\n",
    "ff_masked = flatfield_mask(flatfield)\n",
    "ff_masked_im = ax[0][1].imshow(np.rot90(ff_masked,3), interpolation='nearest') # required to prevent interpolation\n",
    "ax[0][1].set_title(\"Calculated Flatfield Mask (boolean)\")\n",
    "\n",
    "# Show the flatfield image\n",
    "ff_im = ax[1][0].imshow(np.rot90(flatfield,3), vmin = 0, vmax = np.mean(flatfield[~ff_masked]) + 5*np.std(flatfield[~ff_masked]), interpolation='nearest')\n",
    "plt.colorbar(ff_im)\n",
    "ax[1][0].set_title(\"Flatfield Raw Data\")\n",
    "\n",
    "# Show the masks on the flatfield image\n",
    "joined_mask = ff_masked | det_mask\n",
    "cmap = mplc.LinearSegmentedColormap.from_list(\"Mask\", [(256,0,0,0),(256,0,0,256)], N=2)\n",
    "ff_im_masked = ax[1][1].imshow(np.rot90(flatfield,3), interpolation='nearest',\n",
    "                               vmin = 0, vmax = np.mean(flatfield[~ff_masked]) + 5*np.std(flatfield[~ff_masked]))\n",
    "ax[1][1].imshow(np.rot90(joined_mask,3), cmap=cmap, interpolation='nearest')\n",
    "ax[1][1].set_title(\"Flatfield Data Masked\")\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Dependence Flux Calibration for MEX2 Beamline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flux data from recording photodiode and BPMs\n",
    "# 2024 Cycle 3 Flux\n",
    "PROPOSAL_FLUX = '314483_Freychet_09'\n",
    "# Compile and load the flux paths\n",
    "FLUX_DIR_PATH = f'{DRIVE}Datasets/2024-09 SMI/{PROPOSAL_FLUX}/1M/'\n",
    "# FLUX_DIR_PATH = f'D:/Datasets/2024-09 SMI/{PROPOSAL_FLUX}/1M/'\n",
    "FLUX_FILES = [file for file in os.listdir(FLUX_DIR_PATH) if file.endswith('.tif') and \"Sedge\" in file and \"eV\" in file]\n",
    "\n",
    "# Parse the flux data from the filenames\n",
    "flux_energies = []\n",
    "flux_norm_bpm2 = []\n",
    "\"\"\"A collection of flux normalisation values for each flux file, also known as i0.\"\"\"\n",
    "flux_norm_bpm3 = []\n",
    "\"\"\"A collection of flux normalisation values for each flux file, also known as i0.\"\"\"\n",
    "flux_norm_phd = []\n",
    "\"\"\"A collection of flux normalisation values for each flux file, also known as i0.\"\"\"\n",
    "for file in FLUX_FILES:\n",
    "    # Example Format:\n",
    "    # CM_direct_beam_Sedge_2474.75eV_bpm2_124.304_bpm3_4.511_pd_20611.340_id806560_000000_SAXS.tif'\n",
    "    en_idx = file.find('eV')\n",
    "    en = float(file[en_idx-7:en_idx])\n",
    "    bpm2_idx = file.find('bpm2_')\n",
    "    bpm3_idx = file.find('bpm3_')\n",
    "    phd_idx = file.find('pd_')\n",
    "    bpm2 = float(file[bpm2_idx+5:bpm3_idx-1])\n",
    "    bpm3 = float(file[bpm3_idx+5:phd_idx-1])\n",
    "    phd = float(file[phd_idx+3:file.find('_id')])\n",
    "    flux_energies.append(en)\n",
    "    flux_norm_bpm2.append(bpm2)\n",
    "    flux_norm_bpm3.append(bpm3)\n",
    "    flux_norm_phd.append(phd)\n",
    "    \n",
    "# Sort the flux data by energy\n",
    "idx = np.argsort(flux_energies)\n",
    "flux_energies = np.array(flux_energies)[idx].tolist()\n",
    "flux_norm_bpm2 = np.array(flux_norm_bpm2)[idx]\n",
    "flux_norm_bpm3 = np.array(flux_norm_bpm3)[idx]\n",
    "flux_norm_phd = np.array(flux_norm_phd)[idx]\n",
    "    \n",
    "## Additional flux data from silicon reference samples\n",
    "# Define the silicon reference data\n",
    "WAXS_ANGLE_STRINGS = ['wa0', 'wa20']\n",
    "FLUX_SI_CYCLE: str = '2024_3' #YYYY_[1-3]\n",
    "FLUX_SI_PROPOSAL_ID = '316022_McNeill_15' #PPPPPP_[Name]_[#]\n",
    "# FLUX_SI_DIR = f'D:/Datasets/2024-09 SMI/{FLUX_SI_PROPOSAL_ID}/900KW/'\n",
    "FLUX_SI_DIR = f'{DRIVE}Datasets/2024-09 SMI/{FLUX_SI_PROPOSAL_ID}/900KW/'\n",
    "\n",
    "# Load the flux filenames\n",
    "FLUX_SI_FILES = [file for file in os.listdir(FLUX_SI_DIR) if file.endswith('.tif') and \"_Si_\" in file and \"wide\" not in file]\n",
    "FLUX_SI_DATASETS: list[list[str]] = [[\"\"] * len(WAXS_ANGLE_STRINGS) for _ in flux_energies]\n",
    "\"\"\"For each energy, a list of raw data filenames for the silicon reference.\"\"\"\n",
    "for j, file in enumerate(FLUX_SI_FILES):\n",
    "    en_idx = file.find('eV')\n",
    "    en = file[en_idx-7:en_idx]\n",
    "    energies_idx = flux_energies.index(float(en))\n",
    "    waxs_angle_idx = [k for k, angle in enumerate(WAXS_ANGLE_STRINGS) if angle in file][0]\n",
    "    FLUX_SI_DATASETS[energies_idx][waxs_angle_idx] = file\n",
    "\n",
    "pixel_summations: npt.NDArray[float] = np.zeros((len(flux_energies), len(WAXS_ANGLE_STRINGS)))\n",
    "\"\"\"The pixel summations for each sample, energy, and WAXS angle.\"\"\"\n",
    "for j, en in enumerate(flux_energies):\n",
    "    for k, angle in enumerate(WAXS_ANGLE_STRINGS):\n",
    "        if FLUX_SI_DATASETS[j][k] is not None:\n",
    "            # Load the data\n",
    "            img = fabio.open(os.path.join(FLUX_SI_DIR, FLUX_SI_DATASETS[j][k])).data\n",
    "            # Mask the data\n",
    "            new_mask = np.zeros_like(img, dtype=bool).T\n",
    "            apply_boundary_mask_to_array(new_mask)\n",
    "            apply_detector_mask_to_array(new_mask)\n",
    "            new_mask = new_mask.T\n",
    "            img_mask = flatfield_mask(img, SAMPLE_PERCENTILE, 0)\n",
    "            \n",
    "            # Perform a summation excluding masked pixels:\n",
    "            img[img_mask | new_mask] = 0\n",
    "            pixel_summations[j][k] = np.sum(img)\n",
    "            \n",
    "# Normalize by the first value\n",
    "flux_norm_bpm2 /= flux_norm_bpm2[-1]\n",
    "flux_norm_bpm3 /= flux_norm_bpm3[-1]\n",
    "flux_norm_phd /= flux_norm_phd[-1]\n",
    "# Normalize the pixel summations\n",
    "pixel_summations /= pixel_summations[-1]\n",
    "si_ave = np.mean(pixel_summations, axis=1)\n",
    "\n",
    "# Plot results\n",
    "fig,ax = plt.subplots(1,1, figsize=(8, 3), sharex=True)\n",
    "ax.plot(flux_energies, flux_norm_bpm2, label='BPM2')\n",
    "ax.plot(flux_energies, flux_norm_bpm3, label='BPM3')\n",
    "ax.plot(flux_energies, flux_norm_phd, label='PHD')\n",
    "for k, angle in enumerate(WAXS_ANGLE_STRINGS):\n",
    "    ax.plot(flux_energies, pixel_summations[:, k], label=f'Si {angle}')\n",
    "ax.set_xlabel(\"Energy (keV)\")\n",
    "ax.set_ylabel(\"Flux (Arbitrary Units)\")\n",
    "ax.set_title(\"Flux Normalisation\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "# Select a flux channel for the analysis\n",
    "FLUX_CHANNEL = 'si_ave'\n",
    "match FLUX_CHANNEL:\n",
    "    case 'bpm2':\n",
    "        FLUX_NORM = flux_norm_bpm2\n",
    "    case 'bpm3':\n",
    "        FLUX_NORM = flux_norm_bpm3\n",
    "    case 'phd':\n",
    "        FLUX_NORM = flux_norm_phd\n",
    "    case 'si0':\n",
    "        FLUX_NORM = pixel_summations[:, 0]\n",
    "    case 'si20':\n",
    "        FLUX_NORM = pixel_summations[:, 1]\n",
    "    case 'si_ave':\n",
    "        FLUX_NORM = si_ave\n",
    "    case _:\n",
    "        raise ValueError(\"Invalid flux channel selected.\")\n",
    "    \n",
    "FLUX_OPTIONS = {\n",
    "    'bpm2': flux_norm_bpm2,\n",
    "    'bpm3': flux_norm_bpm3,\n",
    "    'phd': flux_norm_phd,\n",
    "    'si0': pixel_summations[:, 0],\n",
    "    'si20': pixel_summations[:, 1],\n",
    "    'si_ave': si_ave\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "\n",
    "##### Locate the files on your computer and define the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CYCLE: str = '2024_3' #YYYY_[1-3]\n",
    "PROPOSAL_ID = '316022_McNeill_12' #PPPPPP_[Name]_[#]\n",
    "## ----------- Path to the raw data -----------\n",
    "\n",
    "# RAW_PATH = f'D:/Datasets/2024-09 SMI/{CYCLE}/{PROPOSAL_ID}/900KW/'\n",
    "# RAW_DIR = f'D:/Datasets/2024-09 SMI/{PROPOSAL_ID}/900KW/'\n",
    "RAW_DIR = f'{DRIVE}Datasets/2024-09 SMI/{PROPOSAL_ID}/900KW/'\n",
    "display(pd.DataFrame(os.listdir(RAW_DIR), columns=[\"Filename\"])) #use tail or head to display a subset\n",
    "\n",
    "## ----------- Create/select the results directory -----------\n",
    "RESULT_DIR = f'{DRIVE}Datasets/2024-09 SMI/{PROPOSAL_ID}/energy_scan_results/'\n",
    "# RESULT_DIR = f'D:/Datasets/2024-09 SMI/{PROPOSAL_ID}/energy_scan_results/'\n",
    "created = False\n",
    "for i in range(len(RESULT_DIR.split(\"/\"))):\n",
    "    if not os.path.isdir(\"/\".join(RESULT_DIR.split(\"/\")[:i+1])) and not (i==0 and RESULT_DIR[0]==\"/\"):\n",
    "        os.mkdir(\"/\".join(RESULT_DIR.split(\"/\")[:i+1]))\n",
    "        created = True\n",
    "if not created:\n",
    "    print(\"Results path exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Organise files into sample names and data for each detector angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_flags: list[str] = [\"A1\", \"p_\"]\n",
    "\"\"\"The strings you want included in the files processed.\"\"\"\n",
    "exclude_flags: list[str] = ['wide']\n",
    "\"\"\"The strings you want excluded in the files processed.\"\"\"\n",
    "\n",
    "# Find all samples and energies\n",
    "samples: list[str] = []\n",
    "\"\"\"String names of the unique samples matching patterns in `filename_flags`\"\"\"\n",
    "sample_angles: list[list[float]] = []\n",
    "\"\"\"Unique angles found in the filenames for each sample\"\"\"\n",
    "sample_angle_energies: list[list[list[float]]] = []\n",
    "\"\"\"Unique energies found in the filenames for each sample and angle\"\"\"\n",
    "\n",
    "for file in sorted(os.listdir(RAW_DIR)):\n",
    "     # Define the flags for the files you want to process, by filtering the filename.\n",
    "     if all([flag in file for flag in filename_flags] \n",
    "            + [flag not in file for flag in exclude_flags]):\n",
    "        # Find the sample name:\n",
    "        idx = file.find('_ai')\n",
    "        sample_substring = file[3:idx-10]\n",
    "        # If sample substring not in list of samples, add it!\n",
    "        if sample_substring not in samples:\n",
    "            sample_idx = len(samples)\n",
    "            samples.append(sample_substring)\n",
    "            # Add new lists for the angles and energies\n",
    "            sample_angles.append([])\n",
    "            sample_angle_energies.append([])\n",
    "        else:\n",
    "            sample_idx = samples.index(sample_substring)\n",
    "        \n",
    "        # Find the angle of incidence:\n",
    "        ai = float(file[idx+3:idx+7])\n",
    "        if ai not in sample_angles[sample_idx]:\n",
    "            ai_idx = len(sample_angles[sample_idx])\n",
    "            sample_angles[sample_idx].append(ai)\n",
    "            # Add new list for the energies\n",
    "            sample_angle_energies[sample_idx].append([])\n",
    "        else:\n",
    "            ai_idx = sample_angles[sample_idx].index(ai)\n",
    "        \n",
    "        # Find the beam energy:\n",
    "        idx = file.find('eV')\n",
    "        en = float(file[idx-7:idx])\n",
    "        if en not in sample_angle_energies[sample_idx][ai_idx]:\n",
    "            sample_angle_energies[sample_idx][ai_idx].append(en)\n",
    "    \n",
    "# Sort the angles and energies\n",
    "for i in range(len(samples)): \n",
    "    for j in range(len(sample_angles[i])):\n",
    "        # Sort the energies\n",
    "        sample_angle_energies[i][j] = sorted(sample_angle_energies[i][j])\n",
    "    # Sort the angles\n",
    "    args = np.argsort(sample_angles[i])\n",
    "    sample_angles[i] = np.array(sample_angles[i])[args].tolist()\n",
    "    sample_angle_energies[i] = np.array(sample_angle_energies[i])[args].tolist()\n",
    "\n",
    "# Specify the WAXS angles strings to look for, in each sample\n",
    "WAXS_ANGLE_STRINGS : list[str] = [\"wa18\"]\n",
    "\n",
    "# Find all TIF image measurements\n",
    "datasets: list[list[list[list[str | None]]]] = [[[\n",
    "                                                [None] * len(WAXS_ANGLE_STRINGS)\n",
    "                                                for k, _ in enumerate(sample_angle_energies[i][j])]\n",
    "                                            for j, _ in enumerate(sample_angles[i])]\n",
    "                                        for i, _ in enumerate(samples)]\n",
    "\"\"\"For each sample, for each angle, for each energy, a list of raw data filenames.\"\"\"\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, file in enumerate(sorted(os.listdir(RAW_DIR))):\n",
    "        if all([flag in file for flag in filename_flags + [sample, '.tif']] \n",
    "                + [flag not in file for flag in exclude_flags]):\n",
    "            # Check which waxs angle the file is\n",
    "            ai_idx = file.find('_ai')\n",
    "            ai = file[ai_idx+3:ai_idx+7]\n",
    "            en_idx = file.find('eV')\n",
    "            en = file[en_idx-7:en_idx]\n",
    "            angles_idx = sample_angles[i].index(float(ai))\n",
    "            energies_idx = sample_angle_energies[i][angles_idx].index(float(en))\n",
    "            waxs_angle_idx = [k for k, angle in enumerate(WAXS_ANGLE_STRINGS) if angle in file][0]\n",
    "            datasets[i][angles_idx][energies_idx][waxs_angle_idx] = file\n",
    "\n",
    "# Display the number of files for each sample:\n",
    "display(\n",
    "      pd.DataFrame([\n",
    "            (sample, np.sum(\n",
    "                [len(sample_angle_energies[i][j]) * len(WAXS_ANGLE_STRINGS)\n",
    "                    for j in range(len(sample_angles[i]))]\n",
    "            ))\n",
    "            for i, sample in enumerate(samples)\n",
    "      ], columns=[\"Sample Name\", \"Number of Files\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Organise the unique energies, and interpolate any missing values for normalisaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_energies = np.unique([en for sample in sample_angle_energies for angle in sample for en in angle])\n",
    "if np.all(global_energies != flux_energies):\n",
    "    print(\"The flux energies do not match the global energies.\")\n",
    "    print(\"Generating interpolated values\")\n",
    "    for i, en in enumerate(global_energies):\n",
    "        if en not in flux_energies:\n",
    "            idx = np.searchsorted(flux_energies, en)\n",
    "            flux_energies.insert(idx, en)\n",
    "            flux_norm_bpm2 = np.r_[flux_norm_bpm2[:idx], np.interp(en, flux_energies, flux_norm_bpm2), flux_norm_bpm2[idx:]]\n",
    "            flux_norm_bpm3 = np.r_[flux_norm_bpm3[:idx], np.interp(en, flux_energies, flux_norm_bpm3), flux_norm_bpm3[idx:]]\n",
    "            flux_norm_phd = np.r_[flux_norm_phd[:idx], np.interp(en, flux_energies, flux_norm_phd), flux_norm_phd[idx:]]\n",
    "            si0 = np.r_[pixel_summations[:, 0][:idx], np.interp(en, flux_energies, pixel_summations[:, 0]), pixel_summations[:, 0][idx:]]\n",
    "            si20 = np.r_[pixel_summations[:, 1][:idx], np.interp(en, flux_energies, pixel_summations[:, 1]), pixel_summations[:, 1][idx:]]\n",
    "            si_ave = np.r_[si_ave[:idx], np.interp(en, flux_energies, si_ave), si_ave[idx:]]\n",
    "            \n",
    "            # Re-assign new objects\n",
    "            FLUX_OPTIONS = {\n",
    "                'bpm2': flux_norm_bpm2,\n",
    "                'bpm3': flux_norm_bpm3,\n",
    "                'phd': flux_norm_phd,\n",
    "                'si0': si0,\n",
    "                'si20': si20,\n",
    "                'si_ave': si_ave\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction\n",
    "### Run the first sample to check everything is working and define the Q fit ranges\n",
    "\n",
    "##### First check sample image exists and looks reasonable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files: list[int] = [#0, 20, 40, -2]\n",
    "                         0, -2]\n",
    "\"\"\"The index of each sample's test file\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, ai in enumerate(sample_angles[i]):\n",
    "        if not (i == 0 or j == 0):\n",
    "            continue\n",
    "        for test_file in test_files:\n",
    "            print(sample, \"|\", datasets[i][j][test_file][0].replace(sample, \"\"))\n",
    "            fig,ax = plt.subplots(len(WAXS_ANGLE_STRINGS),2, figsize=(8,len(WAXS_ANGLE_STRINGS)*1.5), sharex=True, sharey=True)\n",
    "            for k, angle in enumerate(WAXS_ANGLE_STRINGS):\n",
    "                fname = datasets[i][j][test_file][k]\n",
    "                # fig.suptitle(sample)\n",
    "                # Use \n",
    "                img=fabio.open(os.path.join(RAW_DIR, fname)).data\n",
    "                a = ax[k][0] if len(WAXS_ANGLE_STRINGS) > 1 else ax[0]\n",
    "                mappable = a.imshow(img, vmin=0, vmax=np.percentile(img,99))\n",
    "                a.set_title(f\"Raw Data ({angle})\")\n",
    "                plt.colorbar(mappable)\n",
    "                \n",
    "                # Show the masks on the flatfield image\n",
    "                a = ax[k][1] if len(WAXS_ANGLE_STRINGS) > 1 else ax[1]\n",
    "                cmap = mplc.LinearSegmentedColormap.from_list(\"Mask\", [(256,0,0,0),(256,0,0,256)], N=2)\n",
    "                ff_im_masked = a.imshow(img, interpolation='nearest',\n",
    "                                        vmin = 0, vmax = np.percentile(img, 99))\n",
    "                a.imshow(np.rot90(joined_mask,3), cmap=cmap, interpolation='nearest')\n",
    "                a.set_title(\"Masked by flatfield and detector\")\n",
    "                plt.colorbar(ff_im_masked)\n",
    "\n",
    "                if k !=0:\n",
    "                    break\n",
    "        \n",
    "            \n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "        print(\"------------------------------\")\n",
    "        if j!=0:\n",
    "            break\n",
    "    if i!=0:\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLATFIELD_PERCENTILE, SAMPLE_PERCENTILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Tried digging into SMI_beamline > SMI_geometry > stitching_data > stitch.stitching, to understand how the mask is stitched. \n",
    "## Too difficult to follow, for some reason mask is not really applied well in the first sector of the detector.\n",
    "import datetime\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    # At each sample, reset the SMI_waxs object\n",
    "    if SMI_waxs is not None and len(SMI_waxs.ai) != 0:\n",
    "        # for ai in SMI_waxs.ai:\n",
    "        #     ai.reset()\n",
    "        SMI_waxs.ai = []\n",
    "    \n",
    "    for j, ai in enumerate(sample_angles[i]):\n",
    "        if not (i == 0 and j == 2):\n",
    "            continue\n",
    "        for test_file in test_files:\n",
    "            # Setup a figure and open the file\n",
    "            fig,ax = plt.subplots(1,3, figsize=(12,2.5), sharex=True, sharey=True)\n",
    "            fnames = datasets[i][j][test_file]\n",
    "            fname = fnames[0]\n",
    "            \n",
    "            # Collect the metadata\n",
    "            en_idx = fname.find('eV_')\n",
    "            en = float(fname[en_idx-7:en_idx])\n",
    "            ai_idx = fname.find(\"_ai\")\n",
    "            ai2 = float(fname[ai_idx+3:ai_idx+7])\n",
    "            assert(ai == ai2)\n",
    "            \n",
    "            display(pd.DataFrame([\n",
    "                    (fname, en, ai)\n",
    "                    ], columns=[\"Filename\", \"Energy (eV)\", \"Incident Angle (deg)\"]))\n",
    "            \n",
    "            # if i == 0 and j == 0 and (test_file == test_files[0] or test_file == test_files[1]):\n",
    "            #     print(SMI_waxs.ai)\n",
    "            \n",
    "            # Update the geometry/'\n",
    "            tinit = datetime.datetime.now()\n",
    "            SMI_waxs.alphai = ai\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to update geometry: {tfin-tinit}\")\n",
    "            \n",
    "            tinit = datetime.datetime.now()\n",
    "            SMI_waxs.wav = en2wav(en)\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to update wavelength: {tfin-tinit}\")\n",
    "            \n",
    "            # if i == 0 and j == 0 and (test_file == test_files[0] or test_file == test_files[1]):\n",
    "            #     print(SMI_waxs.ai)\n",
    "            \n",
    "            # Reset the masks\n",
    "            for mask in SMI_waxs.masks:\n",
    "                mask[:,:] = False\n",
    "            \n",
    "            # Plot the unmodified data\n",
    "            SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "            \n",
    "            SMI_waxs.stitching_data(interp_factor=2, flag_scale=True, timing=True if i==0 and j==0 else False, perpendicular=True)\n",
    "            # mp = ax[0].imshow(SMI_waxs.img_st,\n",
    "            #         extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "            mp = ax[0].imshow(SMI_waxs.img_st,\n",
    "                    extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "                    # vmin=0,\n",
    "                    vmax=np.percentile(SMI_waxs.img_st, 99)\n",
    "            )\n",
    "            plt.colorbar(mp)\n",
    "            ax[0].set_title(\"Stitched Data (Raw)\")\n",
    "            \n",
    "            # Plot the flatfield / masked normalized data\n",
    "            SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "            apply_detector_mask(SMI_waxs)\n",
    "            # apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "            apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE, outliers=False)\n",
    "            # apply_detector_mask(SMI_waxs)\n",
    "            \n",
    "            SMI_waxs.stitching_data(interp_factor=1, flag_scale=True, perpendicular=True)\n",
    "            # mp = ax[1].imshow(SMI_waxs.img_st,\n",
    "                    # extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "            mp = ax[1].imshow(SMI_waxs.img_st,\n",
    "                    extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "                    # vmin=np.percentile(SMI_waxs.img_st, 1.0), \n",
    "                    vmax=np.percentile(SMI_waxs.img_st, 99.0)\n",
    "            )\n",
    "            plt.colorbar(mp)\n",
    "            ax[1].set_title(\"Stitched (Norm. + Custom Masked)\")\n",
    "            \n",
    "            # Show the total mask over the image\n",
    "            cmap = mplc.LinearSegmentedColormap.from_list(\"Mask\", [(256,0,0,0),(256,0,0,256)], N=2)\n",
    "            mp = ax[2].imshow(SMI_waxs.img_st,\n",
    "                    extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "                    # vmin=np.percentile(SMI_waxs.img_st, 1.0), \n",
    "                    vmax=np.percentile(SMI_waxs.img_st, 99.0)\n",
    "            )\n",
    "            ax[2].imshow(SMI_waxs.mask_st,\n",
    "                    extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1],],\n",
    "                    cmap=cmap\n",
    "            )\n",
    "            plt.colorbar(mp)\n",
    "            ax[2].set_title(\"Detector mask + custom mask\")\n",
    "            plt.show()\n",
    "        if i!=0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate a ROI to check the beam-centre \n",
    "###### Also check that the beamcentre is correct by symmetry (unless sample has anisotropic behavour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the radial angles\n",
    "AZIMUTHAL_WIDTH = 10\n",
    "\"\"\"The +- azimuthal width of the orthogonal range\"\"\"\n",
    "AZIMUTHAL_INPLANE = 10\n",
    "\"\"\"The azimuthal angle for the in-plane scattering\"\"\"\n",
    "AZIMUTHAL_OUTOFPLANE = 80\n",
    "\"\"\"The azimuthal angle for the out-of-plane averaging\"\"\"\n",
    "RADIAL_WIDTH = 35\n",
    "\"\"\"The +- azimuthal width for the radial averaging\"\"\"\n",
    "AZIMUTHAL_RADIAL = 45\n",
    "\"\"\"The azimuthal angle for the radial averaging\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, ai in enumerate(sample_angles[i]):\n",
    "        if not (i==0 or j==0):\n",
    "            continue\n",
    "        for test_file in test_files:\n",
    "            # Setup a figure and open the file\n",
    "            fnames = datasets[i][j][test_file]\n",
    "            if None in fnames:\n",
    "                print(f\"Skipping {sample} due to missing files.\")\n",
    "                continue\n",
    "            fname = fnames[0]\n",
    "            \n",
    "            # Collect the metadata\n",
    "            en_idx = fname.find('eV_')\n",
    "            en = float(fname[en_idx-7:en_idx])\n",
    "            ai_idx = fname.find(\"_ai\")\n",
    "            ai2 = float(fname[ai_idx+3:ai_idx+7])\n",
    "            assert(ai==ai2)\n",
    "            display(pd.DataFrame([\n",
    "                    (fname, en, ai)\n",
    "                    ], columns=[\"Filename\", \"Energy (eV)\", \"Incident Angle (deg)\"]))\n",
    "            \n",
    "            # Update the geometry\n",
    "            SMI_waxs.alphai = ai\n",
    "            SMI_waxs.wav = en2wav(en)\n",
    "\n",
    "            # Show the angles on a plot\n",
    "            fig,ax = plt.subplots(1,1, figsize=(8,4), sharex=True, sharey=True)\n",
    "            ax.set_ylim(SMI_waxs.qz[0], SMI_waxs.qz[-1])\n",
    "            ax.set_xlim(SMI_waxs.qp[0], SMI_waxs.qp[-1])\n",
    "\n",
    "            SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "            apply_detector_mask(SMI_waxs)\n",
    "            apply_flatfield(SMI_waxs, flatfield)\n",
    "            energies_idx = flux_energies.index(en) # for normalisation\n",
    "            for img in SMI_waxs.imgs:\n",
    "                img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "            SMI_waxs.stitching_data(interp_factor=5, flag_scale=True, perpendicular=True)\n",
    "            # mp = ax[1].imshow(SMI_waxs.img_st,\n",
    "                    # extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "            mp = ax.imshow(SMI_waxs.img_st,\n",
    "                    extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "                    vmin=np.percentile(SMI_waxs.img_st, 1.0), \n",
    "                    vmax=np.percentile(SMI_waxs.img_st, 99.0)\n",
    "            )\n",
    "            plt.colorbar(mp)\n",
    "\n",
    "            # Plot the azimuthal and radial angles\n",
    "            colors = ['r', 'orange', 'white'][::-1]\n",
    "            for angle, width in zip([AZIMUTHAL_INPLANE, AZIMUTHAL_OUTOFPLANE, AZIMUTHAL_RADIAL], [AZIMUTHAL_WIDTH, AZIMUTHAL_WIDTH, RADIAL_WIDTH]):\n",
    "                    # Generate a set of x points to plot lines of.\n",
    "                    q_x = np.linspace(0, SMI_waxs.qp[-1], 100)\n",
    "                    # Calculate the x and y gradients for the lines\n",
    "                    m1 = np.tan(np.deg2rad(angle - width)) if angle - width != 90 else np.inf\n",
    "                    m2 = np.tan(np.deg2rad(angle + width)) if angle + width != 90 else np.inf\n",
    "                    # Calculate the x & y values for the lines\n",
    "                    q_x1 = q_x if m1 != np.inf else np.zeros(100)\n",
    "                    q_x2 = q_x if m2 != np.inf else np.zeros(100)\n",
    "                    y1 = m1 * q_x if m1 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                    y2 = m2 * q_x if m2 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                    # Plot the lines\n",
    "                    color = colors.pop()\n",
    "                    ax.plot(q_x1, y1, color=color, linestyle='-', label=f\"{angle} deg\")\n",
    "                    ax.plot(q_x2, y2, color=color, linestyle='-')\n",
    "                    # If gradient is inf, calculate an alternative fill between\n",
    "                    if m2 == np.inf:\n",
    "                            ax.fill_betweenx(y1, q_x1, q_x2, color=color, alpha=0.1)\n",
    "                    else:\n",
    "                            ax.fill_between(q_x, y1, y2, color=color, alpha=0.1)\n",
    "            ax.legend()\n",
    "            plt.show()\n",
    "            # ax.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the azimuthal/radial averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPOINTS_RADIAL_AVE: int = 2000 # Use a number the is consistent with the pixel density?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, ai in enumerate(sample_angles[i]):\n",
    "        if not (i==0 or j==0):\n",
    "            continue\n",
    "        \n",
    "        for test_file in test_files:\n",
    "            # Setup a figure and open the file\n",
    "            fnames = datasets[i][j][test_file]\n",
    "            fname = fnames[0]\n",
    "            \n",
    "            # Collect the metadata\n",
    "            en_idx = fname.find('eV_')\n",
    "            en = float(fname[en_idx-7:en_idx])\n",
    "            ai_idx = fname.find(\"_ai\")\n",
    "            ai2 = float(fname[ai_idx+3:ai_idx+7])\n",
    "            assert ai==ai2\n",
    "            display(pd.DataFrame([\n",
    "                    (fname, en, ai)\n",
    "                    ], columns=[\"Filename\", \"Energy (eV)\", \"Incident Angle (deg)\"]))\n",
    "            \n",
    "            # Update the geometry\n",
    "            tinit = datetime.datetime.now()\n",
    "            SMI_waxs.alphai = ai\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to update geometry: {tfin-tinit}\")\n",
    "            \n",
    "            tinit = datetime.datetime.now()\n",
    "            SMI_waxs.wav = en2wav(en)\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to update wavelength: {tfin-tinit}\")\n",
    "\n",
    "            # Open and stitch the data\n",
    "            tinit = datetime.datetime.now()\n",
    "            SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to open data: {tfin-tinit}\")\n",
    "            \n",
    "            tinit = datetime.datetime.now()\n",
    "            apply_detector_mask(SMI_waxs)\n",
    "            apply_flatfield(SMI_waxs, flatfield)\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to apply masks: {tfin-tinit}\")\n",
    "            \n",
    "            tinit = datetime.datetime.now()\n",
    "            energies_idx = flux_energies.index(en) # for normalisation\n",
    "            for img in SMI_waxs.imgs:\n",
    "                img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to normalise data: {tfin-tinit}\")\n",
    "                \n",
    "            tinit = datetime.datetime.now()\n",
    "            SMI_waxs.stitching_data(interp_factor=2, flag_scale=False, perpendicular=True)\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to stitch data: {tfin-tinit}\")\n",
    "\n",
    "            # Generate radial averages\n",
    "            fig,ax = plt.subplots(2,4, figsize=(12,5), sharex=True, height_ratios=[3,1])\n",
    "            fig.suptitle(f\"{sample}\\n{en:0.2f} eV, {ai:0.3f} deg\")\n",
    "            ax[0][0].set_xscale(\"log\")\n",
    "\n",
    "            tinit = datetime.datetime.now()\n",
    "            # In plane and out of plane\n",
    "            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                    azimuth_range=[90 - (AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH) , 90 - (AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                    npt = NPOINTS_RADIAL_AVE)\n",
    "            q0_IP, I0_IP, I0_IP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                    azimuth_range=[90 - (AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH), 90 - (AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                    npt = NPOINTS_RADIAL_AVE)\n",
    "            q0_OOP, I0_OOP, I0_OOP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "            # Repeat IP and OOP for the consistency checking\n",
    "            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                    azimuth_range=[-90+(AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH), -90+(AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                    npt = NPOINTS_RADIAL_AVE)\n",
    "            q0_IP2, I0_IP2, I0_IP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                    azimuth_range=[-90+(AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH) , -90+(AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                    npt = NPOINTS_RADIAL_AVE)\n",
    "            q0_OOP2, I0_OOP2, I0_OOP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "            # Radial averaging\n",
    "            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                    azimuth_range=[90-(AZIMUTHAL_RADIAL - RADIAL_WIDTH) , -90+(AZIMUTHAL_RADIAL + RADIAL_WIDTH)], \n",
    "                                    npt = NPOINTS_RADIAL_AVE)\n",
    "            q0_R, I0_R, I0_R_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "            # Repeat radial averaging for consistency checking\n",
    "            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                    azimuth_range=[-90+(AZIMUTHAL_RADIAL - RADIAL_WIDTH) , -90+(AZIMUTHAL_RADIAL + RADIAL_WIDTH)], \n",
    "                                    npt = NPOINTS_RADIAL_AVE)\n",
    "            q0_R2, I0_R2, I0_R2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to radial average: {tfin-tinit}\")\n",
    "\n",
    "            tinit = datetime.datetime.now()\n",
    "            # Labels\n",
    "            ax[1][1].set_xlabel(\"$q$ ($^{-1}$)\")\n",
    "            ax[0][0].set_ylabel(\"Intensity (a.u.)\")\n",
    "            ax[1][0].set_ylabel(\"Error (Poisson)\")\n",
    "            ax[0][0].set_title(\"Radial Averaging\")\n",
    "            ax[0][1].set_title(\"In-plane Averaging\")\n",
    "            ax[0][2].set_title(\"Out-of-plane Averaging\")\n",
    "            # Plot the radial averages\n",
    "            ax[0][0].plot(q0_R, I0_R, label=\"Radial\")\n",
    "            ax[0][0].fill_between(q0_R, I0_R - I0_R_err, I0_R + I0_R_err, alpha=0.5)\n",
    "            ax[0][0].plot(q0_R2, I0_R2, label=\"Radial (180)\")\n",
    "            ax[0][0].fill_between(q0_R2, I0_R2 - I0_R2_err, I0_R2 + I0_R2_err, alpha=0.5)\n",
    "            # Plot the in-plane and out-of-plane averages\n",
    "            ax[0][1].plot(q0_IP, I0_IP, label=\"In-plane\")\n",
    "            ax[0][1].fill_between(q0_IP, I0_IP - I0_IP_err, I0_IP + I0_IP_err, alpha=0.5)\n",
    "            ax[0][1].plot(q0_IP2, I0_IP2, label=\"In-plane (180)\")\n",
    "            ax[0][1].fill_between(q0_IP2, I0_IP2 - I0_IP2_err, I0_IP2 + I0_IP2_err, alpha=0.5)\n",
    "            ax[0][2].plot(q0_OOP, I0_OOP, label=\"Out-of-plane\")\n",
    "            ax[0][2].fill_between(q0_OOP, I0_OOP - I0_OOP_err, I0_OOP + I0_OOP_err, alpha=0.5)\n",
    "            ax[0][2].plot(q0_OOP2, I0_OOP2, label=\"Out-of-plane (180)\")\n",
    "            ax[0][2].fill_between(q0_OOP2, I0_OOP2 - I0_OOP2_err, I0_OOP2 + I0_OOP2_err, alpha=0.5)\n",
    "            # Overlap the in-plane and out-of-plane averages to check for consistency\n",
    "            ax[0][3].plot(q0_IP, I0_IP, label=\"In-plane\")\n",
    "            ax[0][3].fill_between(q0_IP, I0_IP - I0_IP_err, I0_IP + I0_IP_err, alpha=0.5)\n",
    "            ax[0][3].plot(q0_OOP, I0_OOP, label=\"Out-of-plane\")\n",
    "            ax[0][3].fill_between(q0_OOP, I0_OOP - I0_OOP_err, I0_OOP + I0_OOP_err, alpha=0.5)\n",
    "            ax[0][3].plot(q0_IP2, I0_IP2, label=\"In-plane (180)\")\n",
    "            ax[0][3].fill_between(q0_IP2, I0_IP2 - I0_IP2_err, I0_IP2 + I0_IP2_err, alpha=0.5)\n",
    "            ax[0][3].plot(q0_OOP2, I0_OOP2, label=\"Out-of-plane (180)\")\n",
    "            ax[0][3].fill_between(q0_OOP2, I0_OOP2 - I0_OOP2_err, I0_OOP2 + I0_OOP2_err, alpha=0.5)\n",
    "\n",
    "            # Plot the errors\n",
    "            ax[1][0].plot(q0_R, I0_R_err, label=\"Radial\")\n",
    "            ax[1][0].plot(q0_R2, I0_R2_err, label=\"Radial (180)\")\n",
    "            ax[1][1].plot(q0_IP, I0_IP_err, label=\"In-plane\")\n",
    "            ax[1][1].plot(q0_IP2, I0_IP2_err, label=\"In-plane (180)\")\n",
    "            ax[1][2].plot(q0_OOP, I0_OOP_err, label=\"Out-of-plane\")\n",
    "            ax[1][2].plot(q0_OOP2, I0_OOP2_err, label=\"Out-of-plane (180)\")\n",
    "            ax[1][3].plot(q0_IP, I0_IP_err, label=\"In-plane\")\n",
    "            ax[1][3].plot(q0_OOP, I0_OOP_err, label=\"Out-of-plane\")\n",
    "            ax[1][3].plot(q0_IP2, I0_IP2_err, label=\"In-plane (180)\")\n",
    "            ax[1][3].plot(q0_OOP2, I0_OOP2_err, label=\"Out-of-plane (180)\")\n",
    "\n",
    "            for a in ax[0]:\n",
    "                a.legend()\n",
    "            for a in ax.flatten():\n",
    "                a.set_yscale(\"log\")\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to plot: {tfin-tinit}\")\n",
    "            # plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If happy with line profiles above, then generate the line profiles for test_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDE: bool = True\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\"\n",
    "if not os.path.isdir(RESULT_DIR):\n",
    "    os.mkdir(RESULT_DIR)\n",
    "with tqdm.notebook.tqdm(total=len(samples), position=2, leave=True, desc=\"Samples\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        with tqdm.notebook.tqdm(total=len(sample_angles[i]), position=1, leave=False, desc=\"Angles\") as pbar1:\n",
    "            for j, ai in enumerate(sample_angles[i]):\n",
    "                # For each file in the sample\n",
    "                with tqdm.notebook.tqdm(total=len(test_files), position=0, leave=False, desc=\"Files\") as pbar2:\n",
    "                    for test_file in test_files:\n",
    "                        ## Create the results directory for the sample\n",
    "                        sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "                        if not os.path.isdir(sample_dir):\n",
    "                            os.mkdir(sample_dir)\n",
    "                            \n",
    "                        ## Create the directories for the images and line profiles\n",
    "                        giwaxs_img_dir = os.path.join(sample_dir, \"giwaxs_flatfielded_images\")\n",
    "                        if not os.path.isdir(giwaxs_img_dir):\n",
    "                            os.mkdir(giwaxs_img_dir)\n",
    "                        line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "                        if not os.path.isdir(line_profiles_dir):\n",
    "                            os.mkdir(line_profiles_dir)\n",
    "                    \n",
    "                    \n",
    "                        for k, fnames in enumerate(datasets[i][j][test_file:test_file+1]):\n",
    "                            fname = fnames[0]\n",
    "                            # Collect the metadata\n",
    "                            en_idx = fname.find('eV_')\n",
    "                            en = float(fname[en_idx-7:en_idx])\n",
    "                            ai_idx = fname.find(\"_ai\")\n",
    "                            ai2 = float(fname[ai_idx+3:ai_idx+7])\n",
    "                            assert ai==ai2\n",
    "                            \n",
    "                            # Generate paths for the output files\n",
    "                            path_det_img = os.path.join(giwaxs_img_dir, f\"{sample}_giwaxs_{ai:0.3f}deg_{en:0.2f}eV.png\")\n",
    "                            path_OOP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_OOP.txt\")\n",
    "                            path_IP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_IP.txt\")\n",
    "                            path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_R.txt\")\n",
    "                            path_det_line_profiles_img = os.path.join(sample_dir, f\"{sample}_line_profile_angles_test.png\")\n",
    "                            \n",
    "                            # Do not override the files if they \"all\" exist already. Override partial file sets though.\n",
    "                            if (not OVERRIDE \n",
    "                                and os.path.isfile(path_det_img) \n",
    "                                and os.path.isfile(path_OOP) \n",
    "                                and os.path.isfile(path_IP) \n",
    "                                and os.path.isfile(path_R)\n",
    "                                and (k != 0 or os.path.isfile(path_det_line_profiles_img))):\n",
    "                                pbar.total -= 1\n",
    "                                continue\n",
    "                            \n",
    "                            # Update the geometry\n",
    "                            SMI_waxs.alphai = ai\n",
    "                            SMI_waxs.wav = en2wav(en)\n",
    "                            \n",
    "                            # Plot the flatfield / masked normalized data\n",
    "                            SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "                            apply_detector_mask(SMI_waxs)\n",
    "                            apply_flatfield(SMI_waxs, flatfield)\n",
    "                            energies_idx = flux_energies.index(en) # for normalisation\n",
    "                            for img in SMI_waxs.imgs:\n",
    "                                img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "                            SMI_waxs.stitching_data(interp_factor=2, flag_scale=False, perpendicular=True)\n",
    "                            \n",
    "                            # Setup a figure and open the file\n",
    "                            fig,ax = plt.subplots(1,1, figsize=(7, 10), dpi=300)\n",
    "                            mp = ax.imshow(SMI_waxs.img_st,\n",
    "                                    extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]],\n",
    "                                    vmin=np.percentile(SMI_waxs.img_st, 1.0), \n",
    "                                    vmax=np.percentile(SMI_waxs.img_st, 99.0) # Avoid extremities\n",
    "                            )\n",
    "                            plt.colorbar(mp)\n",
    "                            ax.set_title(f\"{sample}\\n{en} eV - {ai} deg\")\n",
    "                            # Don't save the image, as we use a different display format later.\n",
    "                            # fig.savefig(path_det_img)\n",
    "                            \n",
    "                            if k==0:\n",
    "                                # Plot the azimuthal and radial angles\n",
    "                                colors = ['r', 'orange', 'white'][::-1]\n",
    "                                for angle, width in zip([AZIMUTHAL_INPLANE, AZIMUTHAL_OUTOFPLANE, AZIMUTHAL_RADIAL], [AZIMUTHAL_WIDTH, AZIMUTHAL_WIDTH, RADIAL_WIDTH]):\n",
    "                                    # Generate a set of x points to plot lines of.\n",
    "                                    q_x = np.linspace(0, SMI_waxs.qp[-1], 100)\n",
    "                                    # Calculate the x and y gradients for the lines\n",
    "                                    m1 = np.tan(np.deg2rad(angle - width)) if angle - width != 90 else np.inf\n",
    "                                    m2 = np.tan(np.deg2rad(angle + width)) if angle + width != 90 else np.inf\n",
    "                                    # Calculate the x & y values for the lines\n",
    "                                    q_x1 = q_x if m1 != np.inf else np.zeros(100)\n",
    "                                    q_x2 = q_x if m2 != np.inf else np.zeros(100)\n",
    "                                    y1 = m1 * q_x if m1 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                                    y2 = m2 * q_x if m2 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                                    # Plot the lines\n",
    "                                    color = colors.pop()\n",
    "                                    ax.plot(q_x1, y1, color=color, linestyle='-', label=f\"{angle} deg\")\n",
    "                                    ax.plot(q_x2, y2, color=color, linestyle='-')\n",
    "                                    # If gradient is inf, calculate an alternative fill between\n",
    "                                    if m2 == np.inf:\n",
    "                                            ax.fill_betweenx(y1, q_x1, q_x2, color=color, alpha=0.1)\n",
    "                                    else:\n",
    "                                            ax.fill_between(q_x, y1, y2, color=color, alpha=0.1)\n",
    "                                ax.set_xlim(*SMI_waxs.qp)\n",
    "                                ax.set_ylim(*SMI_waxs.qz)\n",
    "                                ax.legend()\n",
    "                                fig.savefig(path_det_line_profiles_img, dpi=300)\n",
    "                            plt.close() # Save memory\n",
    "                            \n",
    "                            # Perform the radial/azimuthal averaging\n",
    "                            # In plane and out of plane\n",
    "                            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                    azimuth_range=[90 - (AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH) , 90 - (AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                    npt = NPOINTS_RADIAL_AVE)\n",
    "                            q0_IP, I0_IP, I0_IP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                    azimuth_range=[90 - (AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH), 90 - (AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                    npt = NPOINTS_RADIAL_AVE)\n",
    "                            q0_OOP, I0_OOP, I0_OOP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                            # Repeat IP and OOP for the consistency checking\n",
    "                            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                    azimuth_range=[-90+(AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH), -90+(AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                    npt = NPOINTS_RADIAL_AVE)\n",
    "                            q0_IP2, I0_IP2, I0_IP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                    azimuth_range=[-90+(AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH) , -90+(AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                    npt = NPOINTS_RADIAL_AVE)\n",
    "                            q0_OOP2, I0_OOP2, I0_OOP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                            # Radial averaging\n",
    "                            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                    azimuth_range=[90-(AZIMUTHAL_RADIAL - RADIAL_WIDTH), 90-(AZIMUTHAL_RADIAL + RADIAL_WIDTH)], \n",
    "                                                    npt = NPOINTS_RADIAL_AVE)\n",
    "                            q0_R, I0_R, I0_R_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                            # Repeat radial averaging for consistency checking\n",
    "                            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                    azimuth_range=[-90+(AZIMUTHAL_RADIAL - RADIAL_WIDTH), -90+(AZIMUTHAL_RADIAL + RADIAL_WIDTH)], \n",
    "                                                    npt = NPOINTS_RADIAL_AVE)\n",
    "                            q0_R2, I0_R2, I0_R2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "                            \n",
    "                            # Save the line profiles \n",
    "                            header = (\"Main Data\\t\\tMirror-Y axis Data\\t\\n\" \n",
    "                                    + \"q (^-1)\\tI (a.u.)\\tI_err (a.u.)\\tq (^-1)\\tI (a.u.)\\tI_err (a.u.)\\n\")\n",
    "                            delim = \"\\t\"\n",
    "                            kwargs = {\"header\": header, \"delimiter\": delim}\n",
    "                            np.savetxt(path_OOP, np.array([q0_OOP, I0_OOP, I0_OOP_err, q0_OOP2, I0_OOP2, I0_OOP2_err]).T, **kwargs)\n",
    "                            np.savetxt(path_IP, np.array([q0_IP, I0_IP, I0_IP_err, q0_IP2, I0_IP2, I0_IP2_err]).T, **kwargs)\n",
    "                            np.savetxt(path_R, np.array([q0_R, I0_R, I0_R_err, q0_R2, I0_R2, I0_R2_err]).T, **kwargs)\n",
    "                            \n",
    "                            # Update the progress bars\n",
    "                            pbar2.update(1)\n",
    "                pbar1.update(1)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Fitting\n",
    "##### Using example data, define regions for fluorescence collection in the radial scan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLUOR_RANGE: tuple[float, float] = (0.8, 0.92)\n",
    "\"\"\"Q-range for the fluorescence signal summation; define in a flat region away from any peaks/features!\"\"\"\n",
    "\n",
    "# Plot the example data, and plot the fluorescence signal\n",
    "fig: plt.Figure\n",
    "ax: list[plt.Axes]\n",
    "fig,ax = plt.subplots(1,2, figsize=(12,5))\n",
    "for i, sample in enumerate(samples):\n",
    "    sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "    line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "    for j, ai in enumerate(sample_angles[i]):\n",
    "        if not (i==0 or j==0):\n",
    "            continue\n",
    "        test_dataset = [datasets[i][j][test_file] for test_file in test_files]\n",
    "        # Gather the fluor data\n",
    "        fluor_data = np.zeros((len(test_dataset), 3)) # en, I, I_err\n",
    "        for k, fnames in enumerate(test_dataset):\n",
    "            ## Create the results directory for the sample\n",
    "            fname = fnames[0]\n",
    "            # Collect the metadata\n",
    "            en_idx = fname.find('eV_')\n",
    "            en = float(fname[en_idx-7:en_idx])\n",
    "            ai_idx = fname.find(\"_ai\")\n",
    "            ai = float(fname[ai_idx+3:ai_idx+7])\n",
    "            # Generate paths for the output files\n",
    "            path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_R.txt\")\n",
    "\n",
    "            # Load the data\n",
    "            data = np.loadtxt(path_R, skiprows=3, delimiter=\"\\t\")\n",
    "            q = data[:,0]\n",
    "            I = data[:,1]\n",
    "            I_err = data[:,2]\n",
    "            \n",
    "            # Find the fluorescence signal\n",
    "            mask = (q > FLUOR_RANGE[0]) & (q < FLUOR_RANGE[1])\n",
    "            fluor_data[k, 0] = en\n",
    "            fluor_data[k, 1] = np.trapezoid(I[mask], q[mask])\n",
    "            fluor_data[k, 2] = np.trapezoid(I_err[mask], q[mask])\n",
    "            \n",
    "            # Plot the data\n",
    "            ax[0].plot(q, I, label=f\"{en} eV, {ai} deg\")\n",
    "            \n",
    "        ax[1].plot(fluor_data[:,0], fluor_data[:,1], label=f\"{sample} {ai}\")\n",
    "        ax[1].fill_between(fluor_data[:,0], fluor_data[:,1] - fluor_data[:,2], fluor_data[:,1] + fluor_data[:,2], alpha=0.5)\n",
    "    \n",
    "# ax[0].axvspan(*FLUOR_RANGE, color=\"gray\", alpha=0.5)\n",
    "ax[0].set_xlabel(\"q ($^{-1}$)\")\n",
    "ax[0].set_ylabel(\"Intensity (a.u.)\")\n",
    "ax[0].set_title(\"Radial Averaging\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "# ax[0].legend()\n",
    "ax[1].set_xlabel(\"Energy (eV)\")\n",
    "ax[1].set_ylabel(\"Fluorescence Signal (a.u.)\")\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define q regions for truncation, where summation can be performed to observe changes in amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the q-range for the peaks\n",
    "VERT_PEAKS: list[tuple[float, float]] = [(0.33, 0.45),\n",
    "                                    #(0.36, 0.45),\n",
    "                                    # (0.5, 0.75), etc.\n",
    "                                    ]\n",
    "\"\"\"A list of tuples defining the q-range fitting for each peak of interest in the out-of-plane direction\"\"\"\n",
    "HOR_PEAKS: list[tuple[float, float]] = []\n",
    "\"\"\"A list of tuples defining the q-range fitting for each peak of interest in the in-plane direction\"\"\"\n",
    "RAD_PEAKS: list[tuple[float, float]] = []\n",
    "\"\"\"A list of tuples defining the q-range fitting for each peak of interest across the azimuthal\"\"\"\n",
    "\n",
    "VERT_BEAMSTOP: tuple[float, float] = (0.214, 0.278)\n",
    "\"\"\"A q-range specifying a q-range to collect the change in beamstop intensity\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, ai in enumerate(sample_angles[i]):\n",
    "        for test_file in test_files:\n",
    "            # Setup a figure and open the file\n",
    "            fnames = datasets[i][j][test_file]\n",
    "            fname = fnames[0]\n",
    "            print(fname)\n",
    "            # Collect the metadata\n",
    "            en_idx = fname.find('eV_')\n",
    "            en = float(fname[en_idx-7:en_idx])\n",
    "            ai_idx = fname.find(\"_ai\")\n",
    "            ai2 = float(fname[ai_idx+3:ai_idx+7])\n",
    "            assert ai==ai2\n",
    "            \n",
    "            # Collect the line profiles from disk\n",
    "            sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "            line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "            if not os.path.isdir(line_profiles_dir):\n",
    "                os.mkdir(line_profiles_dir)\n",
    "            path_OOP = os.path.normpath(os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_OOP.txt\"))\n",
    "            path_IP = os.path.normpath(os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_IP.txt\"))\n",
    "            path_R = os.path.normpath(os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_R.txt\"))\n",
    "            \n",
    "            data_OOP = np.loadtxt(path_OOP, delimiter=\"\\t\", unpack=True, skiprows=2, dtype=float)\n",
    "            q0_OOP, I0_OOP, I0_OOP_err = data_OOP[0], data_OOP[1], data_OOP[2]\n",
    "            data_IP = np.loadtxt(path_IP, delimiter=\"\\t\", unpack=True, skiprows=2, dtype=float)\n",
    "            q0_IP, I0_IP, I0_IP_err = data_IP[0], data_IP[1], data_IP[2]\n",
    "            data_R = np.loadtxt(path_R, delimiter=\"\\t\", unpack=True, skiprows=2, dtype=float)\n",
    "            q0_R, I0_R, I0_R_err = data_R[0], data_R[1], data_R[2]\n",
    "\n",
    "            # Find the q-range to the region of interest\n",
    "            if len(VERT_PEAKS) > 0:\n",
    "                idx_OOP = np.where((q0_OOP > np.min(VERT_PEAKS)) & (q0_OOP < np.max(VERT_PEAKS)))\n",
    "            else:\n",
    "                idx_OOP = np.arange(len(q0_OOP))\n",
    "            if len(HOR_PEAKS) > 0:\n",
    "                idx_IP = np.where((q0_IP > np.min(HOR_PEAKS)) & (q0_IP < np.max(HOR_PEAKS)))\n",
    "            else:\n",
    "                idx_IP = np.arange(len(q0_IP))\n",
    "            if len(RAD_PEAKS) > 0:\n",
    "                idx_R = np.where((q0_R > np.min(RAD_PEAKS)) & (q0_R < np.max(RAD_PEAKS)))\n",
    "            else:\n",
    "                idx_R = np.arange(len(q0_R))\n",
    "            # Collect the beamstop region\n",
    "            idx_BS = np.where((q0_OOP > np.min(VERT_BEAMSTOP)) & (q0_OOP < np.max(VERT_BEAMSTOP)))\n",
    "                \n",
    "            # Truncate the data to the region of interest\n",
    "            q0_OOP_TR, I0_OOP_TR, I0_OOP_err_TR = q0_OOP[idx_OOP], I0_OOP[idx_OOP], I0_OOP_err[idx_OOP]\n",
    "            q0_IP_TR, I0_IP_TR, I0_IP_err_TR = q0_IP[idx_IP], I0_IP[idx_IP], I0_IP_err[idx_IP]\n",
    "            q0_R_TR, I0_R_TR, I0_R_err_TR = q0_R[idx_R], I0_R[idx_R], I0_R_err[idx_R]\n",
    "\n",
    "            # Plot the data\n",
    "            fig, ax = plt.subplots(max([len(VERT_PEAKS), len(HOR_PEAKS), len(RAD_PEAKS)]), 4, figsize=(12,4))\n",
    "            if len(ax.shape) == 1:\n",
    "                ax = ax.reshape(1,4)\n",
    "            for k, peak in enumerate(VERT_PEAKS):\n",
    "                idx = (q0_OOP_TR > peak[0]) & (q0_OOP_TR < peak[1])\n",
    "                ax[k][0].plot(q0_OOP_TR[idx], I0_OOP_TR[idx], label=\"Out of plane\")\n",
    "                ax[k][0].fill_between(q0_OOP_TR[idx], (I0_OOP_TR - I0_OOP_err_TR)[idx], (I0_OOP_TR + I0_OOP_err_TR)[idx], alpha=0.5)\n",
    "            for k, peak in enumerate(RAD_PEAKS):\n",
    "                idx = (q0_R_TR > peak[0]) & (q0_R_TR < peak[1])\n",
    "                ax[k][1].plot(q0_R_TR[idx], I0_R_TR[idx], label=\"Radial\")\n",
    "                ax[k][1].fill_between(q0_R_TR[idx], (I0_R_TR - I0_R_err_TR)[idx], (I0_R_TR + I0_R_err_TR)[idx], alpha=0.5)\n",
    "            for k, peak in enumerate(HOR_PEAKS):\n",
    "                idx = (q0_IP_TR > peak[0]) & (q0_IP_TR < peak[1])\n",
    "                ax[k][2].plot(q0_IP_TR[idx], I0_IP_TR[idx], label=\"In plane\")\n",
    "                ax[k][2].fill_between(q0_IP_TR[idx], (I0_IP_TR - I0_IP_err_TR)[idx], (I0_IP_TR + I0_IP_err_TR)[idx], alpha=0.5)\n",
    "            ax[0][0].set_title(\"Out of plane\")\n",
    "            ax[0][1].set_title(\"Radial\")\n",
    "            ax[0][2].set_title(\"In plane\")\n",
    "\n",
    "            for a in ax.flatten():\n",
    "                # a.set_xscale(\"log\")\n",
    "                a.set_yscale(\"log\")\n",
    "            # Plot the area under the beamstop region\n",
    "            ax[0][3].plot(q0_OOP[idx_BS], I0_OOP[idx_BS], label=\"Beamstop\")\n",
    "            ax[0][3].fill_between(q0_OOP[idx_BS], I0_OOP[idx_BS] - I0_OOP_err[idx_BS], I0_OOP[idx_BS] + I0_OOP_err[idx_BS], alpha=0.5)\n",
    "            ax[0][3].set_title(\"Beamstop\")\n",
    "            fig.suptitle(sample)\n",
    "            # plt.close()\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attempt some fitting of the peaks\n",
    "###### Define the peakfit functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_law = lambda x, a=1, b=-1, c=0: a * (x-c)**b\n",
    "\"\"\"A power law function for fitting the data\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "    a : float\n",
    "        The amplitude of the power law, by default 1\n",
    "    b : float\n",
    "        The power of the power law, by default -1\n",
    "    c : float\n",
    "        The translational offset of the power law, by default 0\n",
    "\"\"\"\n",
    "\n",
    "lorentz = lambda x, a=1, b=1, c=1: a * c / ((x - b)**2 + c**2)\n",
    "\"\"\"A Lorentzian function for fitting the data\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "    a : float\n",
    "        The amplitude of the Lorentzian, by default 1\n",
    "    b : float\n",
    "        The peak position of the Lorentzian, by default 1\n",
    "    c : float\n",
    "        The width of the Lorentzian, by default 1\n",
    "\"\"\"\n",
    "\n",
    "gauss = lambda x, a=1, b=1, c=1: a * np.exp(-((x - b)**2) / (2 * c**2))\n",
    "\"\"\"A Gaussian function for fitting the data\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "    a : float\n",
    "        The amplitude of the Gaussian, by default 1\n",
    "    b : float\n",
    "        The peak position of the Gaussian, by default 1\n",
    "    c : float\n",
    "        The width of the Gaussian, by default 1\n",
    "\"\"\"\n",
    "# # Without power law centre:\n",
    "fit_fn_lor = lambda x, a,b,c,d,e,f: np.sqrt((lorentz(x,a,b,c) + power_law(x,d,e))**2 + f**2)\n",
    "fit_fn_lor_log = lambda x, a,b,c,d,e,f: np.log(fit_fn_lor(x,a,b,c,d,e,f))\n",
    "fit_fn_gau = lambda x, a,b,c,d,e,f: np.sqrt((gauss(x,a,b,c) + power_law(x,d,e))**2 + f**2)\n",
    "fit_fn_gau_log = lambda x, a,b,c,d,e,f: np.log(fit_fn_gau(x,a,b,c,d,e,f))\n",
    "\n",
    "FLUOR_fit_fn_lor = lambda x, a,b,c,d,e,f: np.trapz(np.sqrt((power_law(x,d,e))**2 + f**2), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a reasonable guess close to the optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial guesses for the peaks\n",
    "VERT_GUESSES: list[tuple] = [\n",
    "    (100,\t0.39,\t0.017,\t50,\t-2.6,\t3000),\n",
    "]\n",
    "\"\"\"A list of tuples defining the initial guesses for each out-of-plane peak of interest\"\"\"\n",
    "HOR_GUESSES: list[tuple] = []\n",
    "\"\"\"A list of tuples defining the initial guesses for each in-plane peak of interest\"\"\"\n",
    "RAD_GUESSES: list[tuple] = []\n",
    "\"\"\"A list of tuples defining the initial guesses for each radial peak of interest\"\"\"\n",
    "\n",
    "# Define the parameter labels\n",
    "VERT_LABELS: list[list[str]] = [\n",
    "    # [\"Gauss Amp.\", \"Gauss Centre\", \"Gauss Width\", \"Powerlaw Amplitude\", \"Powerlaw Power\", \"Offset\"]\n",
    "    [\"Lorentz Amp.\", \"Lorentz Centre\", \"Lorentz Width\", \"Powerlaw Amplitude\", \"Powerlaw Power\", \"Offset\"]\n",
    "    # [\"Lorentz Amp.\", \"Lorentz Centre\", \"Lorentz Width\", \"Powerlaw Amplitude\", \"Powerlaw Power\", \"Sin Freq.\", \"Sin Phase\", \"Sin Amp.\", \"Offset\"]\n",
    "]\n",
    "\"\"\"A list of lists defining the labels for the fitting parameters for each out-of-plane peak\"\"\"\n",
    "HOR_LABELS: list[list[str]] = []\n",
    "\"\"\"A list of lists defining the labels for the fitting parameters for each in-plane peak\"\"\"\n",
    "RAD_LABELS: list[list[str]] = []\n",
    "\"\"\"A list of lists defining the labels for the fitting parameters for each radial peak\"\"\"\n",
    "\n",
    "# Fitting functions\n",
    "# VERT_FIT_FNS: list[Callable] = [fit_fn_lor_sin]\n",
    "VERT_FIT_FNS: list[Callable] = [fit_fn_lor]\n",
    "# VERT_FIT_FNS: list[Callable] = [fit_fn_gau]\n",
    "\"\"\"A list of fitting functions for the out-of-plane peaks\"\"\"\n",
    "HOR_FIT_FNS: list[Callable] = []\n",
    "\"\"\"A list of fitting functions for the in-plane peaks\"\"\"\n",
    "RAD_FIT_FNS: list[Callable] = []\n",
    "\"\"\"A list of fitting functions for the radial peaks\"\"\"\n",
    "\n",
    "#Bounds\n",
    "# VERT_LB : list[tuple | None] = [(0, 0, 0, 0, -np.inf, 0, -180, 0, 0)]\n",
    "VERT_LB : list[tuple | None] = [(0, 0.35, 0.001, 0, -np.inf, 0)]\n",
    "\"\"\"List of tuples defining the lower bounds for the fitting parameters for each out-of-plane peak\"\"\"\n",
    "# VERT_UB : list[tuple | None] = [(np.inf, np.inf, np.inf, np.inf, -1, np.inf, 180, np.inf, np.inf)]\n",
    "VERT_UB : list[tuple | None] = [(np.inf, 0.42, 0.04, np.inf, -1, np.inf)]\n",
    "\"\"\"List of tuples defining the upper bounds for the fitting parameters for each out-of-plane peak\"\"\"\n",
    "VERT_BOUNDS : list[tuple[tuple | None, tuple | None] | None] = [(VERT_LB[i], VERT_UB[i]) for i in range(len(VERT_LB))]\n",
    "\"\"\"List of tuples of lower-bound/upper-bound tuples, defining the bounds for the fitting parameters for each out-of-plane peak\"\"\"\n",
    "HOR_LB : list[tuple | None] = []\n",
    "\"\"\"List of tuples defining the lower bounds for the fitting parameters for each in-plane peak\"\"\"\n",
    "HOR_UB : list[tuple | None] = []\n",
    "\"\"\"List of tuples defining the upper bounds for the fitting parameters for each in-plane peak\"\"\"\n",
    "HOR_BOUNDS : list[tuple[tuple | None, tuple | None] | None] = [(HOR_LB[i], HOR_UB[i]) for i in range(len(HOR_LB))]\n",
    "\"\"\"List of tuples of lower-bound/upper-bound tuples, defining the bounds for the fitting parameters for each in-plane peak\"\"\"\n",
    "RAD_LB : list[tuple | None] = []\n",
    "\"\"\"List of tuples defining the lower bounds for the fitting parameters for each radial peak\"\"\"\n",
    "RAD_UB : list[tuple | None] = []\n",
    "\"\"\"List of tuples defining the upper bounds for the fitting parameters for each radial peak\"\"\"\n",
    "RAD_BOUNDS : list[tuple[tuple | None, tuple | None] | None] = [(RAD_LB[i], RAD_UB[i]) for i in range(len(RAD_LB))]\n",
    "\"\"\"List of tuples of lower-bound/upper-bound tuples, defining the bounds for the fitting parameters for each radial peak\"\"\"\n",
    "\n",
    "# Convert (None, None) tuples to None for the bounds\n",
    "VERT_BOUNDS = [None if bounds == (None, None) else bounds for bounds in VERT_BOUNDS]\n",
    "HOR_BOUNDS = [None if bounds == (None, None) else bounds for bounds in HOR_BOUNDS]\n",
    "RAD_BOUNDS = [None if bounds == (None, None) else bounds for bounds in RAD_BOUNDS]\n",
    "\n",
    "# Check the number of guesses matches the number of peaks\n",
    "assert len(VERT_PEAKS) == len(VERT_GUESSES) == len(VERT_BOUNDS) == len(VERT_LABELS), \"The number of guesses must match the number of peaks and bounds.\"\n",
    "assert len(HOR_PEAKS) == len(HOR_GUESSES) == len(HOR_BOUNDS) == len(HOR_LABELS), \"The number of guesses must match the number of peaks and bounds.\"\n",
    "assert len(RAD_PEAKS) == len(RAD_GUESSES) == len(RAD_BOUNDS) == len(RAD_LABELS), \"The number of guesses must match the number of peaks and bounds.\"\n",
    "\n",
    "# Pack the fitting functions, guesses and bounds into a single iterable\n",
    "FIT_REGISTER: list[tuple[Literal[\"OOP\"] | Literal[\"IP\"] | Literal[\"RAD\"],\n",
    "                         tuple[float, float],\n",
    "                         Callable, \n",
    "                         tuple, \n",
    "                         tuple[tuple | None, tuple | None] | None,\n",
    "                         list[str]]\n",
    "                   ] = []\n",
    "\"\"\"A list of tuples defining:\n",
    "    1. The peak type (e.g. 'OOP', 'IP', 'RAD')\n",
    "    2. The peak q-range\n",
    "    3. The fitting function\n",
    "    4. The initial guesses tuple\n",
    "    5. The bounds tuple (lower|None, upper|None) | None matching length of guesses tuple.\n",
    "    6. The list of parameter labels (corresponding to 4,5).\n",
    "\"\"\"\n",
    "for i, peak in enumerate(VERT_PEAKS):\n",
    "    FIT_REGISTER.append((\"OOP\", peak, VERT_FIT_FNS[i], VERT_GUESSES[i], VERT_BOUNDS[i], VERT_LABELS[i]))\n",
    "for i, peak in enumerate(HOR_PEAKS):\n",
    "    FIT_REGISTER.append((\"IP\", peak, HOR_FIT_FNS[i], HOR_GUESSES[i], HOR_BOUNDS[i], HOR_LABELS[i]))\n",
    "for i, peak in enumerate(RAD_PEAKS):\n",
    "    FIT_REGISTER.append((\"RAD\", peak, RAD_FIT_FNS[i], RAD_GUESSES[i], RAD_BOUNDS[i], RAD_LABELS[i]))\n",
    "\n",
    "# Display the initial guesses over the data\n",
    "FIT_N = max([len(VERT_PEAKS), len(HOR_PEAKS), len(RAD_PEAKS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, ai in enumerate(sample_angles[i]):\n",
    "        for test_file in test_files:\n",
    "            # Setup a figure and open the file\n",
    "            fnames = datasets[i][j][test_file]\n",
    "            fname = fnames[0]\n",
    "            print(fname)\n",
    "            # Collect the metadata\n",
    "            en_idx = fname.find('eV_')\n",
    "            en = float(fname[en_idx-7:en_idx])\n",
    "            ai_idx = fname.find(\"_ai\")\n",
    "            ai2 = float(fname[ai_idx+3:ai_idx+7])\n",
    "            assert ai==ai2\n",
    "            \n",
    "            # Collect the line profiles from disk\n",
    "            sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "            line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "            if not os.path.isdir(line_profiles_dir):\n",
    "                os.mkdir(line_profiles_dir)\n",
    "            path_OOP = os.path.normpath(os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_OOP.txt\"))\n",
    "            path_IP = os.path.normpath(os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_IP.txt\"))\n",
    "            path_R = os.path.normpath(os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_R.txt\"))\n",
    "            \n",
    "            data_OOP = np.loadtxt(path_OOP, delimiter=\"\\t\", unpack=True, skiprows=2, dtype=float)\n",
    "            q0_OOP, I0_OOP, I0_OOP_err = data_OOP[0], data_OOP[1], data_OOP[2]\n",
    "            data_IP = np.loadtxt(path_IP, delimiter=\"\\t\", unpack=True, skiprows=2, dtype=float)\n",
    "            q0_IP, I0_IP, I0_IP_err = data_IP[0], data_IP[1], data_IP[2]\n",
    "            data_R = np.loadtxt(path_R, delimiter=\"\\t\", unpack=True, skiprows=2, dtype=float)\n",
    "            q0_R, I0_R, I0_R_err = data_R[0], data_R[1], data_R[2]\n",
    "\n",
    "            fig, ax = plt.subplots(FIT_N, 3, figsize=(12,3*FIT_N))\n",
    "            if len(ax.shape) == 1:\n",
    "                ax = ax.reshape(1,3)\n",
    "\n",
    "            counts = [0, 0, 0] # Count the number of peaks for each direction\n",
    "            for k, fit in enumerate(FIT_REGISTER):\n",
    "                peak_type, peak_range, fit_fn, guess, bounds, labels = fit\n",
    "                match peak_type:\n",
    "                    case \"OOP\":\n",
    "                        l = 0\n",
    "                        q, I, I_err = q0_OOP, I0_OOP, I0_OOP_err\n",
    "                    case \"IP\":\n",
    "                        l = 1\n",
    "                        q, I, I_err = q0_IP, I0_IP, I0_IP_err\n",
    "                    case \"RAD\":\n",
    "                        l = 2\n",
    "                        q, I, I_err = q0_R, I0_R, I0_R_err\n",
    "                idxs = np.where((q > peak_range[0]) & (q < peak_range[1]))\n",
    "                q = q[idxs]\n",
    "                I = I[idxs]\n",
    "                I_err = I_err[idxs]\n",
    "                \n",
    "                ax[l][counts[l]].set_title(f\"{peak_type} Peak #{k}\")\n",
    "                ax[l][counts[l]].plot(q, I, label=f\"{peak_type} Data\")\n",
    "                ax[l][counts[l]].fill_between(q, I - I_err, I + I_err, alpha=0.5)\n",
    "                ax[l][counts[l]].plot(q, fit_fn(q, *VERT_GUESSES[k]), label=\"Initial Guess\")\n",
    "                popt, pcov = curve_fit(fit_fn, q, I, p0=guess, sigma=I_err, maxfev=100000, bounds=bounds)\n",
    "                ax[l][counts[l]].plot(q, fit_fn(q, *popt), label=\"Fitted\")\n",
    "                ax[l][counts[l]].legend()\n",
    "                display(pd.DataFrame(\n",
    "                    [popt, np.sqrt(np.diag(pcov))],\n",
    "                    columns=labels,\n",
    "                    index=[f\"{peak_type} Peak #{k} Fit\", f\"{peak_type} Peak #{k} Error\"]\n",
    "                ))\n",
    "                counts[l] += 1\n",
    "            fig.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Monte-Carlo Markov Chains to see if the Least Squares solution is really a good fit of random variables\n",
    "##### Define the likelihood, prior and probability functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fns(fit_fn: Callable, bounds: tuple[tuple | None, tuple | None] | None) -> tuple[Callable, Callable, Callable]:\n",
    "    \"\"\"Genereate the log-likelihood, log-prior and log-probability functions for the fitting\"\"\"\n",
    "    def log_likelihood(theta, x, y, yerr, fit_fn: Callable = fit_fn) -> float:\n",
    "        model_y = fit_fn(x, *theta)\n",
    "        sigma2 = yerr**2\n",
    "        return -0.5 * np.sum((y - model_y)**2 / sigma2 )\n",
    "    \n",
    "    def log_prior(theta, bounds: tuple[tuple | None, tuple | None] | None = bounds) -> float:\n",
    "        \"\"\"Define the log-prior function for the fitting\"\"\"\n",
    "        if bounds is None:\n",
    "            return 0.0\n",
    "        lb, ub = bounds\n",
    "        # Return -np.inf if out of bounds.\n",
    "        if lb is not None:\n",
    "            for i in range(len(theta)):\n",
    "                if lb[i] is not None:\n",
    "                    if lb[i] > theta[i]:\n",
    "                        return -np.inf\n",
    "        if not ub is None:\n",
    "            for i in range(len(theta)):\n",
    "                if ub[i] is not None:\n",
    "                    if ub[i] < theta[i]:\n",
    "                        return -np.inf\n",
    "        return 0.0\n",
    "    \n",
    "    def log_probability(theta, x, y, yerr, lprior: Callable = log_prior, llikelihood: Callable = log_likelihood) -> float:\n",
    "        \"\"\"Define the log-probability function for the fitting\"\"\"\n",
    "        lp = lprior(theta)\n",
    "        if not np.isfinite(lp):\n",
    "            return -np.inf\n",
    "        return lp + llikelihood(theta, x, y, yerr)\n",
    "    \n",
    "    return log_likelihood, log_prior, log_probability\n",
    "\n",
    "LOG_LIKELIHOOD_FNS: list[Callable] = []\n",
    "\"\"\"A list of log-likelihood functions for each peak. By default a Gaussian log-likelihood function is used for each parameter, but can be redefined.\"\"\"\n",
    "LOG_PRIOR_FNS: list[Callable] = []\n",
    "\"\"\"A list of log-prior functions for each peak. By default no prior is defined, but can be redefined.\"\"\"\n",
    "LOG_PROBABILITY_FNS: list[Callable] = []\n",
    "\n",
    "for i, fit in enumerate(FIT_REGISTER):\n",
    "    peak_type, peak_range, fit_fn, guess, bounds, labels = fit\n",
    "    log_likelihood, log_prior, log_probability = gen_fns(fit_fn, bounds)\n",
    "    LOG_LIKELIHOOD_FNS.append(log_likelihood)\n",
    "    LOG_PRIOR_FNS.append(log_prior)\n",
    "    LOG_PROBABILITY_FNS.append(log_probability)\n",
    "\n",
    "# Test the log_probability function - should be non-infinite if working.\n",
    "test_guess = FIT_REGISTER[0][3]\n",
    "test_q = q0_OOP_TR if FIT_REGISTER[0][0] == \"OOP\" else q0_IP_TR if FIT_REGISTER[0][0] == \"IP\" else q0_R_TR\n",
    "test_I = I0_OOP_TR if FIT_REGISTER[0][0] == \"OOP\" else I0_IP_TR if FIT_REGISTER[0][0] == \"IP\" else I0_R_TR\n",
    "test_I_err = I0_OOP_err_TR if FIT_REGISTER[0][0] == \"OOP\" else I0_IP_err_TR if FIT_REGISTER[0][0] == \"IP\" else I0_R_err_TR\n",
    "LOG_PROBABILITY_FNS[0](\n",
    "    theta=test_guess,\n",
    "    x=test_q, \n",
    "    y=test_I, \n",
    "    yerr=test_I_err,\n",
    "    lprior=LOG_PRIOR_FNS[0],\n",
    "    llikelihood=LOG_LIKELIHOOD_FNS[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use emcee sampler to run N walkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the data from the first vert peak\n",
    "qmin, qmax = VERT_PEAKS[0]\n",
    "idxs = np.where((q0_OOP_TR > qmin) & (q0_OOP_TR < qmax))\n",
    "x = q0_OOP_TR[idxs]\n",
    "y = I0_OOP_TR[idxs]\n",
    "yerr = I0_OOP_err_TR[idxs]\n",
    "popt, pcov = curve_fit(VERT_FIT_FNS[0], x, y, sigma=yerr, p0=VERT_GUESSES[0], maxfev=10000, bounds=VERT_BOUNDS[0])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Use the model on the first vert peak.\n",
    "N = 200\n",
    "pos = popt * np.ones((N, len(popt))) * (1 + (1.0e-2 * np.random.randn(N, len(popt)) - 5.0e-3))\n",
    "# Check each parameter is within bounds\n",
    "for i in range(len(VERT_BOUNDS[0][0])):\n",
    "    lb, ub = VERT_BOUNDS[0][0][i], VERT_BOUNDS[0][1][i]\n",
    "    if lb is not None:\n",
    "        pos[:,i] = np.clip(pos[:,i], lb, np.inf)\n",
    "    if ub is not None:\n",
    "        pos[:,i] = np.clip(pos[:,i], -np.inf, ub)\n",
    "nwalkers, ndim = pos.shape\n",
    "\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers, ndim, LOG_PROBABILITY_FNS[0], args=(x, y, yerr, LOG_PRIOR_FNS[0], LOG_LIKELIHOOD_FNS[0])\n",
    ")\n",
    "M = 2000\n",
    "sampler.run_mcmc(pos, M, progress=True)\n",
    "max_tau = None\n",
    "while max_tau is None:\n",
    "    try:\n",
    "        tau = sampler.get_autocorr_time()\n",
    "        max_tau = np.max(tau)\n",
    "    except emcee.autocorr.AutocorrError as e:\n",
    "        tau_estimate = int(np.max(e.tau))\n",
    "        if tau_estimate * 50 < 150000:\n",
    "            print(f\"Estimated max tau: {tau_estimate}, running for {50*tau_estimate} more steps.\")\n",
    "            sampler.run_mcmc(None, tau_estimate * 50, progress=False)\n",
    "        else:\n",
    "            print(f\"Tau estimate too high ({tau_estimate}), to run Tau*50 steps, exiting...\")\n",
    "            break\n",
    "    \n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start:.1f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display the chain output and estimate where the burn-in period (tau) is.\n",
    "fig, axes = plt.subplots(len(VERT_LABELS[0]), figsize=(10, 2*len(VERT_LABELS[0])), sharex=True)\n",
    "MC_samples = sampler.get_chain()\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(MC_samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(MC_samples))\n",
    "    ax.set_ylabel(VERT_LABELS[0][i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "axes[-1].set_xlabel(\"step number\")\n",
    "tau_max = np.max(tau)\n",
    "fig.suptitle(f\"Chain Output - Corr time = {tau_max:0.2f} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_MC_samples = sampler.get_chain(discard=int(tau_max * 3), thin=5, flat=True)\n",
    "fig = corner.corner(\n",
    "    flat_MC_samples, labels=VERT_LABELS[0], truths=popt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result of the Least Squares fit and the MCMC fit\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "q = q0_OOP_TR[idxs]\n",
    "ax[0].plot(q0_OOP_TR[idxs], I0_OOP_TR[idxs], label=\"Data\")\n",
    "ax[0].fill_between(q, I0_OOP_TR[idxs] - I0_OOP_err_TR[idxs], I0_OOP_TR[idxs] + I0_OOP_err_TR[idxs], alpha=0.5)\n",
    "ax[0].plot(q, FIT_REGISTER[0][2](q, *popt), label=\"Least Squares Fit\")\n",
    "ax[0].fill_between(q, FIT_REGISTER[0][2](q, *popt - np.sqrt(np.diag(pcov))), FIT_REGISTER[0][2](q, *popt + np.sqrt(np.diag(pcov))), alpha=0.1)\n",
    "ax[0].legend()\n",
    "                   \n",
    "# Plot the MCMC fit\n",
    "MC_samples = sampler.get_chain(discard=200, thin=5, flat=True)\n",
    "percentiles = [np.percentile(MC_samples[:, i], [16, 50, 84]) for i in range(MC_samples.shape[1])] #Sampling at 1 sigma percentiles\n",
    "values = [p[1] for p in percentiles]\n",
    "error_bounds = np.array([[p[0] - p[1], p[2] - p[1]] for p in percentiles])\n",
    "\n",
    "\n",
    "inds = np.random.randint(len(MC_samples), size=100)\n",
    "for ind in inds:\n",
    "    sample = MC_samples[ind]\n",
    "    ax[1].plot(q, FIT_REGISTER[0][2](q, *sample), \"C1\", alpha=0.1, label=\"MCMC Fit\" if ind == inds[0] else None)\n",
    "ax[1].plot(q0_OOP_TR[idxs], I0_OOP_TR[idxs], label=\"Data\")\n",
    "ax[1].fill_between(q, I0_OOP_TR[idxs] - I0_OOP_err_TR[idxs], I0_OOP_TR[idxs] + I0_OOP_err_TR[idxs], alpha=0.5)\n",
    "ax[1].plot(q, FIT_REGISTER[0][2](q, *values), 'black', label=\"MCMC Fit\")\n",
    "ax[1].plot(q, FIT_REGISTER[0][2](q, *popt), label=\"Least Squares Fit\", c='green')\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "display(pd.DataFrame(\n",
    "    [popt, np.sqrt(np.diag(pcov)), values, error_bounds[:,0], error_bounds[:,1]],\n",
    "    columns=VERT_LABELS[0],\n",
    "    index=[f\"Peak #{i} Fit\", f\"Peak #{i} Error\", \"MCMC Fit\", \"MCMC LB Error\", \"MCMC UB Error\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform reduction and analysis across all samples\n",
    "### Detector images and consequent line profile reduction\n",
    "##### Create a plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_E: float = 2440.0\n",
    "MAX_E: float = 2560.0\n",
    "\n",
    "def generate_energy_video_figure(geom: SMI_beamline.SMI_geometry, \n",
    "                              dataset_minimum:float, \n",
    "                              dataset_maximum: float, \n",
    "                              en: float) -> plt.Figure:\n",
    "    SMI_waxs = geom\n",
    "    if len(SMI_waxs.imgs) == 3:\n",
    "        # Works for single detector plots\n",
    "        figsize = (4,5) \n",
    "    else:\n",
    "        # Works for 0, 20  deg detector angles... generic?\n",
    "        qp = SMI_waxs.qp\n",
    "        qz = SMI_waxs.qz\n",
    "        qp_diff = qp[0] - qp[1]\n",
    "        qz_diff = qz[0] - qz[1]\n",
    "        qmax = np.max([qp_diff, qz_diff])\n",
    "        scale = 4\n",
    "        figsize = (qp_diff/qmax * scale * 1.02, qz_diff/qmax * scale)\n",
    "    \n",
    "    fig = plt.figure(figsize = figsize, dpi=150)\n",
    "    gs = fig.add_gridspec(2,2, width_ratios=[1,0.05], height_ratios=[1, 0.05])\n",
    "    ax = fig.add_subplot(gs[0,0])\n",
    "    ax_cmap = fig.add_subplot(gs[0,1])\n",
    "    ax_ai = fig.add_subplot(gs[1,0])\n",
    "\n",
    "    cmap = plt.get_cmap('terrain')\n",
    "    norm = mplc.LogNorm(vmin=dataset_minimum, vmax=dataset_maximum, clip=True)\n",
    "    mp = ax.matshow(SMI_waxs.img_st,\n",
    "            extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]],\n",
    "            cmap = cmap,\n",
    "            norm=norm\n",
    "    )\n",
    "    colorbar = fig.colorbar(mp, cax=ax_cmap, orientation='vertical', location=\"right\", fraction = 0.05)\n",
    "    colorbar.ax.set_ylabel(\"Intensity (a.u.)\", fontsize=4)\n",
    "    colorbar.ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax2 = ax_ai\n",
    "    ax2.yaxis.set_ticks([])\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.set_xscale(\"linear\")\n",
    "    ax2.set_xlim(MIN_E, MAX_E)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=4)\n",
    "    ax2.plot([en, en], [0, 1], 'r')\n",
    "    ax2.set_xlabel(f\"Beam Energy     {en:0.3f} eV\", loc=\"left\", fontsize=4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "if \"dataset_minimum\" in locals():\n",
    "    plt.ion()\n",
    "    matplotlib.interactive(True)\n",
    "    \n",
    "    SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "    apply_detector_mask(SMI_waxs)\n",
    "    apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "    energies_idx = flux_energies.index(en) # for normalisation\n",
    "    for img in SMI_waxs.imgs:\n",
    "        img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "    SMI_waxs.stitching_data(interp_factor=3, flag_scale=False, perpendicular=True)\n",
    "    \n",
    "    fig = generate_energy_video_figure(SMI_waxs, 1, dataset_maximum, en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reduce the lineprofiles and generate fluorescence profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDE: bool = False\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\"\n",
    "\n",
    "# Check the timing performance\n",
    "timing:bool = False\n",
    "import datetime\n",
    "tcheck = datetime.datetime.now()\n",
    "\n",
    "# Run the script\n",
    "plt.ioff()\n",
    "plt.close('all')\n",
    "matplotlib.interactive(False)\n",
    "with tqdm.notebook.tqdm(total=len(samples), position=0, leave=True, desc=\"All Samples\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        ## Create the results directory for the sample\n",
    "        sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "        if not os.path.isdir(sample_dir):\n",
    "            os.mkdir(sample_dir)\n",
    "            \n",
    "        ## Create the directories for the images and line profiles\n",
    "        giwaxs_img_dir = os.path.join(sample_dir, \"giwaxs_flatfielded_images\")\n",
    "        if not os.path.isdir(giwaxs_img_dir):\n",
    "            os.mkdir(giwaxs_img_dir)\n",
    "        line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "        if not os.path.isdir(line_profiles_dir):\n",
    "            os.mkdir(line_profiles_dir)\n",
    "            \n",
    "        with tqdm.notebook.tqdm(total=len(sample_angles[i]), position=1, leave=False, desc=\"Angles\") as pbar1:\n",
    "            for j, ai in enumerate(sample_angles[i]):\n",
    "                dataset_maximum = None\n",
    "                dataset_minimum = None\n",
    "                with tqdm.notebook.tqdm(total=len(datasets[i][j]), position=2, leave=False, desc=\"Finding dataset maximum / minimum\") as pbar2:\n",
    "                    for k, fnames in enumerate(datasets[i][j]):\n",
    "                        fname = fnames[0]\n",
    "                        # Collect the metadata\n",
    "                        en_idx = fname.find('eV_')\n",
    "                        en = float(fname[en_idx-7:en_idx])\n",
    "                        ai_idx = fname.find(\"_ai\")\n",
    "                        ai2 = float(fname[ai_idx+3:ai_idx+7])\n",
    "                        assert ai==ai2\n",
    "                        \n",
    "                        # Update the geometry\n",
    "                        if timing:\n",
    "                            tcheck = datetime.datetime.now()\n",
    "                            SMI_waxs.alphai = ai\n",
    "                            tupdate = datetime.datetime.now()\n",
    "                            print(f\"Time to update geometry: {tupdate - tcheck}\")\n",
    "                                      \n",
    "                            \n",
    "                            tcheck = datetime.datetime.now()\n",
    "                            SMI_waxs.wav = en2wav(en)\n",
    "                            tupdate = datetime.datetime.now()\n",
    "                            print(f\"Time to update wavelength: {tupdate - tcheck}\")\n",
    "                            \n",
    "                            # Flatfield / masked normalized data\n",
    "                            tcheck = datetime.datetime.now()\n",
    "                            SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "                            tupdate = datetime.datetime.now()\n",
    "                            print(f\"Time to open data: {tupdate - tcheck}\")\n",
    "                            \n",
    "                            tcheck = datetime.datetime.now()\n",
    "                            apply_detector_mask(SMI_waxs)\n",
    "                            apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "                            tupdate = datetime.datetime.now()\n",
    "                            print(f\"Time to apply flatfield and mask: {tupdate - tcheck}\")\n",
    "                            \n",
    "                            energies_idx = global_energies.tolist().index(en) # for normalisation\n",
    "                            for img in SMI_waxs.imgs:\n",
    "                                img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "                                \n",
    "                            tcheck = datetime.datetime.now()\n",
    "                            SMI_waxs.stitching_data(interp_factor=3, flag_scale=False, timing=True, perpendicular=True)\n",
    "                            tupdate = datetime.datetime.now()\n",
    "                            print(f\"Time to stitch data: {tupdate - tcheck}\")\n",
    "                        \n",
    "                        else:\n",
    "                            # Unnecessary to set the correct geometry and wavelength for each image when finding min/max\n",
    "                            # SMI_waxs.alphai = ai\n",
    "                            # SMI_waxs.wav = en2wav(en)\n",
    "                            SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "                            apply_detector_mask(SMI_waxs)\n",
    "                            apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "                            \n",
    "                            energies_idx = global_energies.tolist().index(en) # for normalisation\n",
    "                            for img in SMI_waxs.imgs:\n",
    "                                img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "                            SMI_waxs.stitching_data(interp_factor=3, flag_scale=False, perpendicular=True)\n",
    "                            \n",
    "                        # Collect the maximum and minimum values\n",
    "                        if dataset_maximum is None:\n",
    "                            dataset_maximum = np.max(SMI_waxs.img_st)\n",
    "                        else:\n",
    "                            dataset_maximum = np.max([dataset_maximum, np.max(SMI_waxs.img_st)])\n",
    "                            \n",
    "                        if dataset_minimum is None:\n",
    "                            dataset_minimum = np.min(SMI_waxs.img_st[SMI_waxs.img_st > 0])\n",
    "                        else:\n",
    "                            min_val = np.min(SMI_waxs.img_st[SMI_waxs.img_st > 0])\n",
    "                            dataset_minimum = np.min([dataset_minimum, min_val])\n",
    "                        pbar2.update(1)\n",
    "                \n",
    "                cmap = plt.get_cmap('viridis')\n",
    "                norm = mplc.LogNorm(vmin=dataset_minimum, vmax=dataset_maximum)\n",
    "                \n",
    "                \n",
    "                # Gather the fluor data\n",
    "                fluor_data = np.zeros((len(datasets[i][j]), 3))\n",
    "                \n",
    "                # Create paths for fluor figure and data\n",
    "                path_flour_fig = os.path.join(sample_dir, f\"{sample}_{ai}_fluorescence.png\")\n",
    "                path_flour_data = os.path.join(sample_dir, f\"{sample}_{ai}_fluorescence_data.txt\")\n",
    "                \n",
    "                with tqdm.notebook.tqdm(total=len(datasets[i][j]), position=1, leave=False, desc=\"Sample dataset\") as pbar2:\n",
    "                    # For each file in the sample\n",
    "                    for k, fnames in enumerate(datasets[i][j]):\n",
    "                        fname = fnames[0]\n",
    "                        # Collect the metadata\n",
    "                        en_idx = fname.find('eV_')\n",
    "                        en = float(fname[en_idx-7:en_idx])\n",
    "                        ai_idx = fname.find(\"_ai\")\n",
    "                        ai2 = float(fname[ai_idx+3:ai_idx+7])\n",
    "                        assert ai2 == ai\n",
    "                        \n",
    "                        # Generate paths for the output files\n",
    "                        path_det_img = os.path.join(giwaxs_img_dir, f\"{sample}_giwaxs_{ai:0.3f}deg_{en:0.2f}eV.png\")\n",
    "                        path_OOP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_OOP.txt\")\n",
    "                        path_IP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_IP.txt\")\n",
    "                        path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_R.txt\")\n",
    "                        path_det_line_profiles_img = os.path.join(sample_dir, f\"{sample}_{ai}_line_profile_angles.png\")\n",
    "                        \n",
    "                        # Do not override the files if they \"all\" exist already. Override partial file sets though.\n",
    "                        if (not OVERRIDE \n",
    "                            and os.path.isfile(path_det_img) \n",
    "                            and os.path.isfile(path_OOP) \n",
    "                            and os.path.isfile(path_IP) \n",
    "                            and os.path.isfile(path_R)\n",
    "                            and os.path.isfile(path_flour_fig)\n",
    "                            and os.path.isfile(path_flour_data)\n",
    "                            and (k != 0 or os.path.isfile(path_det_line_profiles_img))):\n",
    "                            pbar2.total -= 1 # Reduce the total count\n",
    "                            continue\n",
    "                        \n",
    "                        # Update the geometry\n",
    "                        SMI_waxs.alphai = ai\n",
    "                        SMI_waxs.wav = en2wav(en)\n",
    "                        \n",
    "                        # flatfield / masked normalized data\n",
    "                        SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "                        apply_detector_mask(SMI_waxs)\n",
    "                        apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "                        energies_idx = flux_energies.index(en) # for normalisation\n",
    "                        for img in SMI_waxs.imgs:\n",
    "                            img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "                        SMI_waxs.stitching_data(interp_factor=2, flag_scale=True, timing= True, perpendicular=True)\n",
    "                        \n",
    "                        # Setup a figure and open the file\n",
    "                        fig = generate_energy_video_figure(SMI_waxs, 1, dataset_maximum, en)\n",
    "                        fig.savefig(path_det_img)\n",
    "                        ax = fig.get_axes()[0]\n",
    "                        \n",
    "                        if k==0:\n",
    "                            # Plot the azimuthal and radial angles\n",
    "                            colors = ['r', 'orange', 'white'][::-1]\n",
    "                            for angle, width in zip([AZIMUTHAL_INPLANE, AZIMUTHAL_OUTOFPLANE, AZIMUTHAL_RADIAL], [AZIMUTHAL_WIDTH, AZIMUTHAL_WIDTH, RADIAL_WIDTH]):\n",
    "                                # Generate a set of x points to plot lines of.\n",
    "                                q_x = np.linspace(0, SMI_waxs.qp[-1], 100)\n",
    "                                # Calculate the x and y gradients for the lines\n",
    "                                m1 = np.tan(np.deg2rad(angle - width)) if angle - width != 90 else np.inf\n",
    "                                m2 = np.tan(np.deg2rad(angle + width)) if angle + width != 90 else np.inf\n",
    "                                # Calculate the x & y values for the lines\n",
    "                                q_x1 = q_x if m1 != np.inf else np.zeros(100)\n",
    "                                q_x2 = q_x if m2 != np.inf else np.zeros(100)\n",
    "                                y1 = m1 * q_x if m1 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                                y2 = m2 * q_x if m2 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                                # Plot the lines\n",
    "                                color = colors.pop()\n",
    "                                ax.plot(q_x1, y1, color=color, linestyle='-', label=f\"{angle} deg\")\n",
    "                                ax.plot(q_x2, y2, color=color, linestyle='-')\n",
    "                                # If gradient is inf, calculate an alternative fill between\n",
    "                                if m2 == np.inf:\n",
    "                                        ax.fill_betweenx(y1, q_x1, q_x2, color=color, alpha=0.1)\n",
    "                                else:\n",
    "                                        ax.fill_between(q_x, y1, y2, color=color, alpha=0.1)\n",
    "                            ax.set_xlim(*SMI_waxs.qp)\n",
    "                            ax.set_ylim(*SMI_waxs.qz)\n",
    "                            ax.legend()\n",
    "                            fig.savefig(path_det_line_profiles_img, dpi=300)\n",
    "                        plt.close(fig) # Save memory\n",
    "                        \n",
    "                        # Perform the radial/azimuthal averaging\n",
    "                        # In plane and out of plane\n",
    "                        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                azimuth_range=[90 - (AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH) , 90 - (AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                npt = NPOINTS_RADIAL_AVE)\n",
    "                        q0_IP, I0_IP, I0_IP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                azimuth_range=[90 - (AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH), 90 - (AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                npt = NPOINTS_RADIAL_AVE)\n",
    "                        q0_OOP, I0_OOP, I0_OOP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                        # Repeat IP and OOP for the consistency checking\n",
    "                        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                azimuth_range=[-90+(AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH), -90+(AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                npt = NPOINTS_RADIAL_AVE)\n",
    "                        q0_IP2, I0_IP2, I0_IP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                azimuth_range=[-90+(AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH) , -90+(AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                npt = NPOINTS_RADIAL_AVE)\n",
    "                        q0_OOP2, I0_OOP2, I0_OOP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                        # Radial averaging\n",
    "                        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                azimuth_range=[90-(AZIMUTHAL_RADIAL - RADIAL_WIDTH), 90-(AZIMUTHAL_RADIAL + RADIAL_WIDTH)], \n",
    "                                                npt = NPOINTS_RADIAL_AVE)\n",
    "                        q0_R, I0_R, I0_R_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                        # Repeat radial averaging for consistency checking\n",
    "                        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                azimuth_range=[-90+(AZIMUTHAL_RADIAL - RADIAL_WIDTH), -90+(AZIMUTHAL_RADIAL + RADIAL_WIDTH)], \n",
    "                                                npt = NPOINTS_RADIAL_AVE)\n",
    "                        q0_R2, I0_R2, I0_R2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "                        \n",
    "                        # Save the line profiles \n",
    "                        header = (\"Main Data\\t\\tMirror-Y axis Data\\t\\n\" \n",
    "                                + \"q (^-1)\\tI (a.u.)\\tI_err (a.u.)\\tq (^-1)\\tI (a.u.)\\tI_err (a.u.)\\n\")\n",
    "                        delim = \"\\t\"\n",
    "                        kwargs = {\"header\": header, \"delimiter\": delim}\n",
    "                        np.savetxt(path_OOP, np.array([q0_OOP, I0_OOP, I0_OOP_err, q0_OOP2, I0_OOP2, I0_OOP2_err]).T, **kwargs)\n",
    "                        np.savetxt(path_IP, np.array([q0_IP, I0_IP, I0_IP_err, q0_IP2, I0_IP2, I0_IP2_err]).T, **kwargs)\n",
    "                        np.savetxt(path_R, np.array([q0_R, I0_R, I0_R_err, q0_R2, I0_R2, I0_R2_err]).T, **kwargs)\n",
    "                        \n",
    "                        # Calculate the Fluorescence data\n",
    "                        mask = (q0_R > FLUOR_RANGE[0]) & (q0_R < FLUOR_RANGE[1])\n",
    "                        fluor_data[k, 0] = en\n",
    "                        fluor_data[k, 1] = np.trapezoid(I0_R[mask], q0_R[mask])\n",
    "                        fluor_data[k, 2] = np.trapezoid(I0_R_err[mask], q0_R[mask])\n",
    "                        \n",
    "                        \n",
    "                        pbar2.update(1)\n",
    "                        plt.close(\"all\")\n",
    "                        # break # Only do the first file for now.\n",
    "                \n",
    "                # Save and plot the Fluroescence data\n",
    "                idx = np.argsort(fluor_data[:,0]) #sort by energy\n",
    "                fluor_data = fluor_data[idx]\n",
    "                if not os.path.isfile(path_flour_data) or OVERRIDE:\n",
    "                    with open(path_flour_data, \"w\") as f:\n",
    "                        f.write(\"Energy (eV)\\tFluorescence (a.u.)\\tFluorescence Error (a.u.)\\n\")\n",
    "                        np.savetxt(f, fluor_data, delimiter=\"\\t\")\n",
    "                \n",
    "                if not os.path.isfile(path_flour_fig) or OVERRIDE:\n",
    "                    fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "                    ax.plot(fluor_data[:,0], fluor_data[:,1], label=\"Fluorescence\")\n",
    "                    ax.fill_between(fluor_data[:,0], fluor_data[:,1] - fluor_data[:,2], fluor_data[:,1] + fluor_data[:,2], alpha=0.5)\n",
    "                    ax.set_xlabel(\"Energy (eV)\")\n",
    "                    ax.set_ylabel(\"Fluorescence (a.u.)\")\n",
    "                    ax.set_title(f\"{sample} Fluorescence\")\n",
    "                    fig.tight_layout()\n",
    "                    fig.savefig(path_flour_fig)\n",
    "                    plt.close(fig)\n",
    "                pbar1.update(1)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Render a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDE: bool = True\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\"\n",
    "\n",
    "import subprocess\n",
    "with tqdm.notebook.tqdm(total=len(samples), position=0, leave=True, desc=\"All Samples\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        ## REQUIRES GLOB SUPPORT BY LIBAVFILTER, NOT INCLUDED IN BUILD BINARIES AVAILALBLE ONLINE.\n",
    "        # img_path_regex = os.path.normpath(os.path.join(giwaxs_img_dir, f\"*.png\"))\n",
    "        # ffmpeg_cmd = f'ffmpeg -framerate 30 -pattern_type glob -i \"{img_path_regex}\" -c:v libx264 -pix_fmt yuv420p \"{vid_path_output}\"'\n",
    "        \n",
    "        ## Results directory for the sample\n",
    "        sample_dir = os.path.join(RESULT_DIR, sample)    \n",
    "        ## Images directory\n",
    "        giwaxs_img_dir = os.path.join(sample_dir, \"giwaxs_flatfielded_images\")\n",
    "        img_list = os.listdir(giwaxs_img_dir)\n",
    "        \n",
    "        ai_set = set()\n",
    "        for img in img_list:\n",
    "            if not \".png\" in img:\n",
    "                continue\n",
    "            ai_idx = img.find(\"deg_\")\n",
    "            ai = img[ai_idx-5:ai_idx]\n",
    "            if ai in ai_set:\n",
    "                continue\n",
    "            ai_set.add(ai)\n",
    "        \n",
    "        with tqdm.notebook.tqdm(total=len(ai_set), position=1, leave=False, desc=\"Angles\") as pbar2:\n",
    "            for ai in ai_set:\n",
    "                path_img_list =  os.path.normpath(os.path.join(giwaxs_img_dir, f\"img_list_ai{ai}.txt\"))\n",
    "                with open(path_img_list, 'w') as f:\n",
    "                    for img in img_list:\n",
    "                        if not \".png\" in img or not f\"{ai}deg\" in img:\n",
    "                            continue\n",
    "                        # f.write(f\"file '{os.path.normpath(os.path.join(giwaxs_img_dir,img))}' duration 0.033333\\n\")\n",
    "                        f.write(f\"file '{img}'\\noutpoint 0.1\\n\")\n",
    "                vid_path_output = os.path.normpath(os.path.join(sample_dir, f\"{sample}_ai{ai}_beam_energy_scan.mp4\"))\n",
    "                \n",
    "                ## Check if path exists\n",
    "                if os.path.exists(vid_path_output) and not OVERRIDE:\n",
    "                    print(\"Video already exists, skipping...\")\n",
    "                    pbar.update(1)\n",
    "                else:\n",
    "                    if os.path.exists(vid_path_output):\n",
    "                        os.remove(vid_path_output)\n",
    "                    ## Video generation\n",
    "                    ffmpeg_cmd = ['ffmpeg', \n",
    "                                '-f', 'concat',\n",
    "                                #   '-safe', '0'\n",
    "                                '-i', '\"' + path_img_list.replace(\"\\\\\", \"/\")  + '\"',\n",
    "                                '-framerate', '30',\n",
    "                                '-c:v', 'libx264',\n",
    "                                '-shortest',\n",
    "                                '-r', '30',\n",
    "                                '-pix_fmt', 'yuv420p',\n",
    "                                '-vf', '\"pad=ceil(iw/2)*2:ceil(ih/2)*2\"',\n",
    "                                #   '-movflags', 'faststart',\n",
    "                                #   '-vf', 'format=yuv420p',\n",
    "                                '\"' + vid_path_output + '\"']\n",
    "                    a = subprocess.run(args=\" \".join(ffmpeg_cmd), capture_output=True)\n",
    "                    # Check for errors!\n",
    "                    display(a.stderr.splitlines())\n",
    "                pbar2.update(1)\n",
    "        pbar.update(1)\n",
    "        if i != len(samples) - 1:\n",
    "            display(\"--------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the video to check for any inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "for i, sample in enumerate(samples):\n",
    "    ## Results directory for the sample\n",
    "    sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "    file_list = [file for file in os.listdir(sample_dir) if \".mp4\" in file]\n",
    "    file_paths = [os.path.normpath(os.path.join(sample_dir, file)) for file in file_list]\n",
    "    videos = [Video(file_path, embed=True) for file_path in file_paths]\n",
    "    for name, video in zip(file_list, videos):\n",
    "        display(name, video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Fitting and MC Sampling\n",
    "##### Reset magic and matplotlib to avoid memory leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ipython causes memory leaks when using interactive matplotlib and generating lots of graphs.\n",
    "# https://github.com/ipython/ipython/issues/7270#issuecomment-355276432\n",
    "%matplotlib inline\n",
    "matplotlib.interactive(False)\n",
    "plt.ioff()\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Least squares fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDE: bool = False\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\"\n",
    "\n",
    "USE_GUESS_FIRST: bool = True\n",
    "\"\"\"Whether to use the initial guess for the least squares fit instead of the previous fit result.\"\"\"\n",
    "\n",
    "dataset_fits: list[list[list[npt.NDArray[np.float64]]]] = [[[\n",
    "        np.zeros(\n",
    "            (len(datasets[i][j]), 1 + len(FIT_REGISTER[k][3]) * 2)\n",
    "        )\n",
    "        for k in range(len(FIT_REGISTER))]\n",
    "    for j in range(len(sample_angles[i]))]\n",
    "for i in range(len(samples))]\n",
    "\"\"\"For each sample, for each angle, for each peak, the parameters: the scan energy, and a set of the least squares popt values and errors\"\"\"\n",
    "\n",
    "dataset_beamstop_area: list[list[npt.NDArray[np.float64]]] = [[np.zeros((len(datasets[i][j]), 3))\n",
    "                                                               for j in range(len(sample_angles[i]))\n",
    "                                                               ]\n",
    "                                                              for i in range(len(samples))]\n",
    "\"\"\"For each sample, for each angle, the scan energy, the beamstop area and error\"\"\"\n",
    "\n",
    "with tqdm.notebook.tqdm(total=len(samples), position=3, leave=True, desc=\"Sample\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        # Setup sample dependent variables\n",
    "        sample_fits = dataset_fits[i]\n",
    "        \n",
    "        # Create the results directory for the sample\n",
    "        sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "        if not os.path.isdir(sample_dir):\n",
    "            os.mkdir(sample_dir)\n",
    "        fits_img_dir = os.path.join(sample_dir, \"ls_fit_images\")\n",
    "        if not os.path.isdir(fits_img_dir):\n",
    "            os.mkdir(fits_img_dir)\n",
    "            \n",
    "        with tqdm.notebook.tqdm(total=len(sample_angles[i]), position=2, leave=False, desc=\"Angles\") as pbar1:\n",
    "            for j, ai in enumerate(sample_angles[i]):\n",
    "                # Setup files to store results as they are generated\n",
    "                paths_ls_fits: list[str] = [os.path.join(sample_dir, f\"{sample}-{ai}-{d}-{q0}_to_{q1}-ls_fits.txt\") for d, (q0,q1), _, _, _, _ in FIT_REGISTER]\n",
    "                paths_beamstop_area: str = os.path.join(sample_dir, f\"{sample}-{ai}-beamstop_integrated-area.txt\")\n",
    "                \n",
    "                # First line header\n",
    "                header_ls = [(\"\\t\".join([\"Beam Energy\"] + [\"Fit Params\"]*len(labels) + [\"Fit Errors\"]*len(labels) + [\"Filename\"])) \n",
    "                            + (\"\\n\" + \"\\t\".join([\"Beam Energy\"] + labels + [label + \"_unc\" for label in labels] + [\"Filename\"]))\n",
    "                            for _, _, _, _, _, labels in FIT_REGISTER ]\n",
    "                header_beamstop = \"Beam Energy\\tBeamstop Area\\tBeamstop Area Error\\tFilename\"\n",
    "                \n",
    "                # Load the angles of incidence already processed    \n",
    "                prev_ls_datasets: list[dict[str, np.ndarray]] = [{} for _ in range(len(FIT_REGISTER))]\n",
    "                \"\"\"For each peak, a dictionary containing the previous least squares fits for each file.\"\"\"\n",
    "                prev_beamstop_area: dict[str, np.ndarray] = {}\n",
    "                \"\"\"A dictionary containing the previous beamstop area for each file.\"\"\"\n",
    "                \n",
    "                # Iterate over each peak\n",
    "                with tqdm.notebook.tqdm(total=len(datasets[i][j]), position=1, leave=False, desc=\"Sample Scans\") as pbar2:\n",
    "                    for k in range(len(FIT_REGISTER)):\n",
    "                        # Get the paths for the peak fit files\n",
    "                        path_ls_fits = paths_ls_fits[k]\n",
    "                        # Check if the file exists already, and load previous/existing data.\n",
    "                        if os.path.isfile(path_ls_fits) and not OVERRIDE:\n",
    "                            # Ignore the UserWarning from numpy for empty files - taken care of below.\n",
    "                            with warnings.catch_warnings():\n",
    "                                warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                                prev_ls_fits = np.loadtxt(path_ls_fits, delimiter=\"\\t\", skiprows=2, dtype=str)\n",
    "                            \n",
    "                            if len(prev_ls_fits) == 0:\n",
    "                                # No data in the file, skip loading.\n",
    "                                continue\n",
    "                            if len(prev_ls_fits.shape) == 1:\n",
    "                                prev_ls_fits = prev_ls_fits.reshape(1, -1)\n",
    "                            # Collect common fits and load into memory\n",
    "                            for fname in prev_ls_fits[:, -1]:\n",
    "                                # Check if the file has been MCMC fitted as well, and is loadable data:\n",
    "                                if fname in datasets[i][j]:\n",
    "                                    idx = np.where(prev_ls_fits[:, -1] == fname)[0][0]\n",
    "                                    prev_ls_datasets[k][fname] = prev_ls_fits[idx, :-1].astype(float)\n",
    "                            print(f\"Loaded {len(prev_ls_datasets[k])} previous fits for peak {k}.\")\n",
    "                    \n",
    "                    # Load the beamstop area data\n",
    "                    if os.path.isfile(paths_beamstop_area) and not OVERRIDE:\n",
    "                        # Ignore the UserWarning from numpy for empty files - taken care of below.\n",
    "                        with warnings.catch_warnings():\n",
    "                            warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                            prev_beamstop_data = np.loadtxt(paths_beamstop_area, delimiter=\"\\t\", skiprows=1, dtype=str)\n",
    "                            \n",
    "                        if len(prev_beamstop_data) == 0:\n",
    "                            # No data in the file, skip loading.\n",
    "                            pass\n",
    "                        else:\n",
    "                            if len(prev_beamstop_data.shape) == 1:\n",
    "                                prev_beamstop_data = prev_beamstop_data.reshape(1, -1)\n",
    "                            for fname in prev_beamstop_data[:, -1]:\n",
    "                                if fname in datasets[i][j]:\n",
    "                                    idx = np.where(prev_beamstop_data[:, -1] == fname)[0][0]\n",
    "                                    prev_beamstop_area[fname] = prev_beamstop_data[idx, :-1].astype(float)\n",
    "                            print(f\"Loaded {len(prev_beamstop_area)} previous beamstop areas.\")\n",
    "                    \n",
    "                    # Begin writing the files\n",
    "                    open_ls_fits = [open(paths_ls_fits[k], \"w\") for k in range(len(FIT_REGISTER))]\n",
    "                    open_beamstop_area = open(paths_beamstop_area, \"w\")\n",
    "                    try: # Ensure the files are closed\n",
    "                        # Write the headers and previous data\n",
    "                        for k in range(len(FIT_REGISTER)):\n",
    "                            open_ls_fits[k].write(header_ls[k])\n",
    "                            for fname in prev_ls_datasets[k].keys():\n",
    "                                open_ls_fits[k].write(\"\\n\" + \"\\t\".join([str(val) for val in prev_ls_datasets[k][fname]] + [fname]))\n",
    "                        \n",
    "                        # Write the beamstop area header and previous data\n",
    "                        open_beamstop_area.write(header_beamstop)\n",
    "                        for fname in prev_beamstop_area.keys():\n",
    "                            open_beamstop_area.write(\"\\n\" + \"\\t\".join([str(val) for val in prev_beamstop_area[fname]] + [fname]))\n",
    "                                \n",
    "                        # Loop over all files in the sample dataset\n",
    "                        for l, fnames in enumerate(datasets[i][j]):\n",
    "                            if None in fnames:\n",
    "                                pbar2.total -= 1\n",
    "                                continue\n",
    "                            fname: str = fnames[0]\n",
    "                            # Check if the file has already been processed\n",
    "                            #-----------------------------------------------\n",
    "                            if not OVERRIDE and all(\n",
    "                                [fname in prev_ls_dataset.keys()\n",
    "                                for prev_ls_dataset in prev_ls_datasets]\n",
    "                                ):\n",
    "                                print(f\"File data already exists for all peaks, skipping `{fname}`.\")\n",
    "                                # Reload the existing data and skip.\n",
    "                                for k in range(len(FIT_REGISTER)):\n",
    "                                    dataset_fits[i][j][k][l,:] = prev_ls_datasets[k][fname]\n",
    "                                if fname in prev_beamstop_area:\n",
    "                                    dataset_beamstop_area[i][j][l,:] = prev_beamstop_area[fname]\n",
    "                                pbar2.total -= 1\n",
    "                                continue\n",
    "                            \n",
    "                            ### Otherwise, load the data and perform the fits\n",
    "                            # Collect the metadata\n",
    "                            en_idx = fname.find('eV_')\n",
    "                            en = float(fname[en_idx-7:en_idx])\n",
    "                            ai_idx = fname.find(\"_ai\")\n",
    "                            ai = float(fname[ai_idx+3:ai_idx+7])\n",
    "                            \n",
    "                            # Setup the file dependent output paths\n",
    "                            path_det_line_profiles_img = os.path.join(sample_dir, f\"{sample}_line_profile_angles.png\")\n",
    "                            \n",
    "                            # Collect the line profiles from disk\n",
    "                            line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "                            if not os.path.isdir(line_profiles_dir):\n",
    "                                os.mkdir(line_profiles_dir)\n",
    "                            path_OOP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_OOP.txt\")\n",
    "                            path_IP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_IP.txt\")\n",
    "                            path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_R.txt\")\n",
    "                            \n",
    "                            q0_OOP, I0_OOP, I0_OOP_err, _, _, _ = np.loadtxt(path_OOP, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                            q0_IP, I0_IP, I0_IP_err, _, _, _ = np.loadtxt(path_IP, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                            q0_R, I0_R, I0_R_err, _, _, _ = np.loadtxt(path_R, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                            \n",
    "                            # Beamstop region of interest:\n",
    "                            BS_idx = np.where((q0_OOP > VERT_BEAMSTOP[0]) & (q0_OOP < VERT_BEAMSTOP[1]))\n",
    "                            q0_BS, I0_BS, I0_BS_err = q0_OOP[BS_idx], I0_OOP[BS_idx], np.sqrt(I0_OOP[BS_idx])\n",
    "                            beamstop_data = [en, np.trapezoid(I0_BS, q0_BS), np.trapezoid(np.sqrt(I0_BS), q0_BS)]\n",
    "                            dataset_beamstop_area[i][j][l, :] = beamstop_data\n",
    "                            open_beamstop_area.write(\"\\n\" + \"\\t\".join([str(param) for param in (beamstop_data + [fname])]))\n",
    "                            \n",
    "                            # Perform a fit to the data for each listed peak.\n",
    "                            for k, (d, peak, fit_fn, guess, bounds, labels) in enumerate(FIT_REGISTER):\n",
    "                                # Setup the data\n",
    "                                if d == \"OOP\":\n",
    "                                    q0, I0, I0_err = q0_OOP, I0_OOP, I0_OOP_err\n",
    "                                elif d == \"IP\":\n",
    "                                    q0, I0, I0_err = q0_IP, I0_IP, I0_IP_err\n",
    "                                elif d == \"RAD\":\n",
    "                                    q0, I0, I0_err = q0_R, I0_R, I0_R_err\n",
    "                                \n",
    "                                # Remove rows with NaN or zero values and collect the fit subset data\n",
    "                                idx = np.where((~np.isnan(I0)) & (I0 != 0) & (q0 > peak[0]) & (q0 < peak[1]))\n",
    "                                q, I, I_err = q0[idx], I0[idx], I0_err[idx]\n",
    "                                    \n",
    "                                # Define the output paths\n",
    "                                fits_img_peak_dir = os.path.join(fits_img_dir, f\"{d}-{peak[0]}_to_{peak[1]}_per\")\n",
    "                                if not os.path.isdir(fits_img_peak_dir):\n",
    "                                    os.mkdir(fits_img_peak_dir)\n",
    "                                path_fit_img = os.path.join(fits_img_peak_dir, f\"{sample}_fit-{d}-peak_{peak[0]}-{peak[1]}_per-{en:0.2f}eV-{ai:0.3f}deg.png\")\n",
    "                                \n",
    "                                # Perform a least squres fit\n",
    "                                if USE_GUESS_FIRST:\n",
    "                                    try: \n",
    "                                        popt, pcov = curve_fit(fit_fn, q, I, sigma=I_err, p0=guess, maxfev=100000, bounds=bounds)\n",
    "                                    except RuntimeError as e:\n",
    "                                        if l > 0:\n",
    "                                            print(\"Re-attempting fit again with previous fit instead of initial guess.\")\n",
    "                                            popt, pcov = curve_fit(fit_fn, q, I, sigma=I_err, p0=dataset_fits[i][j][k][l-1, 1:1+len(guess)], maxfev=100000, bounds=bounds)\n",
    "                                        else:\n",
    "                                            raise e\n",
    "                                else:\n",
    "                                    try:\n",
    "                                        if l > 0:\n",
    "                                            popt, pcov = curve_fit(fit_fn, q, I, sigma=I_err, p0=dataset_fits[i][j][k][l-1, 1:1+len(guess)], maxfev=100000, bounds=bounds)\n",
    "                                        else:\n",
    "                                            popt, pcov = curve_fit(fit_fn, q, I, sigma=I_err, p0=guess, maxfev=100000, bounds=bounds)\n",
    "                                    except RuntimeError as e:\n",
    "                                        print(\"Re-attempting fit again with initial guess instead of previous fit.\")\n",
    "                                        popt, pcov = curve_fit(fit_fn, q, I, sigma=I_err, p0=guess, maxfev=100000, bounds=bounds)\n",
    "                                data_line = np.r_[[en], popt, np.sqrt(np.diag(pcov))]\n",
    "                                dataset_fits[i][j][k][l, :] = data_line\n",
    "                                open_ls_fits[k].write(\"\\n\" + \"\\t\".join([str(val) for val in data_line] + [fname])) #Save the data\n",
    "\n",
    "                                # Plot the fit\n",
    "                                fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "                                ax.plot(q, I, label=\"Data\")\n",
    "                                ax.fill_between(q, I - I_err, I + I_err, alpha=0.2)\n",
    "                                ax.plot(q, fit_fn(q, *popt), label=\"Least Squares Fit\")\n",
    "                                # ax.fill_between(q, fit_fn(q, *popt - np.sqrt(np.diag(pcov))), fit_fn(q, *popt + np.sqrt(np.diag(pcov))), alpha=0.1)\n",
    "                                ax.set_ylim(0, 1.1 * np.max(I))\n",
    "                                ax.set_title(f\"{sample}\\n{ai} deg - {en} eV - Peak {peak[0]}-{peak[1]}\")\n",
    "                                fig.savefig(path_fit_img)\n",
    "                                fig.tight_layout()\n",
    "                                plt.close() # Save memory\n",
    "                            plt.close(\"all\")\n",
    "                            pbar2.update(1)\n",
    "                            \n",
    "                        # Close the files\n",
    "                        for f1 in open_ls_fits:\n",
    "                            f1.close()\n",
    "                        open_beamstop_area.close()\n",
    "                        \n",
    "                        # Generate final graphics for the angle of incidence fits to each peak.\n",
    "                        #-----------------------------------------------------------------------\n",
    "                        for k, (d, peak, fit_fn, guess, bounds, labels) in enumerate(FIT_REGISTER):\n",
    "                            path_ls_summary = os.path.join(sample_dir, f\"{sample}-{ai}-{d}_Peak-{peak[0]}_to_{peak[1]}-_ls_fits_summary.png\")\n",
    "                        \n",
    "                            # Load the data\n",
    "                            ls_fits = dataset_fits[i][j][k]\n",
    "                            # Re-index the data\n",
    "                            angles = ls_fits[:, 0]\n",
    "                            ls_popt = ls_fits[:, 1:1+len(labels)]\n",
    "                            ls_errors = ls_fits[:, 1+len(labels):]\n",
    "                            \n",
    "                            # Plot the LS data\n",
    "                            fig, ax = plt.subplots(len(labels), 1, figsize=(8, 2*len(labels)))\n",
    "                            for z in range(len(labels)):\n",
    "                                ax[z].plot(angles, ls_popt[:, z], label=labels[z])\n",
    "                                ax[z].fill_between(angles, ls_popt[:, z] - ls_errors[:, z], ls_popt[:, z] + ls_errors[:, z], alpha=0.1)\n",
    "                                ax[z].set_ylabel(labels[z])\n",
    "                                mn,mx = np.min(ls_popt[:, z]), np.max(ls_popt[:, z])\n",
    "                                diff = np.abs(mx - mn)\n",
    "                                mn_lim = mn - 0.1 * diff\n",
    "                                mx_lim = mx + 0.1 * diff\n",
    "                                ax[z].set_ylim(mn_lim, mx_lim)\n",
    "                            ax[-1].set_xlabel(\"Beam Energy (eV)\")\n",
    "                            fig.suptitle(f\"{sample} - {ai}deg AOI\\n{d} Peak {peak[0]:0.2f} to {peak[1]:0.2f} - Least Squares Fit Summary\")\n",
    "                            fig.savefig(path_ls_summary)\n",
    "                            fig.tight_layout()\n",
    "                            plt.close()\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {sample}: {e}\")\n",
    "                        # Close the files\n",
    "                        for f1 in open_ls_fits:\n",
    "                            f1.close()\n",
    "                        open_beamstop_area.close()\n",
    "                        raise e\n",
    "                pbar1.update(1)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional close command in case files are still open\n",
    "for f1 in open_ls_fits:\n",
    "    f1.close()\n",
    "open_beamstop_area.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a global fit?\n",
    "\n",
    "OVERRIDE: bool = False\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\"\n",
    "\n",
    "USE_INDIVIDUAL_RESULTS: bool = True\n",
    "\"\"\"Whether to use the individual previous fit results as the guess parameters.\"\"\"\n",
    "\n",
    "# Define new global fitting functions\n",
    "def fit_fn_lor_global(x:npt.NDArray, lor_pos:float, lor_width:float, *args: list[float]):\n",
    "    \"\"\"\n",
    "    Generates a Lorentzian function for a global fitting function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : npt.array\n",
    "        2D array of x values, the first index is the particular series, the second is the x values.\n",
    "    lor_pos : float\n",
    "        The global position of the Lorentzian peak.\n",
    "    lor_width : float\n",
    "        The global width of the Lorentzian peak.\n",
    "    *args : list[float]\n",
    "        The parameters for the Lorentzian function.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    if len(args) % 4 != 0:\n",
    "        raise ValueError(f\"Incorrect number of arguments ({len(args)}) for the Lorentzian function (expected multiple of 4).\")\n",
    "    N_PEAKS = len(args) // 4\n",
    "    \n",
    "    if N_PEAKS != x.shape[0]:\n",
    "        raise ValueError(f\"Incorrect number of x series ({x.shape[0]}) for the Lorentzian function (expected {N_PEAKS}).\")\n",
    "    \n",
    "    y = np.zeros_like(x)\n",
    "    for i in range(N_PEAKS):\n",
    "        idx = i*4\n",
    "        # try:\n",
    "        y[i] += fit_fn_lor(x[i], args[idx], lor_pos, lor_width, *args[idx+1:idx+4])\n",
    "        # except Exception as e:\n",
    "        #     print(f\"a: {args[idx]}, b: {lor_pos}, c: {lor_width}, d,e,f: {args[idx+1:idx+4]}\")\n",
    "        #     raise e\n",
    "    return y.flatten()\n",
    "\n",
    "GLOBAL_FIT_MAPPING: dict[Callable, tuple[Callable, list[int]]] = {\n",
    "    fit_fn_lor: (fit_fn_lor_global, [1,2]),\n",
    "}\n",
    "\"\"\"A mapping of peak fitting functions to their global fitting functions and the indices of the global parameters.\"\"\"\n",
    "\n",
    "global_dataset_fits: list[list[list[np.ndarray]]] = [[[np.zeros((1 + 2 * len(datasets[i]) * ( # Datapoints and 2 for value/error pairs.\n",
    "                                                len(FIT_REGISTER[j][3]) # The number of parameters for each peak\n",
    "                                                - len(GLOBAL_FIT_MAPPING[FIT_REGISTER[j][2]])) # Subtract number of global params\n",
    "                                            ,))\n",
    "                                            for j in range(len(FIT_REGISTER))]\n",
    "                                            for a in range(len(sample_angles[i]))]\n",
    "                                            for i in range(len(samples))]\n",
    "\"\"\"For each sample, for each peak, the parameters: the angle of incidence and a set of the least squares popt values and errors\"\"\"\n",
    "\n",
    "# Create the headers for each global peak.\n",
    "global_headers = []\n",
    "for j in range(len(FIT_REGISTER)):\n",
    "    # For each peak collect the labels\n",
    "    labels = FIT_REGISTER[j][5]\n",
    "    # Remove global labels\n",
    "    rem_idx = GLOBAL_FIT_MAPPING[FIT_REGISTER[j][2]][1]\n",
    "    rem_idx.sort()\n",
    "    # Truncate labels to remove the global labels\n",
    "    global_labels = [label for k, label in enumerate(labels) if k in rem_idx]\n",
    "    labels = [label for k, label in enumerate(labels) if k not in rem_idx]\n",
    "    # Add the labels to the register\n",
    "    global_headers.append(\"\\t\".join([\"Beam Energy\"] + [\"Global Params\"]*len(global_labels) + [\"Fit Params\"]*len(labels)\n",
    "                                + [\"Global Errors\"]*len(global_labels) + [\"Fit Errors\"]*len(labels))\n",
    "                    + \"\\n\"\n",
    "                    + \"\\t\".join([\"Energy (eV)\"] + global_labels + labels\n",
    "                                + [label + \"_unc\" for label in global_labels + labels])\n",
    "                    )\n",
    "\n",
    "with tqdm.notebook.tqdm(total=int(len(samples)), position=2, leave=True, desc=\"Sample\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        if any([name in sample for name in []]):\n",
    "            pbar.total -= 1\n",
    "            continue\n",
    "        # Create the results directory for the sample\n",
    "        sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "        if not os.path.isdir(sample_dir):\n",
    "            os.mkdir(sample_dir)\n",
    "            \n",
    "        with tqdm.notebook.tqdm(total=len(sample_angles[i]), position=2, leave=False, desc=\"Angles\") as pbar1:\n",
    "            for a, ai in enumerate(sample_angles[i]):\n",
    "                \n",
    "                # Setup files to store results.\n",
    "                path_global_fits: list[str] = [os.path.join(sample_dir, f\"{sample}-{ai:0.2f}-{d}-{q0}_to_{q1}-global_ls_fit.txt\") for d, (q0,q1), _, _, _, _ in FIT_REGISTER]    \n",
    "                # Iterate over each peak\n",
    "                with tqdm.notebook.tqdm(total=len(FIT_REGISTER), position=1, leave=False, desc=\"Global Peak Fits\") as pbar2:\n",
    "                    for j, (d, (qmin, qmax), fn, guess, bounds, _) in enumerate(FIT_REGISTER):\n",
    "                        # Get the paths for the global peak fit\n",
    "                        path_global_fit = path_global_fits[j]\n",
    "                        path_global_fit_img = path_global_fit.replace(\".txt\", \".png\")\n",
    "                        path_global_fit_summary = path_global_fit.replace(\".txt\", \"_summary.png\")\n",
    "                        \n",
    "                        # Check if the file exists already, and load previous/existing data.\n",
    "                        if os.path.isfile(path_global_fit) and not OVERRIDE:\n",
    "                            # Skip\n",
    "                            pbar2.total -= 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Concatenate all data required for the global fit.\n",
    "                        global_x = []\n",
    "                        global_y = []\n",
    "                        global_y_err = []\n",
    "                        en_list = []\n",
    "                        ai_list = []\n",
    "                        for k, fnames in enumerate(datasets[i][a]):\n",
    "                            fname = fnames[0]\n",
    "                            # Collect the metadata\n",
    "                            en_idx = fname.find('eV_')\n",
    "                            en = float(fname[en_idx-7:en_idx])\n",
    "                            ai_idx = fname.find(\"_ai\")\n",
    "                            ai = float(fname[ai_idx+3:ai_idx+7])\n",
    "                        \n",
    "                            # Collect the line profiles from disk\n",
    "                            line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "                            path_OOP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_OOP.txt\")\n",
    "                            path_IP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_IP.txt\")\n",
    "                            path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_R.txt\")\n",
    "                            \n",
    "                            # Load the data\n",
    "                            q0_OOP, I0_OOP, I0_OOP_err, _, _, _ = np.loadtxt(path_OOP, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                            q0_IP, I0_IP, I0_IP_err, _, _, _ = np.loadtxt(path_IP, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                            q0_R, I0_R, I0_R_err, _, _, _ = np.loadtxt(path_R, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                            \n",
    "                            # Truncate the data to the peak region (exclude 0's and nans) and add the data to the global fit\n",
    "                            match FIT_REGISTER[j][0]:\n",
    "                                case \"OOP\":\n",
    "                                    q0_idxs = (q0_OOP > qmin) & (q0_OOP < qmax) & ~np.isnan(I0_OOP) & (I0_OOP != 0)\n",
    "                                    global_x.append(q0_OOP[q0_idxs])\n",
    "                                    global_y.append(I0_OOP[q0_idxs])\n",
    "                                    global_y_err.append(I0_OOP_err[q0_idxs])\n",
    "                                case \"IP\":\n",
    "                                    q0_idxs = (q0_IP > qmin) & (q0_IP < qmax) & ~np.isnan(I0_IP) & (I0_IP != 0)\n",
    "                                    global_x.append(q0_IP[q0_idxs])\n",
    "                                    global_y.append(I0_IP[q0_idxs])\n",
    "                                    global_y_err.append(I0_IP_err[q0_idxs])\n",
    "                                case \"RAD\":\n",
    "                                    q0_idxs = (q0_R > qmin) & (q0_R < qmax) & ~np.isnan(I0_R) & (I0_R != 0)\n",
    "                                    global_x.append(q0_R[q0_idxs])\n",
    "                                    global_y.append(I0_R[q0_idxs])\n",
    "                                    global_y_err.append(I0_R_err[q0_idxs])\n",
    "                                case _:\n",
    "                                    raise ValueError(\"Invalid data type for global fit.\")\n",
    "                            \n",
    "                            en_list.append(en)\n",
    "                            ai_list.append(ai)\n",
    "                                \n",
    "                        # Put data into a 2D array. If size doesn't match for all columns, then truncate the lowest common length\n",
    "                        # This is essential so each fit has the same number of data points, and can be assigned individual parameters.\n",
    "                        lcl: int = global_x[0].shape[0] # Lowest common length\n",
    "                        homogenous: bool = True # Homogenous length?\n",
    "                        for x in global_x[1:]:\n",
    "                            if x.shape[0] < lcl:\n",
    "                                lcl = x.shape[0]\n",
    "                                homogenous = False\n",
    "                        # Correct the lengths\n",
    "                        if not homogenous:\n",
    "                            for k in range(len(global_x)):\n",
    "                                global_x[k] = global_x[k][:lcl]\n",
    "                                global_y[k] = global_y[k][:lcl]\n",
    "                                global_y_err[k] = global_y_err[k][:lcl]\n",
    "                        # Make into 2D array\n",
    "                        global_x = np.array(global_x)\n",
    "                        global_y = np.array(global_y)\n",
    "                        global_y_err = np.array(global_y_err)\n",
    "                        \n",
    "                        # Perform the global fit\n",
    "                        global_fit_fn, global_params_idxs = GLOBAL_FIT_MAPPING[fn]\n",
    "                        global_guess = []\n",
    "                        global_min = []\n",
    "                        global_max = []\n",
    "                        global_bounds = (global_min, global_max)\n",
    "                        if USE_INDIVIDUAL_RESULTS and not np.all(dataset_fits[i][j] == 0):\n",
    "                            ## Use previous results as guesses\n",
    "                            # average the global parameters from individual locals\n",
    "                            ave = np.mean(dataset_fits[i][a][j], axis=0)\n",
    "                            global_guess.extend([ave[k+1] for k in global_params_idxs]) #Add one for the angle of incidence first column.\n",
    "                            global_min.extend([lb for l, lb in enumerate(bounds[0]) if l in global_params_idxs])\n",
    "                            global_max.extend([ub for l, ub in enumerate(bounds[1]) if l in global_params_idxs])\n",
    "                            # Add the peak parameters\n",
    "                            for k in range(len(datasets[i][a])):\n",
    "                                global_guess.extend([dataset_fits[i][a][j][k, 1 + l] for l in range(len(guess)) if l not in global_params_idxs])\n",
    "                            global_min.extend([lb for l, lb in enumerate(bounds[0]) if l not in global_params_idxs] * len(datasets[i][a]))\n",
    "                            global_max.extend([ub for l, ub in enumerate(bounds[1]) if l not in global_params_idxs] * len(datasets[i][a]))\n",
    "                        else: \n",
    "                            ##Use guesses.\n",
    "                            # Add the global parameters\n",
    "                            global_guess.extend([guess_p for k, guess_p in enumerate(guess) if k in global_params_idxs])\n",
    "                            global_min.extend([lb for k, lb in enumerate(bounds[0]) if k in global_params_idxs])\n",
    "                            global_max.extend([ub for k, ub in enumerate(bounds[1]) if k in global_params_idxs])\n",
    "                            # Add the peak parameters\n",
    "                            global_guess.extend([param for k, param in enumerate(guess) if k not in global_params_idxs] * len(datasets[i][a]))\n",
    "                            global_min.extend([lb for k, lb in enumerate(bounds[0]) if k not in global_params_idxs] * len(datasets[i][a]))\n",
    "                            global_max.extend([ub for k, ub in enumerate(bounds[1]) if k not in global_params_idxs] * len(datasets[i][a]))\n",
    "                        \n",
    "                        # Perform the global fit!\n",
    "                        try:\n",
    "                            popt, pcov = curve_fit(global_fit_fn, global_x, global_y.flatten(), # Flattening is required for the regression calculation. Global fit also returns flat.\n",
    "                                                sigma=global_y_err.flatten(), \n",
    "                                                p0=global_guess, bounds=global_bounds,\n",
    "                                                # maxfev=100*len(datasets[i]),\n",
    "                                                maxfev=100000,\n",
    "                                                )\n",
    "                        except RuntimeError as e:\n",
    "                            error = e\n",
    "                            display(e.args)\n",
    "                            raise e\n",
    "                        perr = np.sqrt(np.diag(pcov))\n",
    "                        # popt = global_guess\n",
    "                        # perr = np.zeros_like(popt)\n",
    "                        \n",
    "                        peak = (qmin, qmax)\n",
    "                        \n",
    "                        # Prepare the data for saving, by arranging the global parameters and the peak parameters\n",
    "                        N_locals = len(guess) - len(global_params_idxs)\n",
    "                        N_globals = len(global_params_idxs)\n",
    "                        global_params = popt[:N_globals]\n",
    "                        global_errs = perr[:N_globals]\n",
    "                        \n",
    "                        lines = []\n",
    "                        for k in range(len(datasets[i][a])):\n",
    "                            if k==0:\n",
    "                                lines.append(np.r_[[en_list[k]], \n",
    "                                                global_params,\n",
    "                                                popt[N_globals + k * N_locals: N_globals + (k+1) * N_locals],\n",
    "                                                global_errs,\n",
    "                                                perr[N_globals + k * N_locals: N_globals + (k+1) * N_locals]])\n",
    "                            else: \n",
    "                                lines.append(np.r_[[en_list[k]], \n",
    "                                                np.ones(N_globals) * np.nan, \n",
    "                                                popt[N_globals + k * N_locals: N_globals + (k+1) * N_locals],\n",
    "                                                np.ones(N_globals) * np.nan, \n",
    "                                                perr[N_globals + k * N_locals: N_globals + (k+1) * N_locals]])\n",
    "                        \n",
    "                        # Write to file\n",
    "                        with open(path_global_fit, \"w\") as f:\n",
    "                            f.write(global_headers[j])\n",
    "                            for line in lines:\n",
    "                                f.write(\"\\n\" + \"\\t\".join([str(val) for val in line]))\n",
    "                        \n",
    "                        # Plot all fitted data\n",
    "                        cmap = plt.get_cmap(\"viridis\")\n",
    "                        import matplotlib.colors as mc\n",
    "                        norm = mc.Normalize(vmin = np.min(en_list), vmax = np.max(en_list))\n",
    "                                        \n",
    "                        y_fit = global_fit_fn(global_x, *popt).reshape(global_x.shape)\n",
    "                            \n",
    "                        fig, ax = plt.subplots(1, 1, figsize=(12, 0.1 * len(datasets[i][a])))\n",
    "                        for k in range(len(datasets[i][a])):\n",
    "                            # Plot each subset of the fit data, and the relative fit.\n",
    "                            offset = k * 0.1\n",
    "                            line = ax.plot(global_x[k], np.log(global_y[k]) + offset, alpha=0.7, linewidth=1, marker=\".\", markersize=2, c=cmap(norm(en_list[k])))\n",
    "                            ax.plot(global_x[k], np.log(y_fit[k]) + offset,\n",
    "                                    alpha=0.8, \n",
    "                                    color=line[0].get_color(),\n",
    "                                    linewidth=1,\n",
    "                            )\n",
    "                        ax.set_title(f\"{sample}\\naoi {ai}, {d} Peak {peak[0]:0.2f} to {peak[1]:0.2f}\\nGlobal Least Squares Fit\")\n",
    "                        ax.set_xlabel(\"q (^-1)\")\n",
    "                        ax.set_ylabel(\"Log Intensity (a.u.)\")\n",
    "                        fig.tight_layout()\n",
    "                        fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax, label=\"Beam Energy (eV)\")\n",
    "                        fig.savefig(path_global_fit_img, transparent=False, dpi=300)\n",
    "                        \n",
    "                        # Create a plot of the parameters:\n",
    "                        fig, axs = plt.subplots(N_locals, 1, figsize=(8, 3*(N_locals)))\n",
    "                        \n",
    "                        labels = FIT_REGISTER[j][5]\n",
    "                        # Remove global labels\n",
    "                        rem_idx = GLOBAL_FIT_MAPPING[FIT_REGISTER[j][2]][1]\n",
    "                        local_labels = [labels[i] for i in range(len(labels)) if i not in rem_idx]\n",
    "                        \n",
    "                        for k in range(N_locals):\n",
    "                            param_data = popt[N_globals + k::N_locals]\n",
    "                            param_err = perr[N_globals + k::N_locals]\n",
    "                            axs[k].plot(en_list, param_data)\n",
    "                            axs[k].fill_between(en_list, param_data - param_err, param_data + param_err, alpha=0.1)\n",
    "                            axs[k].set_ylabel(local_labels[k])\n",
    "                            mn,mx = np.min(param_data), np.max(param_data)\n",
    "                            mn_lim = mn - 0.1 * np.abs(mn)\n",
    "                            mx_lim = mx + 0.1 * np.abs(mx)\n",
    "                            axs[k].set_ylim(mn_lim, mx_lim)\n",
    "                        axs[-1].set_xlabel(\"Beam Energy (eV)\")\n",
    "                            \n",
    "                        fig.suptitle(f\"{sample}\\naoi {ai}, {d} Peak {peak[0]:0.2f} to {peak[1]:0.2f}\\nGlobal Least Squares Fit Summary\")\n",
    "                        fig.tight_layout()\n",
    "                        fig.savefig(path_global_fit_summary)\n",
    "                        \n",
    "                        # Update the fit progress bar\n",
    "                        pbar2.update(1)\n",
    "                # Update the angle progress bar\n",
    "                pbar1.update(1)\n",
    "        # Update the sample progress bar\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCMC_SAMPLING : bool = False\n",
    "\"\"\"Whether to use MCMC sampling to fit the data or not\"\"\"\n",
    "N: int = 200\n",
    "\"\"\"The number of walkers to use in the MCMC sampling\"\"\"\n",
    "M = 1000\n",
    "\"\"\"The default number of steps to use in the MCMC sampling\"\"\"\n",
    "\n",
    "dataset_MC_fits: list[list[np.ndarray]] = [[np.zeros(\n",
    "                                                (len(datasets[i]), 1 + len(FIT_REGISTER[j][3]) * 3)\n",
    "                                            )\n",
    "                                            for j in range(len(FIT_REGISTER))]\n",
    "                                            for i in range(len(samples))]\n",
    "\"\"\"For each sample, for each peak, the parameters: the angle of incidence and a set of the MCMC opt values and lb/ub\"\"\"\n",
    "\n",
    "\n",
    "OVERRIDE: bool = False\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\"\n",
    "\n",
    "with tqdm.notebook.tqdm(total=np.sum([len(datasets[i]) for i in range(len(datasets))]), position=2, leave=True, desc=\"Sample\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        # Setup sample dependent variables\n",
    "        sample_fits = dataset_fits[i]\n",
    "        sample_MC_fits = dataset_MC_fits[i]\n",
    "        \n",
    "        # Create the results directory for the sample\n",
    "        sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "        if not os.path.isdir(sample_dir):\n",
    "            os.mkdir(sample_dir)\n",
    "        mcmc_img_dir = os.path.join(sample_dir, \"mcmc_fit_images\")\n",
    "        if not os.path.isdir(mcmc_img_dir):\n",
    "            os.mkdir(mcmc_img_dir)\n",
    "            \n",
    "        # Setup files to store results as they are generated\n",
    "        paths_mcmc_fits: list[str] = [os.path.join(sample_dir, f\"{sample}-{d}-{q0}_to_{q1}-mcmc_fits.txt\") for d, (q0,q1), _, _, _, _ in FIT_REGISTER]\n",
    "        \n",
    "        # First line header\n",
    "        header_mcmc = [(\"\\t\".join([\"Angle of Incidence\"] + [\"MC Fit Params\"]*len(labels) + [\"MC LB Errors\"]*len(labels) + [\"MC UB Errors\"]*len(labels) + [\"Filename\"]))\n",
    "                    + (\"\\n\" + \"\\t\".join([\"Angle of Incidence\"] + labels + [label + \"_unc_lb\" for label in labels] + [label + \"_unc_ub\" for label in labels] + [\"Filename\"]))\n",
    "                    for _, _, _, _, _, labels in FIT_REGISTER]\n",
    "                \n",
    "        # Load the angles of incidence already processed    \n",
    "        prev_mcmc_datasets: list[dict[str, np.ndarray]] = [{} for _ in range(len(FIT_REGISTER))]\n",
    "        \"\"\"For each peak, a dictionary containing the previous mcmc fits for each file.\"\"\"\n",
    "        \n",
    "        # Iterate over each peak\n",
    "        with tqdm.notebook.tqdm(total=len(datasets[i]), position=1, leave=False, desc=\"Sample Scans\") as pbar2:\n",
    "            for j in range(len(FIT_REGISTER)):\n",
    "                # Get the paths for the peak fit files\n",
    "                path_mcmc_fits = paths_mcmc_fits[j]\n",
    "                # Check if the file exists already, and load previous/existing data.\n",
    "                if os.path.isfile(path_mcmc_fits) and not OVERRIDE:\n",
    "                    # Ignore the UserWarning from numpy for empty files - taken care of below.\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                        prev_mcmc_fits = np.loadtxt(path_mcmc_fits, delimiter=\"\\t\", skiprows=2, dtype=str)\n",
    "                    \n",
    "                    if len(prev_mcmc_fits) == 0:\n",
    "                        # No data in the file, skip loading.\n",
    "                        continue\n",
    "                    if len(prev_mcmc_fits.shape) == 1:\n",
    "                        prev_mcmc_fits = prev_mcmc_fits.reshape(1, -1)\n",
    "                    # Collect common fits and load into memory\n",
    "                    for fname in prev_mcmc_fits[:, -1]:\n",
    "                        # Check if the file has been MCMC fitted as well, and is loadable data:\n",
    "                        if fname in datasets[i]:\n",
    "                            idx = np.where(prev_mcmc_fits[:, -1] == fname)[0][0]\n",
    "                            prev_mcmc_datasets[j][fname] = prev_mcmc_fits[idx, :-1].astype(float)\n",
    "                    print(f\"Loaded {len(prev_mcmc_datasets[j])} previous fits for peak {j}.\")\n",
    "            \n",
    "            \n",
    "            # Begin writing the files\n",
    "            open_mcmc_fits = [open(paths_mcmc_fits[j], \"w\") for j in range(len(FIT_REGISTER))]\n",
    "            try:\n",
    "                # Write the headers and previous data\n",
    "                for j in range(len(FIT_REGISTER)):\n",
    "                    open_mcmc_fits[j].write(header_mcmc[j])\n",
    "                    for fname in prev_mcmc_datasets[j].keys():\n",
    "                        open_mcmc_fits[j].write(\"\\n\" + \"\\t\".join([str(val) for val in prev_mcmc_datasets[j][fname]] + [fname]))\n",
    "                        \n",
    "                # Loop over all files in the sample dataset\n",
    "                for j, fname in enumerate(datasets[i]):\n",
    "                    # Check if the file has already been processed\n",
    "                    #-----------------------------------------------\n",
    "                    if not OVERRIDE and all(\n",
    "                        [fname in prev_mcmc_dataset.keys()\n",
    "                        for prev_mcmc_dataset in prev_mcmc_datasets]\n",
    "                        ):\n",
    "                        print(f\"File data already exists for all peaks, skipping `{fname}`.\")\n",
    "                        # Reload the existing data and skip.\n",
    "                        for k in range(len(FIT_REGISTER)):\n",
    "                            dataset_MC_fits[i][k][j,:] = prev_mcmc_datasets[k][fname]\n",
    "                        \n",
    "                        pbar.total -= 1\n",
    "                        pbar2.total -= 1\n",
    "                        continue\n",
    "                    \n",
    "                    ### Otherwise, load the data and perform the fits\n",
    "                    # Collect the metadata\n",
    "                    en_idx = fname.find('eV_')\n",
    "                    en = float(fname[en_idx-7:en_idx])\n",
    "                    ai_idx = fname.find(\"_ai\")\n",
    "                    ai = float(fname[ai_idx+3:ai_idx+7])\n",
    "                    \n",
    "                    # Collect the line profiles from disk\n",
    "                    line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "                    path_OOP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_OOP.txt\")\n",
    "                    path_IP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_IP.txt\")\n",
    "                    path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{ai:0.3f}deg_{en:0.2f}eV_R.txt\")\n",
    "                    \n",
    "                    q0_OOP, I0_OOP, I0_OOP_err, _, _, _ = np.loadtxt(path_OOP, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                    q0_IP, I0_IP, I0_IP_err, _, _, _ = np.loadtxt(path_IP, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                    q0_R, I0_R, I0_R_err, _, _, _ = np.loadtxt(path_R, delimiter=\"\\t\", unpack=True, skiprows=2)\n",
    "                    \n",
    "                    # Perform a fit to the data for each listed peak.\n",
    "                    for k, (d, peak, fit_fn, guess, bounds, labels) in enumerate(FIT_REGISTER):\n",
    "                        # Setup the data\n",
    "                        if d == \"OOP\":\n",
    "                            q0, I0, I0_err = q0_OOP, I0_OOP, I0_OOP_err\n",
    "                        elif d == \"IP\":\n",
    "                            q0, I0, I0_err = q0_IP, I0_IP, I0_IP_err\n",
    "                        elif d == \"RAD\":\n",
    "                            q0, I0, I0_err = q0_R, I0_R, I0_R_err\n",
    "                        \n",
    "                        # Remove rows with NaN or zero values and collect the fit subset data\n",
    "                        idx = np.where((~np.isnan(I0)) & (I0 != 0) & (q0 > peak[0]) & (q0 < peak[1]))\n",
    "                        q, I, I_err = q0[idx], I0[idx], I0_err[idx]\n",
    "                            \n",
    "                        # Define the output paths\n",
    "                        mcmc_img_peak_dir = os.path.join(mcmc_img_dir, f\"{d}-{peak[0]}_to_{peak[1]}_per\")\n",
    "                        if not os.path.isdir(mcmc_img_peak_dir):\n",
    "                            os.mkdir(mcmc_img_peak_dir)\n",
    "                        path_mc_chain = os.path.join(mcmc_img_peak_dir, f\"mcmc_chain-{sample}-peak_{peak[0]}-{peak[1]}_per-{en:0.2f}eV-{ai:0.3f}deg-{d}.png\")\n",
    "                        path_mc_corner_plot = os.path.join(mcmc_img_peak_dir, f\"corner_plot-{sample}-peak_{peak[0]}-{peak[1]}_per-{en:0.2f}eV-{ai:0.3f}deg-{d}.png\")\n",
    "                        \n",
    "                        # Obtain the least squres fit\n",
    "                        popt = dataset_fits[i][k][j, 1:1+len(guess)]\n",
    "\n",
    "                        # # Plot the fit\n",
    "                        # fig, ax = plt.subplots(1, 1, figsize=(4, 2.5))\n",
    "                        # ax.plot(q, I, label=\"Data\")\n",
    "                        # ax.fill_between(q, I - I_err, I + I_err, alpha=0.2)\n",
    "                        # ax.plot(q, fit_fn(q, *popt), label=\"Least Squares Fit\")\n",
    "                        # # ax.fill_between(q, fit_fn(q, *popt - np.sqrt(np.diag(pcov))), fit_fn(q, *popt + np.sqrt(np.diag(pcov))), alpha=0.1)\n",
    "                        # ax.set_ylim(0, 1.1 * np.max(I))\n",
    "                        # ax.set_title(f\"{sample}\\n{en} eV - {ai} deg - Peak {peak[0]}-{peak[1]}\")\n",
    "                        # fig.savefig(path_fit_img)\n",
    "                        # plt.close() # Save memory\n",
    "                        \n",
    "                        # Create the MCMC fitter\n",
    "                        init_position = (popt * np.ones((N, len(popt))) # Add some small random magnitude variance on each parameter.\n",
    "                                        * (1 + (1.0e-2 * np.random.randn(N, len(popt)) - 5.0e-3))) \n",
    "                        for b in range(len(bounds[0])):\n",
    "                            lb, ub = bounds[0][b], bounds[1][b]\n",
    "                            if lb is not None:\n",
    "                                init_position[:,b] = np.clip(init_position[:,b], lb, np.inf)\n",
    "                            if ub is not None:\n",
    "                                init_position[:,b] = np.clip(init_position[:,b], -np.inf, ub)\n",
    "                                \n",
    "                        nwalkers, ndim = init_position.shape\n",
    "                        sampler = emcee.EnsembleSampler(\n",
    "                            nwalkers, ndim, LOG_PROBABILITY_FNS[k], args=(q, I, I_err, LOG_PRIOR_FNS[k], LOG_LIKELIHOOD_FNS[k])\n",
    "                        )\n",
    "                        # Run for M steps\n",
    "                        sampler.run_mcmc(init_position, M, progress=True, progress_kwargs={\"leave\": False, \"position\":0})\n",
    "                        # Check the autocorrelation time, and run for more steps if necessary.\n",
    "                        max_tau = None\n",
    "                        sample_attempts = 1\n",
    "                        while max_tau is None:\n",
    "                            try:\n",
    "                                tau = sampler.get_autocorr_time()\n",
    "                                max_tau = np.nanmax(tau)\n",
    "                            except emcee.autocorr.AutocorrError as e:\n",
    "                                if np.isnan(e.tau).all():\n",
    "                                    print(\"Nan tau, running for 1000 more steps.\")\n",
    "                                    sampler.run_mcmc(None, 1000, progress=False, progress_kwargs={\"leave\": False, \"position\":0})\n",
    "                                else:\n",
    "                                    tau_estimate = int(np.nanmax(e.tau))\n",
    "                                    if tau_estimate * 50 < 150000:\n",
    "                                        print(f\"Estimated max tau: {tau_estimate}, running for {50*tau_estimate} more steps.\")\n",
    "                                        sampler.run_mcmc(None, tau_estimate * 50, progress=False, progress_kwargs={\"leave\": False, \"position\":0})\n",
    "                                    elif len(sampler.get_chain()) > 2 * tau_estimate:\n",
    "                                        max_tau = tau_estimate\n",
    "                                    else:\n",
    "                                        print(\"Tau estimate unreasonably large.\")\n",
    "                                        sample_attempts = 11\n",
    "                            sample_attempts += 1\n",
    "                            if sample_attempts > 10: # Break if the sample attempts exceed 10\n",
    "                                break\n",
    "                            \n",
    "                        # Skip the MCMC fit if the sample attempts exceed 10.\n",
    "                        if sample_attempts > 10:\n",
    "                            print(\"Skipping MCMC fit, using nans.\")\n",
    "                            dataset_MC_fits[i][k][j,:] = np.full((1 + len(labels)*3, ), np.nan)\n",
    "                            pbar.update(1)\n",
    "                            pbar2.update(1)\n",
    "                            continue\n",
    "                        \n",
    "                        # Create a default max_tau value if it is not found.\n",
    "                        if np.isnan(max_tau):\n",
    "                            max_tau = 0\n",
    "                            \n",
    "                        # Get the samples and discard some for the burn-in period\n",
    "                        MC_samples = sampler.get_chain()\n",
    "                        flat_MC_samples = sampler.get_chain(discard=int(max_tau * 3), thin=5, flat=True)\n",
    "\n",
    "                        # Save the MCMC fit results\n",
    "                        percentiles = [np.percentile(MC_samples[:, :, z], [16, 50, 84]) for z in range(MC_samples.shape[2])] #Sampling at 1 sigma percentiles\n",
    "                        values = np.array([p[1] for p in percentiles])\n",
    "                        gauss_error = np.array([[p[0] - p[1], p[2] - p[1]] for p in percentiles])\n",
    "                        \n",
    "                        mc_data_line = np.r_[[ai], values, gauss_error[:,0], gauss_error[:,1]]\n",
    "                        # Save the MC data to the open file\n",
    "                        open_mcmc_fits[k].write(\"\\n\" + \"\\t\".join([str(val) for val in mc_data_line] + [fname]))\n",
    "                        dataset_MC_fits[i][k][j, :] = mc_data_line\n",
    "                        \n",
    "                        # Plot the chain output\n",
    "                        fig, axes = plt.subplots(len(labels), figsize=(10, 2*len(labels)), sharex=True)\n",
    "                        for z in range(ndim):\n",
    "                            ax = axes[z]\n",
    "                            ax.plot(MC_samples[:, :, z], \"k\", alpha=0.3)\n",
    "                            ax.set_xlim(0, len(MC_samples))\n",
    "                            ax.set_ylabel(labels[z])\n",
    "                            ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "                        axes[-1].set_xlabel(\"step number\")\n",
    "                        fig.suptitle(f\"Chain Output - Corr time = {max_tau:0.2f} steps\")\n",
    "                        fig.savefig(path_mc_chain)\n",
    "                        plt.close()\n",
    "                    \n",
    "                        # Create a corner plot        \n",
    "                        fig = corner.corner(\n",
    "                            flat_MC_samples, labels=labels, truths=popt\n",
    "                        )\n",
    "                        fig.savefig(path_mc_corner_plot)\n",
    "                        plt.close()\n",
    "                    plt.close(\"all\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                    pbar2.update(1)\n",
    "                    \n",
    "                # Close the files\n",
    "                for f1, f2 in zip(open_ls_fits, open_mcmc_fits):\n",
    "                    f1.close()\n",
    "                    f2.close()\n",
    "                open_beamstop_area.close()\n",
    "                \n",
    "                # Generate final graphics for the angle of incidence fits to each peak.\n",
    "                #-----------------------------------------------------------------------\n",
    "                for k, (d, peak, fit_fn, guess, bounds, labels) in enumerate(FIT_REGISTER):\n",
    "                    path_ls_summary = os.path.join(sample_dir, f\"{sample}-peak_{peak[0]}-{peak[1]}_per-ls_fits_summary.png\")\n",
    "                    path_mcmc_summary = os.path.join(sample_dir, f\"{sample}-peak_{peak[0]}-{peak[1]}_per-mcmc_fits_summary.png\")\n",
    "                    path_overlap_summary = os.path.join(sample_dir, f\"{sample}-peak_{peak[0]}-{peak[1]}_per-overlap_fits_summary.png\")\n",
    "                \n",
    "                    # Load the data\n",
    "                    ls_fits = dataset_fits[i][k]\n",
    "                    MC_fits = dataset_MC_fits[i][k]\n",
    "                    # Re-index the data\n",
    "                    angles = ls_fits[:, 0]\n",
    "                    ls_popt = ls_fits[:, 1:1+len(labels)]\n",
    "                    ls_errors = ls_fits[:, 1+len(labels):]\n",
    "                    MC_popt = MC_fits[:, 1:1+len(labels)]\n",
    "                    MC_lb_errors = MC_fits[:, 1+len(labels):1+2*len(labels)]\n",
    "                    MC_ub_errors = MC_fits[:, 1+2*len(labels):]\n",
    "                    \n",
    "                    # Plot the MCMC data\n",
    "                    fig, ax = plt.subplots(len(labels), 1, figsize=(8, 2*len(labels)))\n",
    "                    for z in range(len(labels)):\n",
    "                        ax[z].plot(angles, MC_popt[:, z], label=labels[z])\n",
    "                        ax[z].fill_between(angles, MC_popt[:, z] - MC_lb_errors[:, z], MC_popt[:, z] + MC_ub_errors[:, z], alpha=0.1)\n",
    "                        ax[z].set_ylabel(labels[z])\n",
    "                    fig.suptitle(f\"{sample}\\n{d} Peak {peak[0]:0.2f} to {peak[1]:0.2f} - MCMC Fit Summary\")\n",
    "                    fig.savefig(path_mcmc_summary)\n",
    "                    plt.close()\n",
    "                    \n",
    "                    # Plot the overlap data\n",
    "                    fig, ax = plt.subplots(len(labels), 1, figsize=(8, 2*len(labels)))\n",
    "                    for z in range(len(labels)):\n",
    "                        ax[z].plot(angles, ls_popt[:, z], label=\"Least Squares Fit\")\n",
    "                        ax[z].fill_between(angles, ls_popt[:, z] - ls_errors[:, z], ls_popt[:, z] + ls_errors[:, z], alpha=0.1)\n",
    "                        ax[z].plot(angles, MC_popt[:, z], label=\"MCMC Fit\")\n",
    "                        ax[z].fill_between(angles, MC_popt[:, z] - MC_lb_errors[:, z], MC_popt[:, z] + MC_ub_errors[:, z], alpha=0.1)\n",
    "                        ax[z].set_ylabel(labels[z])\n",
    "                        ax[z].legend()\n",
    "                    fig.suptitle(f\"{sample}\\n{d} Peak {peak[0]:0.2f} to {peak[1]:0.2f} - MCMC/LS Fit Overlap Summary\")\n",
    "                    fig.savefig(path_overlap_summary)\n",
    "                    plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample}: {e}\")\n",
    "                # Close the files\n",
    "                for f2 in open_mcmc_fits:\n",
    "                    f2.close()\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f2 in open_mcmc_fits:\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\"Stop Here.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
