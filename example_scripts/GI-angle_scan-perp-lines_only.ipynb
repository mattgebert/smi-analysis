{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMI Data Reduction for Grazing-Incidence Energy Scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "##### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Prevent numpy from using multiple threads when using emcee multiprocessing.\n",
    "# These packages should all be installed if the procedure was followed\n",
    "%matplotlib inline\n",
    "import matplotlib, matplotlib.pyplot as plt, matplotlib.colors as mplc\n",
    "matplotlib.interactive(True)\n",
    "plt.ion()\n",
    "from smi_analysis import SMI_beamline\n",
    "import numpy as np, numpy.typing as npt\n",
    "import pandas as pd\n",
    "import fabio\n",
    "import logging\n",
    "import scipy.constants as const\n",
    "import time\n",
    "import corner\n",
    "from typing import Literal, Callable\n",
    "import emcee\n",
    "import tqdm, tqdm.notebook # For progress bars\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "import datetime\n",
    "import subprocess\n",
    "\n",
    "# Setup options\n",
    "fabio.TiffIO.logger.setLevel(logging.ERROR)\n",
    "pd.set_option(\"display.width\", 1000) #display large filenames\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "en2wav = lambda en: const.h * const.c / (const.e * en)\n",
    "\"\"\"Function to convert energy (eV) to wavelength\"\"\"\n",
    "en2wav(2.45e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimental configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry: Literal['Transmission'] | Literal['Reflection'] = 'Reflection'\n",
    "\"\"\"The measurement geometry\"\"\"\n",
    "energy: float = 2.45e3\n",
    "\"\"\"The energy (keV) at which the scan is performed\"\"\"\n",
    "wavelength: float = en2wav(energy)\n",
    "\"\"\"The wavelength corresponding to `energy`.\"\"\"\n",
    "beamstop_type: Literal[\"pindiode\"] | Literal['rod'] = 'pindiode'\n",
    "\"\"\"The beamstop type\"\"\"\n",
    "incident_angle = 0\n",
    "\"\"\"The default incident angle (varies) in degrees\"\"\"\n",
    "\n",
    "#WAXS\n",
    "detector_waxs: Literal['Pilatus900kw'] | Literal['Pilatus1m'] = 'Pilatus900kw'\n",
    "\"\"\"Type of WAXS/SAXS detector\"\"\"\n",
    "sdd_waxs: float | int = 280 # In mm\n",
    "\"\"\"Sample to detector distance in millimeters\"\"\"\n",
    "center_waxs: tuple[int|float, int|float] = (#97, 1255.9]\n",
    "                                            213, 97)\n",
    "\"\"\"Coordinates of the beam centre at 0 degrees, for the middle detector strip\"\"\"\n",
    "bs_pos_waxs: list[tuple[int, int]] = [(0, 0),\n",
    "                                      (0, 0), \n",
    "                                      (0, 0)\n",
    "                                     ]\n",
    "\"\"\"The position of the center of the beam stop for each detector angle; [0,0] implies not measured. \n",
    "This coordinate is relative to the stitch of the 3 detector strips.\"\"\"\n",
    "detector_angles: list[int | float] | npt.NDArray[np.float64 | np.int_] = np.deg2rad(np.array([17.7]) - 0.06) #0.06 is the correction for the WAXS 0 deg detector position\n",
    "\"\"\"The angles of the detector in radians. \n",
    "May need to include corrections (-0.06 degs at 0, -0.36 at 20 deg) for position offsets.\"\"\"\n",
    "\n",
    "\n",
    "display(pd.DataFrame([\n",
    "    (\"Geometry\", geometry),\n",
    "    (\"Energy (keV)\", energy),\n",
    "    (\"Wavelength (nm)\", wavelength * 1e9),\n",
    "    (\"Sample to Detector Distance (mm)\", sdd_waxs),\n",
    "    (\"Beamstop Type\", beamstop_type),\n",
    "    (\"Incident Angle (deg)\", incident_angle),\n",
    "    (\"Detector Type\", detector_waxs),\n",
    "    (\"Center Coords\", center_waxs),\n",
    "    (\"Beamstop Coords\", bs_pos_waxs),\n",
    "    (\"Detector Angles\", detector_angles)\n",
    "], columns=[\"Parameter\", \"Value\"]))\n",
    "\n",
    "#Test the configuration can be loaded!\n",
    "SMI_waxs = SMI_beamline.SMI_geometry(geometry = geometry,\n",
    "                                     wav = wavelength,\n",
    "                                     sdd = sdd_waxs,\n",
    "                                     alphai = incident_angle,\n",
    "                                     detector = detector_waxs,\n",
    "                                     center = center_waxs,\n",
    "                                     bs_pos = bs_pos_waxs,\n",
    "                                     bs_kind = beamstop_type,\n",
    "                                     det_angles=detector_angles)\n",
    "\n",
    "angles = [detector_angles[0] - np.deg2rad(7.47), detector_angles[0], detector_angles[0] + np.deg2rad(7.47)]\n",
    "SMI_waxs.calculate_integrator_gi(angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatfield Data\n",
    "Data to normalise the detector pixels and remove background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Past beamline data for flat fielding (normalizing default pixel intensities)\n",
    "# Note this is done at 2478eV, not all energies.\n",
    "\n",
    "DRIVE = \"D:/\"\n",
    "\n",
    "# # 2024 Cycle 2 Flatfielding\n",
    "# CYCLE_FLAT = '2024_3'\n",
    "# PROPOSAL_FLAT= '314483-Freychet-Flatfielding'\n",
    "# FLAT_FILE = 'GF_flatfield_Sedge_2450uhighg1600_WZY11_wa30deg_2478eV_20s_id701601_000000_WAXS.tif'\n",
    "\n",
    "# 2024 Cycle 3 Flatfielding\n",
    "PROPOSAL_FLAT= '314483_Freychet_08'\n",
    "FLAT_FILE = 'GF_GF_flatfield_Sedge_2450uhighg1600_Y2_06_2477.00eV_wa20deg_id807229_000000_WAXS.tif'\n",
    "\n",
    "# Compile and load the flatfield path\n",
    "FLAT_DIR_PATH = f'{DRIVE}Datasets/2024-09 SMI/{PROPOSAL_FLAT}/900KW/'\n",
    "flatfield: npt.NDArray = np.rot90(fabio.open(os.path.join(FLAT_DIR_PATH, FLAT_FILE)).data, 1)\n",
    "\n",
    "fig,ax = plt.subplots(2,1, sharex=True, sharey=True, figsize=(8,5))\n",
    "p = 99.9\n",
    "percentile = np.percentile(flatfield, p) #99.9th percentile\n",
    "ax[0].imshow(np.rot90(flatfield, 3), vmin=0, vmax=percentile, interpolation=None)\n",
    "ax[0].set_title(\"Flatfielding Data\")\n",
    "\n",
    "erronous =  (flatfield > percentile) * 1.0\n",
    "ax[1].imshow(np.rot90(erronous,3), vmin=0, vmax=np.max(erronous)/5, interpolation=None)\n",
    "ax[1].set_title(f\"{p}th percentile pixels\")\n",
    "fig.tight_layout()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra functions for SMI Beamline Masking\n",
    "\n",
    "##### Detector Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_detector_mask_to_array(\n",
    "    mask: npt.NDArray = np.zeros((1475, 195), dtype=bool)\n",
    ") -> npt.NDArray[np.bool]:\n",
    "    \"\"\"Sets an array mask for bad pixels; should only be applied to the middle column array\"\"\"\n",
    "    mask[1254:1256, 47] = True\n",
    "    mask[979:1050, 0:100] = True\n",
    "    mask[967, 67] = True\n",
    "    mask[490:555, 100:] = True\n",
    "    mask[1231:1233, 174] = True\n",
    "    mask[1414: 1416, 179] = True\n",
    "    mask[858:860, 5] = True\n",
    "    mask[414, 6] = True\n",
    "    mask[394, 138] = True\n",
    "    mask[364:366, 41] = True\n",
    "    mask[364:366, 96] = True\n",
    "    mask[304:306, 96:98] =  True\n",
    "    mask[988, 188:194] = True\n",
    "    mask[:, 1] = True\n",
    "    mask[473, 20] = True\n",
    "    mask[98, 5] = True\n",
    "    mask[141, 111] = True\n",
    "    mask[240:300, 0:50] = True\n",
    "    mask[300:425, 125:] = True\n",
    "    mask[181:183, 97:99] = True\n",
    "    mask[553:555, 99:100] = True\n",
    "    return mask\n",
    "\n",
    "def apply_boundary_mask_to_array(\n",
    "    mask: npt.NDArray = np.zeros((1475, 195), dtype=bool)\n",
    ") -> npt.NDArray[np.bool]:\n",
    "    \"\"\"Sets an array mask for the boundary pixels; should only be applied to the middle column array\"\"\"\n",
    "    mask[0, :] = True\n",
    "    mask[-1, :] = True\n",
    "    mask[:, 0] = True\n",
    "    mask[:, -1] = True\n",
    "    return mask\n",
    "\n",
    "def apply_detector_mask(geom: SMI_beamline.SMI_geometry) -> None:\n",
    "    \"\"\"Applies a pre-defined mask for the bad pixels in the SMI beamline\"\"\"\n",
    "    for i, mask in enumerate(geom.masks):\n",
    "        # Dead pixels in the 2nd detector strip.\n",
    "        if i%3 == 1: # For multiple WAXS images, always masks the 2nd strip.\n",
    "            apply_detector_mask_to_array(mask)\n",
    "        # Add a mask on the boundary\n",
    "        apply_boundary_mask_to_array(mask)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flatield Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLATFIELD_PERCENTILE = 99.5\n",
    "\"\"\"The percentile of the flatfield data to mask.\"\"\"\n",
    "# SAMPLE_PERCENTILE = 100.00\n",
    "SAMPLE_PERCENTILE = 100.00\n",
    "\"\"\"The percentile of the real data to mask.\"\"\"\n",
    "\n",
    "# For flatfielding, ignore/mask reigons between detector pixels.\n",
    "FLATFIELD_SLICES = [slice(0, 195),  # Ign. flatfield above first frame\n",
    "                    slice(211, 406),# Ign. flatfield outside middle  \n",
    "                    slice(-195, None)] # Ign. flatfield below first frame\n",
    "            \n",
    "def flatfield_mask(flatfield: npt.NDArray = flatfield, \n",
    "                   percentile: float = FLATFIELD_PERCENTILE, \n",
    "                   min: float | int = 0) -> npt.NDArray[np.bool]:\n",
    "        \"\"\"\n",
    "        Returns a mask of flatfield data as a boolean numpy array.\n",
    "        \n",
    "        Masks pixels above the `percentile` (by default 99.9)\n",
    "        and values less than `min` (by default 1).\n",
    "        \"\"\"\n",
    "        # Calculate the 99.9th percentile of the total flatfield data\n",
    "        p = np.percentile(flatfield, percentile) #99.9th percentile\n",
    "        erronous =  flatfield > p\n",
    "        # Also mask pixels well away from the standard deviation\n",
    "        mean = np.mean(flatfield)\n",
    "        std = np.std(flatfield)\n",
    "        erronous |= flatfield > mean + 5*std\n",
    "        \n",
    "        # Also mask negative and zero pixels\n",
    "        negative = flatfield < min\n",
    "        # Return the overlap of the erronous and negative masks.\n",
    "        return erronous | negative\n",
    "\n",
    "def apply_flatfield(geom: SMI_beamline.SMI_geometry, \n",
    "                    flatfield: npt.NDArray, \n",
    "                    flat_percentile: float | int = FLATFIELD_PERCENTILE,\n",
    "                    img_percentile: float | int = SAMPLE_PERCENTILE,\n",
    "                    outliers: bool = False,\n",
    "                    min : float | int = 1) -> None:\n",
    "    \"\"\"Applies a pre-defined flatfield mask and normalisation for the SMI beamline object\"\"\"\n",
    "    flatmask = flatfield_mask(flatfield=flatfield, percentile=flat_percentile, min = min)\n",
    "    for i, (mask, img) in enumerate(zip(geom.masks, geom.imgs)):\n",
    "        fmask_i = flatmask[:, FLATFIELD_SLICES[i % 3]]\n",
    "        # Apply the masking values\n",
    "        masking_values = np.where(fmask_i == True)\n",
    "        mask[masking_values] = True\n",
    "        # Apply the normalisation \n",
    "        flat = flatfield[:, FLATFIELD_SLICES[i%3]] # Get flatfield panel\n",
    "        flat[flat < 0] = 10000 # avoid negative values - note these are already masked\n",
    "        # Multiply img by max of flatfield, then divide by flatfield to keep integer precision of img.\n",
    "        temp = ((img * np.max(flatfield[:, FLATFIELD_SLICES[i%3]] * ~mask))\n",
    "                / (flat * ~mask +1)) * ~mask # Avoid divide by zero, and zero mask values.\n",
    "        # This line creates a runtime error due to casting... img is int32, flat is float64\n",
    "        img[:] = temp.astype(np.int32)\n",
    "    \n",
    "        # Repeat mask for very large values erronously normalised\n",
    "        if not img_percentile is None and img_percentile < 100:\n",
    "            mask2 = flatfield_mask(flatfield=img, percentile=img_percentile, min = 0) # only consider positive values.\n",
    "            masking_values2 = np.where(mask2 == True)\n",
    "            mask[masking_values2] = True\n",
    "        \n",
    "        # Mask outliers in the data\n",
    "        if outliers:\n",
    "            img_mean = np.mean(img[~mask])\n",
    "            img_std = np.std(img[~mask])\n",
    "            mask[img > img_mean + 10*img_std] = True\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the stages of Flatfield masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, figsize=(16,6), dpi=50, sharex=True, sharey=True)\n",
    "\n",
    "# Detector image is (619, 1475) #619 is 195 * 3 + 2 * 17\n",
    "# Show the detector masking\n",
    "grid = np.zeros((195, 1475), dtype=bool)\n",
    "\n",
    "\n",
    "det_mask = np.c_[apply_boundary_mask_to_array(grid.copy().T),\n",
    "                 np.zeros((17, 1475), dtype=bool).T,\n",
    "                 apply_detector_mask_to_array(apply_boundary_mask_to_array(grid.copy().T)), # 195 pixels\n",
    "                 np.zeros((17, 1475), dtype=bool).T,\n",
    "                apply_boundary_mask_to_array(grid.copy().T)]\n",
    "    \n",
    "\n",
    "det_im = ax[0][0].imshow(np.rot90(det_mask,3), interpolation='nearest') # required to prevent interpolation\n",
    "ax[0][0].set_title(\"Pre-defined Detector Mask (boolean)\")\n",
    "\n",
    "# Show the flatfield masking\n",
    "ff_masked = flatfield_mask(flatfield)\n",
    "ff_masked_im = ax[0][1].imshow(np.rot90(ff_masked,3), interpolation='nearest') # required to prevent interpolation\n",
    "ax[0][1].set_title(\"Calculated Flatfield Mask (boolean)\")\n",
    "\n",
    "# Show the flatfield image\n",
    "ff_im = ax[1][0].imshow(np.rot90(flatfield,3), vmin = 0, vmax = np.mean(flatfield[~ff_masked]) + 5*np.std(flatfield[~ff_masked]), interpolation='nearest')\n",
    "plt.colorbar(ff_im)\n",
    "ax[1][0].set_title(\"Flatfield Raw Data\")\n",
    "\n",
    "# Show the masks on the flatfield image\n",
    "joined_mask = ff_masked | det_mask\n",
    "cmap = mplc.LinearSegmentedColormap.from_list(\"Mask\", [(256,0,0,0),(256,0,0,256)], N=2)\n",
    "ff_im_masked = ax[1][1].imshow(np.rot90(flatfield,3), interpolation='nearest',\n",
    "                               vmin = 0, vmax = np.mean(flatfield[~ff_masked]) + 5*np.std(flatfield[~ff_masked]))\n",
    "ax[1][1].imshow(np.rot90(joined_mask,3), cmap=cmap, interpolation='nearest')\n",
    "ax[1][1].set_title(\"Flatfield Data Masked\")\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Dependence Flux Calibration for MEX2 Beamline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flux data from recording photodiode and BPMs\n",
    "# 2024 Cycle 3 Flux\n",
    "PROPOSAL_FLUX = '314483_Freychet_09'\n",
    "# Compile and load the flux paths\n",
    "FLUX_DIR_PATH = f'{DRIVE}Datasets/2024-09 SMI/{PROPOSAL_FLUX}/1M/'\n",
    "FLUX_FILES = [file for file in os.listdir(FLUX_DIR_PATH) if file.endswith('.tif') and \"Sedge\" in file and \"eV\" in file]\n",
    "\n",
    "# Parse the flux data from the filenames\n",
    "flux_energies = []\n",
    "flux_norm_bpm2 = []\n",
    "\"\"\"A collection of flux normalisation values for each flux file, also known as i0.\"\"\"\n",
    "flux_norm_bpm3 = []\n",
    "\"\"\"A collection of flux normalisation values for each flux file, also known as i0.\"\"\"\n",
    "flux_norm_phd = []\n",
    "\"\"\"A collection of flux normalisation values for each flux file, also known as i0.\"\"\"\n",
    "for file in FLUX_FILES:\n",
    "    # Example Format:\n",
    "    # CM_direct_beam_Sedge_2474.75eV_bpm2_124.304_bpm3_4.511_pd_20611.340_id806560_000000_SAXS.tif'\n",
    "    en_idx = file.find('eV')\n",
    "    en = float(file[en_idx-7:en_idx])\n",
    "    bpm2_idx = file.find('bpm2_')\n",
    "    bpm3_idx = file.find('bpm3_')\n",
    "    phd_idx = file.find('pd_')\n",
    "    bpm2 = float(file[bpm2_idx+5:bpm3_idx-1])\n",
    "    bpm3 = float(file[bpm3_idx+5:phd_idx-1])\n",
    "    phd = float(file[phd_idx+3:file.find('_id')])\n",
    "    flux_energies.append(en)\n",
    "    flux_norm_bpm2.append(bpm2)\n",
    "    flux_norm_bpm3.append(bpm3)\n",
    "    flux_norm_phd.append(phd)\n",
    "    \n",
    "# Sort the flux data by energy\n",
    "idx = np.argsort(flux_energies)\n",
    "flux_energies = np.array(flux_energies)[idx].tolist()\n",
    "flux_norm_bpm2 = np.array(flux_norm_bpm2)[idx]\n",
    "flux_norm_bpm3 = np.array(flux_norm_bpm3)[idx]\n",
    "flux_norm_phd = np.array(flux_norm_phd)[idx]\n",
    "    \n",
    "## Additional flux data from silicon reference samples\n",
    "# Define the silicon reference data\n",
    "WAXS_ANGLE_STRINGS = ['wa0', 'wa20']\n",
    "FLUX_SI_CYCLE: str = '2024_3' #YYYY_[1-3]\n",
    "FLUX_SI_PROPOSAL_ID = '316022_McNeill_15' #PPPPPP_[Name]_[#]\n",
    "FLUX_SI_DIR = f'{DRIVE}Datasets/2024-09 SMI/{FLUX_SI_PROPOSAL_ID}/900KW/'\n",
    "\n",
    "# Load the flux filenames\n",
    "FLUX_SI_FILES = [file for file in os.listdir(FLUX_SI_DIR) if file.endswith('.tif') and \"_Si_\" in file and \"wide\" not in file]\n",
    "FLUX_SI_DATASETS: list[list[str]] = [[\"\"] * len(WAXS_ANGLE_STRINGS) for _ in flux_energies]\n",
    "\"\"\"For each energy, a list of raw data filenames for the silicon reference.\"\"\"\n",
    "for j, file in enumerate(FLUX_SI_FILES):\n",
    "    en_idx = file.find('eV')\n",
    "    en = file[en_idx-7:en_idx]\n",
    "    energies_idx = flux_energies.index(float(en))\n",
    "    waxs_angle_idx = [k for k, angle in enumerate(WAXS_ANGLE_STRINGS) if angle in file][0]\n",
    "    FLUX_SI_DATASETS[energies_idx][waxs_angle_idx] = file\n",
    "\n",
    "pixel_summations: npt.NDArray[float] = np.zeros((len(flux_energies), len(WAXS_ANGLE_STRINGS)))\n",
    "\"\"\"The pixel summations for each sample, energy, and WAXS angle.\"\"\"\n",
    "for j, en in enumerate(flux_energies):\n",
    "    for k, angle in enumerate(WAXS_ANGLE_STRINGS):\n",
    "        if FLUX_SI_DATASETS[j][k] is not None:\n",
    "            # Load the data\n",
    "            img = fabio.open(os.path.join(FLUX_SI_DIR, FLUX_SI_DATASETS[j][k])).data\n",
    "            # Mask the data\n",
    "            new_mask = np.zeros_like(img, dtype=bool).T\n",
    "            apply_boundary_mask_to_array(new_mask)\n",
    "            apply_detector_mask_to_array(new_mask)\n",
    "            new_mask = new_mask.T\n",
    "            img_mask = flatfield_mask(img, SAMPLE_PERCENTILE, 0)\n",
    "            \n",
    "            # Perform a summation excluding masked pixels:\n",
    "            img[img_mask | new_mask] = 0\n",
    "            pixel_summations[j][k] = np.sum(img)\n",
    "            \n",
    "# Normalize by the first value\n",
    "flux_norm_bpm2 /= flux_norm_bpm2[-1]\n",
    "flux_norm_bpm3 /= flux_norm_bpm3[-1]\n",
    "flux_norm_phd /= flux_norm_phd[-1]\n",
    "# Normalize the pixel summations\n",
    "pixel_summations /= pixel_summations[-1]\n",
    "si_ave = np.mean(pixel_summations, axis=1)\n",
    "\n",
    "# Plot results\n",
    "fig,ax = plt.subplots(1,1, figsize=(8, 3), sharex=True)\n",
    "ax.plot(flux_energies, flux_norm_bpm2, label='BPM2')\n",
    "ax.plot(flux_energies, flux_norm_bpm3, label='BPM3')\n",
    "ax.plot(flux_energies, flux_norm_phd, label='PHD')\n",
    "for k, angle in enumerate(WAXS_ANGLE_STRINGS):\n",
    "    ax.plot(flux_energies, pixel_summations[:, k], label=f'Si {angle}')\n",
    "ax.set_xlabel(\"Energy (keV)\")\n",
    "ax.set_ylabel(\"Flux (Arbitrary Units)\")\n",
    "ax.set_title(\"Flux Normalisation\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "# Select a flux channel for the analysis\n",
    "FLUX_CHANNEL = 'si_ave'\n",
    "match FLUX_CHANNEL:\n",
    "    case 'bpm2':\n",
    "        FLUX_NORM = flux_norm_bpm2\n",
    "    case 'bpm3':\n",
    "        FLUX_NORM = flux_norm_bpm3\n",
    "    case 'phd':\n",
    "        FLUX_NORM = flux_norm_phd\n",
    "    case 'si0':\n",
    "        FLUX_NORM = pixel_summations[:, 0]\n",
    "    case 'si20':\n",
    "        FLUX_NORM = pixel_summations[:, 1]\n",
    "    case 'si_ave':\n",
    "        FLUX_NORM = si_ave\n",
    "    case _:\n",
    "        raise ValueError(\"Invalid flux channel selected.\")\n",
    "    \n",
    "FLUX_OPTIONS = {\n",
    "    'bpm2': flux_norm_bpm2,\n",
    "    'bpm3': flux_norm_bpm3,\n",
    "    'phd': flux_norm_phd,\n",
    "    'si0': pixel_summations[:, 0],\n",
    "    'si20': pixel_summations[:, 1],\n",
    "    'si_ave': si_ave\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "\n",
    "##### Locate the files on your computer and define the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CYCLE: str = '2024_3' #YYYY_[1-3]\n",
    "PROPOSAL_ID = '316022_McNeill_12' #PPPPPP_[Name]_[#]\n",
    "## ----------- Path to the raw data -----------\n",
    "\n",
    "# RAW_PATH = f'D:/Datasets/2024-09 SMI/{CYCLE}/{PROPOSAL_ID}/900KW/'\n",
    "RAW_DIR = f'{DRIVE}Datasets/2024-09 SMI/{PROPOSAL_ID}/900KW/'\n",
    "display(pd.DataFrame(os.listdir(RAW_DIR), columns=[\"Filename\"])) #use tail or head to display a subset\n",
    "\n",
    "## ----------- Create/select the results directory -----------\n",
    "RESULT_DIR = f'{DRIVE}Datasets/2024-09 SMI/{PROPOSAL_ID}/angle_scan_results/'\n",
    "created = False\n",
    "for i in range(len(RESULT_DIR.split(\"/\"))):\n",
    "    if not os.path.isdir(\"/\".join(RESULT_DIR.split(\"/\")[:i+1])) and not (i==0 and RESULT_DIR[0]==\"/\"):\n",
    "        os.mkdir(\"/\".join(RESULT_DIR.split(\"/\")[:i+1]))\n",
    "        created = True\n",
    "if not created:\n",
    "    print(\"Results path exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Organise files into sample names and data for each detector angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_flags: list[str] = ['wide']\n",
    "\"\"\"The strings you want included in the files processed.\"\"\"\n",
    "exclude_flags: list[str] = []\n",
    "\"\"\"The strings you want excluded in the files processed.\"\"\"\n",
    "\n",
    "# Find all samples and energies\n",
    "samples: list[str] = []\n",
    "\"\"\"String names of the unique samples matching patterns in `filename_flags`\"\"\"\n",
    "sample_energies: list[list[float]] = []\n",
    "\"\"\"Unique energies found in the filenames for each sample\"\"\"\n",
    "sample_energy_angles: list[list[list[float]]] = []\n",
    "\"\"\"Unique angles found in the filenames for each sample and energy\"\"\"\n",
    "\n",
    "for file in sorted(os.listdir(RAW_DIR)):\n",
    "     # Define the flags for the files you want to process, by filtering the filename.\n",
    "     if all([flag in file for flag in filename_flags] \n",
    "            + [flag not in file for flag in exclude_flags]):\n",
    "        # Find the sample name:\n",
    "        idx1 = file.find(\"scan_\")\n",
    "        idx2 = file.find(\"eV_\")\n",
    "        sample_substring = file[idx1+5:idx2-8]\n",
    "        \n",
    "        # If sample substring not in list of samples, add it!\n",
    "        if sample_substring not in samples:\n",
    "            sample_idx = len(samples)\n",
    "            samples.append(sample_substring)\n",
    "            # Add new lists for the angles and energies\n",
    "            sample_energies.append([])\n",
    "            sample_energy_angles.append([])\n",
    "        else:\n",
    "            sample_idx = samples.index(sample_substring)\n",
    "        \n",
    "        # Find the beam energy:\n",
    "        en = float(file[idx2-7:idx2])\n",
    "        if en not in sample_energies[sample_idx]:\n",
    "            en_idx = len(sample_energies[sample_idx])\n",
    "            sample_energies[sample_idx].append(en)\n",
    "            # Add new list for the energies\n",
    "            sample_energy_angles[sample_idx].append([])\n",
    "        else:\n",
    "            en_idx = sample_energies[sample_idx].index(en)\n",
    "        \n",
    "        # Find the angle of incidence:\n",
    "        idx = file.find('_ai')\n",
    "        ai = float(file[idx+3:idx+7])\n",
    "        if ai not in sample_energy_angles[sample_idx][en_idx]:\n",
    "            sample_energy_angles[sample_idx][en_idx].append(ai)\n",
    "    \n",
    "# Sort the energies and angles\n",
    "for i in range(len(samples)): \n",
    "    for j in range(len(sample_energies[i])):\n",
    "        # Sort the energies\n",
    "        sample_energy_angles[i][j] = sorted(sample_energy_angles[i][j])\n",
    "    # Sort the angles\n",
    "    args = np.argsort(sample_energies[i])\n",
    "    sample_energies[i] = np.array(sample_energies[i])[args].tolist()\n",
    "    sample_energy_angles[i] = np.array(sample_energy_angles[i])[args].tolist()\n",
    "\n",
    "# Specify the WAXS angles strings to look for, in each sample\n",
    "WAXS_ANGLE_STRINGS : list[str] = [\"wa18\"]\n",
    "\n",
    "# Find all TIF image measurements\n",
    "datasets: list[list[list[list[str | None]]]] = [[[\n",
    "                                                [None] * len(WAXS_ANGLE_STRINGS)\n",
    "                                                for k, _ in enumerate(sample_energy_angles[i][j])]\n",
    "                                            for j, _ in enumerate(sample_energies[i])]\n",
    "                                        for i, _ in enumerate(samples)]\n",
    "\"\"\"For each sample, for each energy, for each angle, a list of raw data filenames.\"\"\"\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, file in enumerate(sorted(os.listdir(RAW_DIR))):\n",
    "        if all([flag in file for flag in filename_flags + [sample, '.tif']] \n",
    "                + [flag not in file for flag in exclude_flags]):\n",
    "            # Check which waxs angle the file is\n",
    "            en_idx = file.find('eV_')\n",
    "            en = file[en_idx-7:en_idx]\n",
    "            ai_idx = file.find('_ai')\n",
    "            ai = file[ai_idx+3:ai_idx+7]\n",
    "            sample_idx = file.find(\"scan_\")\n",
    "            # Double check the sample name matches the filename\n",
    "            sample_substring = file[sample_idx+5:en_idx-8]\n",
    "            if sample_substring != sample:\n",
    "                continue\n",
    "            # Find the index of the energy and angle\n",
    "            energies_idx = sample_energies[i].index(float(en))\n",
    "            angles_idx = sample_energy_angles[i][energies_idx].index(float(ai))\n",
    "            waxs_angle_idx = [k for k, angle in enumerate(WAXS_ANGLE_STRINGS) if angle in file][0]\n",
    "            datasets[i][energies_idx][angles_idx][waxs_angle_idx] = file\n",
    "\n",
    "# Display the number of files for each sample:\n",
    "display(\n",
    "      pd.DataFrame([\n",
    "            (sample, np.sum(\n",
    "                [len(sample_energy_angles[i][j]) * len(WAXS_ANGLE_STRINGS)\n",
    "                for j in range(len(sample_energies[i]))]\n",
    "            ))\n",
    "            for i, sample in enumerate(samples)\n",
    "      ], columns=[\"Sample Name\", \"Number of Files\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Organise the unique energies, and interpolate any missing values for normalisaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_energies = np.unique([en for en in sample_energies])\n",
    "if np.all(global_energies != flux_energies):\n",
    "    print(\"The flux energies do not match the global energies.\")\n",
    "    print(\"Generating interpolated values\")\n",
    "    for i, ai in enumerate(global_energies):\n",
    "        if ai not in flux_energies:\n",
    "            idx = np.searchsorted(flux_energies, en)\n",
    "            flux_energies.insert(idx, en)\n",
    "            flux_norm_bpm2 = np.r_[flux_norm_bpm2[:idx], np.interp(en, flux_energies, flux_norm_bpm2), flux_norm_bpm2[idx:]]\n",
    "            flux_norm_bpm3 = np.r_[flux_norm_bpm3[:idx], np.interp(en, flux_energies, flux_norm_bpm3), flux_norm_bpm3[idx:]]\n",
    "            flux_norm_phd = np.r_[flux_norm_phd[:idx], np.interp(en, flux_energies, flux_norm_phd), flux_norm_phd[idx:]]\n",
    "            si0 = np.r_[pixel_summations[:, 0][:idx], np.interp(en, flux_energies, pixel_summations[:, 0]), pixel_summations[:, 0][idx:]]\n",
    "            si20 = np.r_[pixel_summations[:, 1][:idx], np.interp(en, flux_energies, pixel_summations[:, 1]), pixel_summations[:, 1][idx:]]\n",
    "            si_ave = np.r_[si_ave[:idx], np.interp(en, flux_energies, si_ave), si_ave[idx:]]\n",
    "            \n",
    "            # Re-assign new objects\n",
    "            FLUX_OPTIONS = {\n",
    "                'bpm2': flux_norm_bpm2,\n",
    "                'bpm3': flux_norm_bpm3,\n",
    "                'phd': flux_norm_phd,\n",
    "                'si0': si0,\n",
    "                'si20': si20,\n",
    "                'si_ave': si_ave\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction\n",
    "### Run the first sample to check everything is working and define the Q fit ranges\n",
    "\n",
    "##### First check sample image exists and looks reasonable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files: list[int] = [\n",
    "    # 0, 20, 40, -2\n",
    "    0, -2\n",
    "    ]\n",
    "\"\"\"The index of each sample's test file\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, en in enumerate(sample_energies[i]):\n",
    "        for test_file in test_files:\n",
    "            print(sample, \"|\", datasets[i][j][test_file][0].replace(sample, \"\"))\n",
    "            fig,ax = plt.subplots(len(WAXS_ANGLE_STRINGS),2, figsize=(8,len(WAXS_ANGLE_STRINGS)*1.5), sharex=True, sharey=True)\n",
    "            if len(WAXS_ANGLE_STRINGS) == 1:\n",
    "                ax = [ax]\n",
    "            for k, angle in enumerate(WAXS_ANGLE_STRINGS):\n",
    "                fname = datasets[i][j][test_file][k]\n",
    "                # fig.suptitle(sample)\n",
    "                # Use \n",
    "                img=fabio.open(os.path.join(RAW_DIR, fname)).data\n",
    "                mappable = ax[k][0].imshow(img, vmin=0, vmax=np.percentile(img,99))\n",
    "                ax[k][0].set_title(f\"Raw Data ({angle})\")\n",
    "                plt.colorbar(mappable)\n",
    "                \n",
    "                # Show the masks on the flatfield image\n",
    "                cmap = mplc.LinearSegmentedColormap.from_list(\"Mask\", [(256,0,0,0),(256,0,0,256)], N=2)\n",
    "                ff_im_masked = ax[k][1].imshow(img, interpolation='nearest',\n",
    "                                        vmin = 0, vmax = np.percentile(img, 99))\n",
    "                ax[k][1].imshow(np.rot90(joined_mask,3), cmap=cmap, interpolation='nearest')\n",
    "                ax[k][1].set_title(\"Masked by flatfield and detector\")\n",
    "                plt.colorbar(ff_im_masked)\n",
    "            \n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "        print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate an image of the corrected missing wedge + updated mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    # At each sample, reset the SMI_waxs object\n",
    "    if SMI_waxs is not None and len(SMI_waxs.ai) != 0:\n",
    "        # for ai in SMI_waxs.ai:\n",
    "        #     ai.reset()\n",
    "        SMI_waxs.ai = []\n",
    "    for j, en in enumerate(sample_energies[i]):\n",
    "        for test_file in test_files:\n",
    "            # Setup a figure and open the file\n",
    "            fig,ax = plt.subplots(1,3, figsize=(12,2.5), sharex=True, sharey=True)\n",
    "            fnames = datasets[i][j][test_file]\n",
    "            fname = fnames[0]\n",
    "            \n",
    "            # Collect the metadata\n",
    "            en_idx = fname.find('eV_')\n",
    "            en2 = float(fname[en_idx-7:en_idx])\n",
    "            ai_idx = fname.find(\"_ai\")\n",
    "            ai = float(fname[ai_idx+3:ai_idx+7])\n",
    "            assert(en == en2)\n",
    "            \n",
    "            display(pd.DataFrame([\n",
    "                    (fname, en, ai)\n",
    "                    ], columns=[\"Filename\", \"Energy (eV)\", \"Incident Angle (deg)\"]))\n",
    "            \n",
    "            # Update the geometry/'\n",
    "            tinit = datetime.datetime.now()\n",
    "            SMI_waxs.alphai = ai\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to update geometry: {tfin-tinit}\")\n",
    "            \n",
    "            tinit = datetime.datetime.now()\n",
    "            SMI_waxs.wav = en2wav(en)\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to update wavelength: {tfin-tinit}\")\n",
    "            \n",
    "            # Reset the masks\n",
    "            for mask in SMI_waxs.masks:\n",
    "                mask[:,:] = False\n",
    "            \n",
    "            # Plot the unmodified data\n",
    "            SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "            \n",
    "            SMI_waxs.stitching_data(interp_factor=2, flag_scale=False, timing=True if i==0 and j==0 else False, perpendicular=True)\n",
    "            # mp = ax[0].imshow(SMI_waxs.img_st,\n",
    "            #         extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "            mp = ax[0].imshow(SMI_waxs.img_st,\n",
    "                    extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "                    # vmin=0,\n",
    "                    vmax=np.percentile(SMI_waxs.img_st, 99)\n",
    "            )\n",
    "            plt.colorbar(mp)\n",
    "            ax[0].set_title(\"Stitched Data (Raw)\")\n",
    "            \n",
    "            # Plot the flatfield / masked normalized data\n",
    "            SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "            apply_detector_mask(SMI_waxs)\n",
    "            apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "            apply_detector_mask(SMI_waxs)\n",
    "            \n",
    "            # Show the masks on the flatfield image\n",
    "            a = ax[1]\n",
    "            cmap = mplc.LinearSegmentedColormap.from_list(\"Mask\", [(256,0,0,0),(256,0,0,256)], N=2)\n",
    "            ff_im_masked = a.imshow(SMI_waxs.imgs[0], interpolation='nearest',\n",
    "                                    vmin = 0, vmax = np.percentile(img, 99))\n",
    "            a.imshow(SMI_waxs.masks[0], cmap=cmap, interpolation='nearest')\n",
    "            a.set_title(\"Masked by flatfield and detector\")\n",
    "            plt.colorbar(ff_im_masked)\n",
    "            \n",
    "            \n",
    "            SMI_waxs.stitching_data(interp_factor=1, flag_scale=True, perpendicular=True)\n",
    "            # mp = ax[1].imshow(SMI_waxs.img_st,\n",
    "                    # extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "            mp = ax[1].imshow(SMI_waxs.img_st,\n",
    "                    extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "                    # vmin=np.percentile(SMI_waxs.img_st, 1.0), \n",
    "                    vmax=np.percentile(SMI_waxs.img_st, 99.0)\n",
    "            )\n",
    "            plt.colorbar(mp)\n",
    "            ax[1].set_title(\"Stitched (Norm. + Custom Masked)\")\n",
    "            \n",
    "            # Show the total mask over the image\n",
    "            cmap = mplc.LinearSegmentedColormap.from_list(\"Mask\", [(256,0,0,0),(256,0,0,256)], N=2)\n",
    "            mp = ax[2].imshow(SMI_waxs.img_st,\n",
    "                    extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "                    # vmin=np.percentile(SMI_waxs.img_st, 1.0), \n",
    "                    vmax=np.percentile(SMI_waxs.img_st, 99.0)\n",
    "            )\n",
    "            ax[2].imshow(SMI_waxs.mask_st,\n",
    "                    extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1],],\n",
    "                    cmap=cmap,\n",
    "                    interpolation='nearest'\n",
    "            )\n",
    "            # plt.colorbar(mp)\n",
    "            ax[2].set_title(\"Detector mask + custom mask\")\n",
    "            fig.show()\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate a ROI to check the beam-centre \n",
    "###### Also check that the beamcentre is correct by symmetry (unless sample has anisotropic behavour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the radial angles\n",
    "AZIMUTHAL_WIDTH = 10\n",
    "\"\"\"The +- azimuthal width of the orthogonal range\"\"\"\n",
    "AZIMUTHAL_INPLANE = 10\n",
    "\"\"\"The azimuthal angle for the in-plane scattering\"\"\"\n",
    "AZIMUTHAL_OUTOFPLANE = 80\n",
    "\"\"\"The azimuthal angle for the out-of-plane averaging\"\"\"\n",
    "RADIAL_WIDTH = 35\n",
    "\"\"\"The +- azimuthal width for the radial averaging\"\"\"\n",
    "AZIMUTHAL_RADIAL = 45\n",
    "\"\"\"The azimuthal angle for the radial averaging\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, en in enumerate(sample_energies[i]):\n",
    "        for test_file in test_files:\n",
    "            # Setup a figure and open the file\n",
    "            fnames = datasets[i][j][test_file]\n",
    "            if None in fnames:\n",
    "                print(f\"Skipping {sample} due to missing files.\")\n",
    "                continue\n",
    "            fname = fnames[0]\n",
    "            \n",
    "            # Collect the metadata\n",
    "            en_idx = fname.find('eV_')\n",
    "            en2 = float(fname[en_idx-7:en_idx])\n",
    "            ai_idx = fname.find(\"_ai\")\n",
    "            ai = float(fname[ai_idx+3:ai_idx+7])\n",
    "            assert(en == en2)\n",
    "            display(pd.DataFrame([\n",
    "                    (fname, en, ai)\n",
    "                    ], columns=[\"Filename\", \"Energy (eV)\", \"Incident Angle (deg)\"]))\n",
    "            \n",
    "            # Update the geometry\n",
    "            SMI_waxs.alphai = ai\n",
    "            SMI_waxs.wav = en2wav(en)\n",
    "\n",
    "            # Show the angles on a plot\n",
    "            fig,ax = plt.subplots(1,1, figsize=(8,4), sharex=True, sharey=True)\n",
    "            ax.set_ylim(SMI_waxs.qz[0], SMI_waxs.qz[-1])\n",
    "            ax.set_xlim(SMI_waxs.qp[0], SMI_waxs.qp[-1])\n",
    "\n",
    "            SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "            apply_detector_mask(SMI_waxs)\n",
    "            apply_flatfield(SMI_waxs, flatfield)\n",
    "            energies_idx = flux_energies.index(en) # for normalisation\n",
    "            for img in SMI_waxs.imgs:\n",
    "                img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "            SMI_waxs.stitching_data(interp_factor=5, flag_scale=True, perpendicular=True)\n",
    "            # mp = ax[1].imshow(SMI_waxs.img_st,\n",
    "                    # extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "            mp = ax.imshow(SMI_waxs.img_st,\n",
    "                    extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]], \n",
    "                    vmin=np.percentile(SMI_waxs.img_st, 1.0), \n",
    "                    vmax=np.percentile(SMI_waxs.img_st, 99.0)\n",
    "            )\n",
    "            plt.colorbar(mp)\n",
    "\n",
    "            # Plot the azimuthal and radial angles\n",
    "            colors = ['r', 'orange', 'white'][::-1]\n",
    "            for angle, width in zip([AZIMUTHAL_INPLANE, AZIMUTHAL_OUTOFPLANE, AZIMUTHAL_RADIAL], [AZIMUTHAL_WIDTH, AZIMUTHAL_WIDTH, RADIAL_WIDTH]):\n",
    "                    # Generate a set of x points to plot lines of.\n",
    "                    q_x = np.linspace(0, SMI_waxs.qp[-1], 100)\n",
    "                    # Calculate the x and y gradients for the lines\n",
    "                    m1 = np.tan(np.deg2rad(angle - width)) if angle - width != 90 else np.inf\n",
    "                    m2 = np.tan(np.deg2rad(angle + width)) if angle + width != 90 else np.inf\n",
    "                    # Calculate the x & y values for the lines\n",
    "                    q_x1 = q_x if m1 != np.inf else np.zeros(100)\n",
    "                    q_x2 = q_x if m2 != np.inf else np.zeros(100)\n",
    "                    y1 = m1 * q_x if m1 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                    y2 = m2 * q_x if m2 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                    # Plot the lines\n",
    "                    color = colors.pop()\n",
    "                    ax.plot(q_x1, y1, color=color, linestyle='-', label=f\"{angle} deg\")\n",
    "                    ax.plot(q_x2, y2, color=color, linestyle='-')\n",
    "                    # If gradient is inf, calculate an alternative fill between\n",
    "                    if m2 == np.inf:\n",
    "                            ax.fill_betweenx(y1, q_x1, q_x2, color=color, alpha=0.1)\n",
    "                    else:\n",
    "                            ax.fill_between(q_x, y1, y2, color=color, alpha=0.1)\n",
    "            ax.legend()\n",
    "            plt.show()\n",
    "            # ax.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the azimuthal/radial averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPOINTS_RADIAL_AVE: int = 2000 # Use a number the is consistent with the pixel density?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, en in enumerate(sample_energies[i]):\n",
    "        for test_file in test_files:\n",
    "            # Setup a figure and open the file\n",
    "            fnames = datasets[i][j][test_file]\n",
    "            fname = fnames[0]\n",
    "            \n",
    "            # Collect the metadata\n",
    "            en_idx = fname.find('eV_')\n",
    "            en2 = float(fname[en_idx-7:en_idx])\n",
    "            ai_idx = fname.find(\"_ai\")\n",
    "            ai2 = float(fname[ai_idx+3:ai_idx+7])\n",
    "            assert en==en2\n",
    "            display(pd.DataFrame([\n",
    "                    (fname, en, ai)\n",
    "                    ], columns=[\"Filename\", \"Energy (eV)\", \"Incident Angle (deg)\"]))\n",
    "            \n",
    "            # Update the geometry\n",
    "            tinit = datetime.datetime.now()\n",
    "            SMI_waxs.alphai = ai\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to update geometry: {tfin-tinit}\")\n",
    "            \n",
    "            tinit = datetime.datetime.now()\n",
    "            SMI_waxs.wav = en2wav(en)\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to update wavelength: {tfin-tinit}\")\n",
    "\n",
    "            # Open and stitch the data\n",
    "            tinit = datetime.datetime.now()\n",
    "            SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to open data: {tfin-tinit}\")\n",
    "            \n",
    "            tinit = datetime.datetime.now()\n",
    "            apply_detector_mask(SMI_waxs)\n",
    "            apply_flatfield(SMI_waxs, flatfield)\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to apply masks: {tfin-tinit}\")\n",
    "            \n",
    "            tinit = datetime.datetime.now()\n",
    "            energies_idx = flux_energies.index(en) # for normalisation\n",
    "            for img in SMI_waxs.imgs:\n",
    "                img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to normalise data: {tfin-tinit}\")\n",
    "                \n",
    "            tinit = datetime.datetime.now()\n",
    "            SMI_waxs.stitching_data(interp_factor=2, flag_scale=False, perpendicular=True)\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to stitch data: {tfin-tinit}\")\n",
    "\n",
    "            # Generate radial averages\n",
    "            fig,ax = plt.subplots(2,4, figsize=(12,5), sharex=True, height_ratios=[3,1])\n",
    "            fig.suptitle(f\"{sample}\\n{en:0.2f} eV, {ai:0.3f} deg\")\n",
    "            ax[0][0].set_xscale(\"log\")\n",
    "\n",
    "            tinit = datetime.datetime.now()\n",
    "            # In plane and out of plane\n",
    "            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                    azimuth_range=[90 - (AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH) , 90 - (AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                    npt = NPOINTS_RADIAL_AVE)\n",
    "            q0_IP, I0_IP, I0_IP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                    azimuth_range=[90 - (AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH), 90 - (AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                    npt = NPOINTS_RADIAL_AVE)\n",
    "            q0_OOP, I0_OOP, I0_OOP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "            # Repeat IP and OOP for the consistency checking\n",
    "            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                    azimuth_range=[-90+(AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH), -90+(AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                    npt = NPOINTS_RADIAL_AVE)\n",
    "            q0_IP2, I0_IP2, I0_IP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                    azimuth_range=[-90+(AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH) , -90+(AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                    npt = NPOINTS_RADIAL_AVE)\n",
    "            q0_OOP2, I0_OOP2, I0_OOP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "            # Radial averaging\n",
    "            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                    azimuth_range=[90-(AZIMUTHAL_RADIAL - RADIAL_WIDTH) , -90+(AZIMUTHAL_RADIAL + RADIAL_WIDTH)], \n",
    "                                    npt = NPOINTS_RADIAL_AVE)\n",
    "            q0_R, I0_R, I0_R_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "            # Repeat radial averaging for consistency checking\n",
    "            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                    azimuth_range=[-90+(AZIMUTHAL_RADIAL - RADIAL_WIDTH) , -90+(AZIMUTHAL_RADIAL + RADIAL_WIDTH)], \n",
    "                                    npt = NPOINTS_RADIAL_AVE)\n",
    "            q0_R2, I0_R2, I0_R2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to radial average: {tfin-tinit}\")\n",
    "\n",
    "            tinit = datetime.datetime.now()\n",
    "            # Labels\n",
    "            ax[1][1].set_xlabel(\"$q$ ($Å^{-1}$)\")\n",
    "            ax[0][0].set_ylabel(\"Intensity (a.u.)\")\n",
    "            ax[1][0].set_ylabel(\"Error (Poisson)\")\n",
    "            ax[0][0].set_title(\"Radial Averaging\")\n",
    "            ax[0][1].set_title(\"In-plane Averaging\")\n",
    "            ax[0][2].set_title(\"Out-of-plane Averaging\")\n",
    "            # Plot the radial averages\n",
    "            ax[0][0].plot(q0_R, I0_R, label=\"Radial\")\n",
    "            ax[0][0].fill_between(q0_R, I0_R - I0_R_err, I0_R + I0_R_err, alpha=0.5)\n",
    "            ax[0][0].plot(q0_R2, I0_R2, label=\"Radial (180)\")\n",
    "            ax[0][0].fill_between(q0_R2, I0_R2 - I0_R2_err, I0_R2 + I0_R2_err, alpha=0.5)\n",
    "            # Plot the in-plane and out-of-plane averages\n",
    "            ax[0][1].plot(q0_IP, I0_IP, label=\"In-plane\")\n",
    "            ax[0][1].fill_between(q0_IP, I0_IP - I0_IP_err, I0_IP + I0_IP_err, alpha=0.5)\n",
    "            ax[0][1].plot(q0_IP2, I0_IP2, label=\"In-plane (180)\")\n",
    "            ax[0][1].fill_between(q0_IP2, I0_IP2 - I0_IP2_err, I0_IP2 + I0_IP2_err, alpha=0.5)\n",
    "            ax[0][2].plot(q0_OOP, I0_OOP, label=\"Out-of-plane\")\n",
    "            ax[0][2].fill_between(q0_OOP, I0_OOP - I0_OOP_err, I0_OOP + I0_OOP_err, alpha=0.5)\n",
    "            ax[0][2].plot(q0_OOP2, I0_OOP2, label=\"Out-of-plane (180)\")\n",
    "            ax[0][2].fill_between(q0_OOP2, I0_OOP2 - I0_OOP2_err, I0_OOP2 + I0_OOP2_err, alpha=0.5)\n",
    "            # Overlap the in-plane and out-of-plane averages to check for consistency\n",
    "            ax[0][3].plot(q0_IP, I0_IP, label=\"In-plane\")\n",
    "            ax[0][3].fill_between(q0_IP, I0_IP - I0_IP_err, I0_IP + I0_IP_err, alpha=0.5)\n",
    "            ax[0][3].plot(q0_OOP, I0_OOP, label=\"Out-of-plane\")\n",
    "            ax[0][3].fill_between(q0_OOP, I0_OOP - I0_OOP_err, I0_OOP + I0_OOP_err, alpha=0.5)\n",
    "            ax[0][3].plot(q0_IP2, I0_IP2, label=\"In-plane (180)\")\n",
    "            ax[0][3].fill_between(q0_IP2, I0_IP2 - I0_IP2_err, I0_IP2 + I0_IP2_err, alpha=0.5)\n",
    "            ax[0][3].plot(q0_OOP2, I0_OOP2, label=\"Out-of-plane (180)\")\n",
    "            ax[0][3].fill_between(q0_OOP2, I0_OOP2 - I0_OOP2_err, I0_OOP2 + I0_OOP2_err, alpha=0.5)\n",
    "\n",
    "            # Plot the errors\n",
    "            ax[1][0].plot(q0_R, I0_R_err, label=\"Radial\")\n",
    "            ax[1][0].plot(q0_R2, I0_R2_err, label=\"Radial (180)\")\n",
    "            ax[1][1].plot(q0_IP, I0_IP_err, label=\"In-plane\")\n",
    "            ax[1][1].plot(q0_IP2, I0_IP2_err, label=\"In-plane (180)\")\n",
    "            ax[1][2].plot(q0_OOP, I0_OOP_err, label=\"Out-of-plane\")\n",
    "            ax[1][2].plot(q0_OOP2, I0_OOP2_err, label=\"Out-of-plane (180)\")\n",
    "            ax[1][3].plot(q0_IP, I0_IP_err, label=\"In-plane\")\n",
    "            ax[1][3].plot(q0_OOP, I0_OOP_err, label=\"Out-of-plane\")\n",
    "            ax[1][3].plot(q0_IP2, I0_IP2_err, label=\"In-plane (180)\")\n",
    "            ax[1][3].plot(q0_OOP2, I0_OOP2_err, label=\"Out-of-plane (180)\")\n",
    "\n",
    "            for a in ax[0]:\n",
    "                a.legend()\n",
    "            for a in ax.flatten():\n",
    "                a.set_yscale(\"log\")\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            tfin = datetime.datetime.now()\n",
    "            print(f\"Time to plot: {tfin-tinit}\")\n",
    "            # plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If happy with line profiles above, then generate the line profiles for test_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDE: bool = False\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(RESULT_DIR):\n",
    "    os.mkdir(RESULT_DIR)\n",
    "with tqdm.notebook.tqdm(total=len(samples), position=2, leave=True, desc=\"Samples\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        with tqdm.notebook.tqdm(total=len(sample_energies[i]), position=1, leave=False, desc=\"Energies\") as pbar1:\n",
    "            for j, en in enumerate(sample_energies[i]):\n",
    "                # For each file in the sample\n",
    "                with tqdm.notebook.tqdm(total=len(test_files), position=0, leave=False, desc=\"Files\") as pbar2:\n",
    "                    for test_file in test_files:\n",
    "                        ## Create the results directory for the sample\n",
    "                        sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "                        if not os.path.isdir(sample_dir):\n",
    "                            os.mkdir(sample_dir)\n",
    "                            \n",
    "                        ## Create the directories for the images and line profiles\n",
    "                        giwaxs_img_dir = os.path.join(sample_dir, \"giwaxs_flatfielded_images\")\n",
    "                        if not os.path.isdir(giwaxs_img_dir):\n",
    "                            os.mkdir(giwaxs_img_dir)\n",
    "                        line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "                        if not os.path.isdir(line_profiles_dir):\n",
    "                            os.mkdir(line_profiles_dir)\n",
    "                    \n",
    "                    \n",
    "                        for k, fnames in enumerate(datasets[i][j][test_file:test_file+1]):\n",
    "                            fname = fnames[0]\n",
    "                            # Collect the metadata\n",
    "                            en_idx = fname.find('eV_')\n",
    "                            en2 = float(fname[en_idx-7:en_idx])\n",
    "                            ai_idx = fname.find(\"_ai\")\n",
    "                            ai = float(fname[ai_idx+3:ai_idx+7])\n",
    "                            assert en==en2\n",
    "                            \n",
    "                            # Generate paths for the output files\n",
    "                            path_det_img = os.path.join(giwaxs_img_dir, f\"{sample}_giwaxs_{en:0.2f}eV_{ai:0.3f}deg.png\")\n",
    "                            path_OOP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_OOP.txt\")\n",
    "                            path_IP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_IP.txt\")\n",
    "                            path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_R.txt\")\n",
    "                            path_det_line_profiles_img = os.path.join(sample_dir, f\"{sample}_line_profile_angles_test.png\")\n",
    "                            \n",
    "                            # Do not override the files if they \"all\" exist already. Override partial file sets though.\n",
    "                            if (not OVERRIDE \n",
    "                                and os.path.isfile(path_det_img) \n",
    "                                and os.path.isfile(path_OOP) \n",
    "                                and os.path.isfile(path_IP) \n",
    "                                and os.path.isfile(path_R)\n",
    "                                and (k != 0 or os.path.isfile(path_det_line_profiles_img))):\n",
    "                                pbar.total -= 1\n",
    "                                continue\n",
    "                            \n",
    "                            # Update the geometry\n",
    "                            SMI_waxs.alphai = ai\n",
    "                            SMI_waxs.wav = en2wav(en)\n",
    "                            \n",
    "                            # Plot the flatfield / masked normalized data\n",
    "                            SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "                            apply_detector_mask(SMI_waxs)\n",
    "                            apply_flatfield(SMI_waxs, flatfield)\n",
    "                            energies_idx = flux_energies.index(en) # for normalisation\n",
    "                            for img in SMI_waxs.imgs:\n",
    "                                img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "                            SMI_waxs.stitching_data(interp_factor=2, flag_scale=False, perpendicular=True)\n",
    "                            \n",
    "                            # Setup a figure and open the file\n",
    "                            fig,ax = plt.subplots(1,1, figsize=(7, 10), dpi=300)\n",
    "                            mp = ax.imshow(SMI_waxs.img_st,\n",
    "                                    extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]],\n",
    "                                    vmin=np.percentile(SMI_waxs.img_st, 1.0), \n",
    "                                    vmax=np.percentile(SMI_waxs.img_st, 99.0) # Avoid extremities\n",
    "                            )\n",
    "                            plt.colorbar(mp)\n",
    "                            ax.set_title(f\"{sample}\\n{en} eV - {ai} deg\")\n",
    "                            # Don't save the image, as we use a different display format later.\n",
    "                            # fig.savefig(path_det_img)\n",
    "                            \n",
    "                            if k==0:\n",
    "                                # Plot the azimuthal and radial angles\n",
    "                                colors = ['r', 'orange', 'white'][::-1]\n",
    "                                for angle, width in zip([AZIMUTHAL_INPLANE, AZIMUTHAL_OUTOFPLANE, AZIMUTHAL_RADIAL], [AZIMUTHAL_WIDTH, AZIMUTHAL_WIDTH, RADIAL_WIDTH]):\n",
    "                                    # Generate a set of x points to plot lines of.\n",
    "                                    q_x = np.linspace(0, SMI_waxs.qp[-1], 100)\n",
    "                                    # Calculate the x and y gradients for the lines\n",
    "                                    m1 = np.tan(np.deg2rad(angle - width)) if angle - width != 90 else np.inf\n",
    "                                    m2 = np.tan(np.deg2rad(angle + width)) if angle + width != 90 else np.inf\n",
    "                                    # Calculate the x & y values for the lines\n",
    "                                    q_x1 = q_x if m1 != np.inf else np.zeros(100)\n",
    "                                    q_x2 = q_x if m2 != np.inf else np.zeros(100)\n",
    "                                    y1 = m1 * q_x if m1 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                                    y2 = m2 * q_x if m2 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                                    # Plot the lines\n",
    "                                    color = colors.pop()\n",
    "                                    ax.plot(q_x1, y1, color=color, linestyle='-', label=f\"{angle} deg\")\n",
    "                                    ax.plot(q_x2, y2, color=color, linestyle='-')\n",
    "                                    # If gradient is inf, calculate an alternative fill between\n",
    "                                    if m2 == np.inf:\n",
    "                                            ax.fill_betweenx(y1, q_x1, q_x2, color=color, alpha=0.1)\n",
    "                                    else:\n",
    "                                            ax.fill_between(q_x, y1, y2, color=color, alpha=0.1)\n",
    "                                ax.set_xlim(*SMI_waxs.qp)\n",
    "                                ax.set_ylim(*SMI_waxs.qz)\n",
    "                                ax.legend()\n",
    "                                fig.savefig(path_det_line_profiles_img, dpi=300)\n",
    "                            plt.close() # Save memory\n",
    "                            \n",
    "                            # Perform the radial/azimuthal averaging\n",
    "                            # In plane and out of plane\n",
    "                            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                    azimuth_range=[90 - (AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH) , 90 - (AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                    npt = NPOINTS_RADIAL_AVE)\n",
    "                            q0_IP, I0_IP, I0_IP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                    azimuth_range=[90 - (AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH), 90 - (AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                    npt = NPOINTS_RADIAL_AVE)\n",
    "                            q0_OOP, I0_OOP, I0_OOP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                            # Repeat IP and OOP for the consistency checking\n",
    "                            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                    azimuth_range=[-90+(AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH), -90+(AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                    npt = NPOINTS_RADIAL_AVE)\n",
    "                            q0_IP2, I0_IP2, I0_IP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                    azimuth_range=[-90+(AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH) , -90+(AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                    npt = NPOINTS_RADIAL_AVE)\n",
    "                            q0_OOP2, I0_OOP2, I0_OOP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                            # Radial averaging\n",
    "                            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                    azimuth_range=[90-(AZIMUTHAL_RADIAL - RADIAL_WIDTH), 90-(AZIMUTHAL_RADIAL + RADIAL_WIDTH)], \n",
    "                                                    npt = NPOINTS_RADIAL_AVE)\n",
    "                            q0_R, I0_R, I0_R_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                            # Repeat radial averaging for consistency checking\n",
    "                            SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                    azimuth_range=[-90+(AZIMUTHAL_RADIAL - RADIAL_WIDTH), -90+(AZIMUTHAL_RADIAL + RADIAL_WIDTH)], \n",
    "                                                    npt = NPOINTS_RADIAL_AVE)\n",
    "                            q0_R2, I0_R2, I0_R2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "                            \n",
    "                            # Save the line profiles \n",
    "                            header = (\"Main Data\\t\\tMirror-Y axis Data\\t\\n\" \n",
    "                                    + \"q (Å^-1)\\tI (a.u.)\\tI_err (a.u.)\\tq (Å^-1)\\tI (a.u.)\\tI_err (a.u.)\\n\")\n",
    "                            delim = \"\\t\"\n",
    "                            kwargs = {\"header\": header, \"delimiter\": delim}\n",
    "                            np.savetxt(path_OOP, np.array([q0_OOP, I0_OOP, I0_OOP_err, q0_OOP2, I0_OOP2, I0_OOP2_err]).T, **kwargs)\n",
    "                            np.savetxt(path_IP, np.array([q0_IP, I0_IP, I0_IP_err, q0_IP2, I0_IP2, I0_IP2_err]).T, **kwargs)\n",
    "                            np.savetxt(path_R, np.array([q0_R, I0_R, I0_R_err, q0_R2, I0_R2, I0_R2_err]).T, **kwargs)\n",
    "                            \n",
    "                            # Update the progress bars\n",
    "                            pbar2.update(1)\n",
    "                pbar1.update(1)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using example data, define regions for fluorescence collection in the radial scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLUOR_RANGE: tuple[float, float] = (0.8, 0.92)\n",
    "\"\"\"Q-range for the fluorescence signal summation; define in a flat region away from any peaks/features!\"\"\"\n",
    "\n",
    "# Plot the example data, and plot the fluorescence signal\n",
    "fig: plt.Figure\n",
    "ax: list[plt.Axes]\n",
    "fig,ax = plt.subplots(1,2, figsize=(12,5))\n",
    "for i, sample in enumerate(samples):\n",
    "    sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "    line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "    for j, en in enumerate(sample_energies[i]):\n",
    "        test_dataset = [datasets[i][j][test_file] for test_file in test_files]\n",
    "        # Gather the fluor data\n",
    "        fluor_data = np.zeros((len(test_dataset), 3)) # en, I, I_err\n",
    "        for k, fnames in enumerate(test_dataset):\n",
    "            ## Create the results directory for the sample\n",
    "            fname = fnames[0]\n",
    "            # Collect the metadata\n",
    "            en_idx = fname.find('eV_')\n",
    "            en2 = float(fname[en_idx-7:en_idx])\n",
    "            ai_idx = fname.find(\"_ai\")\n",
    "            ai = float(fname[ai_idx+3:ai_idx+7])\n",
    "            assert en == en2\n",
    "            # Generate paths for the output files\n",
    "            path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_R.txt\")\n",
    "\n",
    "            # Load the data\n",
    "            data = np.loadtxt(path_R, skiprows=3, delimiter=\"\\t\")\n",
    "            q = data[:,0]\n",
    "            I = data[:,1]\n",
    "            I_err = data[:,2]\n",
    "            \n",
    "            # Find the fluorescence signal\n",
    "            mask = (q > FLUOR_RANGE[0]) & (q < FLUOR_RANGE[1])\n",
    "            fluor_data[k, 0] = ai\n",
    "            fluor_data[k, 1] = np.trapezoid(I[mask], q[mask])\n",
    "            fluor_data[k, 2] = np.trapezoid(I_err[mask], q[mask])\n",
    "            \n",
    "            # Plot the data\n",
    "            ax[0].plot(q, I, label=f\"{en} eV, {ai} deg\")\n",
    "            \n",
    "        ax[1].plot(fluor_data[:,0], fluor_data[:,1], label=f\"{sample} {en}eV\")\n",
    "        ax[1].fill_between(fluor_data[:,0], fluor_data[:,1] - fluor_data[:,2], fluor_data[:,1] + fluor_data[:,2], alpha=0.5)\n",
    "    \n",
    "# ax[0].axvspan(*FLUOR_RANGE, color=\"gray\", alpha=0.5)\n",
    "ax[0].set_xlabel(\"q (Å$^{-1}$)\")\n",
    "ax[0].set_ylabel(\"Intensity (a.u.)\")\n",
    "ax[0].set_title(\"Radial Averaging\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "# ax[0].legend()\n",
    "ax[1].set_xlabel(\"Angle of Incidence (deg)\")\n",
    "ax[1].set_ylabel(\"Fluorescence Signal (a.u.)\")\n",
    "ax[1].legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the reduction for all files, and generate videos if FFMPEG is installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate the video images function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_A: float = 0.2\n",
    "MAX_A: float = 10\n",
    "\n",
    "def generate_energy_video_figure(geom: SMI_beamline.SMI_geometry, \n",
    "                              dataset_minimum:float, \n",
    "                              dataset_maximum: float, \n",
    "                              en: float) -> plt.Figure:\n",
    "    SMI_waxs = geom\n",
    "    if len(SMI_waxs.imgs) == 3:\n",
    "        # Works for single detector plots\n",
    "        figsize = (4,5) \n",
    "    else:\n",
    "        # Works for 0, 20  deg detector angles... generic?\n",
    "        qp = SMI_waxs.qp\n",
    "        qz = SMI_waxs.qz\n",
    "        qp_diff = qp[0] - qp[1]\n",
    "        qz_diff = qz[0] - qz[1]\n",
    "        qmax = np.max([qp_diff, qz_diff])\n",
    "        scale = 4\n",
    "        figsize = (qp_diff/qmax * scale * 1.02, qz_diff/qmax * scale)\n",
    "    \n",
    "    fig = plt.figure(figsize = figsize, dpi=150)\n",
    "    gs = fig.add_gridspec(2,2, width_ratios=[1,0.05], height_ratios=[1, 0.05])\n",
    "    ax = fig.add_subplot(gs[0,0])\n",
    "    ax_cmap = fig.add_subplot(gs[0,1])\n",
    "    ax_ai = fig.add_subplot(gs[1,0])\n",
    "\n",
    "    cmap = plt.get_cmap('terrain')\n",
    "    norm = mplc.LogNorm(vmin=dataset_minimum, vmax=dataset_maximum, clip=True)\n",
    "    mp = ax.matshow(SMI_waxs.img_st,\n",
    "            extent=[SMI_waxs.qp[0], SMI_waxs.qp[-1], SMI_waxs.qz[0], SMI_waxs.qz[-1]],\n",
    "            cmap = cmap,\n",
    "            norm=norm\n",
    "    )\n",
    "    colorbar = fig.colorbar(mp, cax=ax_cmap, orientation='vertical', location=\"right\", fraction = 0.05)\n",
    "    colorbar.ax.set_ylabel(\"Intensity (a.u.)\", fontsize=4)\n",
    "    colorbar.ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax2 = ax_ai\n",
    "    ax2.yaxis.set_ticks([])\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.set_xscale(\"linear\")\n",
    "    ax2.set_xlim(MIN_A, MAX_A)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=4)\n",
    "    ax2.plot([en, en], [0, 1], 'r')\n",
    "    ax2.set_xlabel(f\"Beam Energy     {en:0.3f} eV\", loc=\"left\", fontsize=4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "if \"dataset_minimum\" in locals():\n",
    "    plt.ion()\n",
    "    matplotlib.interactive(True)\n",
    "    \n",
    "    SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "    apply_detector_mask(SMI_waxs)\n",
    "    apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "    energies_idx = flux_energies.index(en) # for normalisation\n",
    "    for img in SMI_waxs.imgs:\n",
    "        img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "    SMI_waxs.stitching_data(interp_factor=3, flag_scale=False, perpendicular=True)\n",
    "    \n",
    "    fig = generate_energy_video_figure(SMI_waxs, 1, dataset_maximum, en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reduce the lineprofiles and generate fluorescence profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDE: bool = False\n",
    "\"\"\"Whether to override the analysis reduction or not.\"\"\"\n",
    "STITCH_MINMAX: bool = False\n",
    "\"\"\"Whether to stitch the data to find the minimum and maximum values for the dataset, \n",
    "otherwise just use raw values from 2D detector array using fabio.\"\"\"\n",
    "\n",
    "# Check the timing performance\n",
    "timing:bool = False\n",
    "tcheck = datetime.datetime.now()\n",
    "\n",
    "# Run the script\n",
    "plt.ioff()\n",
    "matplotlib.interactive(False)\n",
    "with tqdm.notebook.tqdm(total=len(samples), position=0, leave=True, desc=\"All Samples\") as pbar:\n",
    "    for i, sample in enumerate(samples):\n",
    "        ## Create the results directory for the sample\n",
    "        sample_dir = os.path.join(RESULT_DIR, sample)\n",
    "        if not os.path.isdir(sample_dir):\n",
    "            os.mkdir(sample_dir)\n",
    "            \n",
    "        ## Create the directories for the images and line profiles\n",
    "        giwaxs_img_dir = os.path.join(sample_dir, \"giwaxs_flatfielded_images\")\n",
    "        if not os.path.isdir(giwaxs_img_dir):\n",
    "            os.mkdir(giwaxs_img_dir)\n",
    "        line_profiles_dir = os.path.join(sample_dir, \"line_profiles\")\n",
    "        if not os.path.isdir(line_profiles_dir):\n",
    "            os.mkdir(line_profiles_dir)\n",
    "            \n",
    "        with tqdm.notebook.tqdm(total=len(sample_energies[i]), position=1, leave=False, desc=\"Energies\") as pbar1:\n",
    "            for j, en in enumerate(sample_energies[i]):\n",
    "                \n",
    "                # Check if dataset is already processed\n",
    "                processed = True\n",
    "                if OVERRIDE:\n",
    "                    processed = False\n",
    "                else:\n",
    "                    for k, fnames in enumerate(datasets[i][j]):\n",
    "                        fname = fnames[0]\n",
    "                        # Collect the metadata\n",
    "                        en_idx = fname.find('eV_')\n",
    "                        en2 = float(fname[en_idx-7:en_idx])\n",
    "                        ai_idx = fname.find(\"_ai\")\n",
    "                        ai = float(fname[ai_idx+3:ai_idx+7])\n",
    "                        assert en == en2\n",
    "                        \n",
    "                        # Generate paths for the output files\n",
    "                        path_det_img = os.path.join(giwaxs_img_dir, f\"{sample}_giwaxs_{en:0.2f}eV_{ai:0.3f}deg.png\")\n",
    "                        path_OOP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_OOP.txt\")\n",
    "                        path_IP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_IP.txt\")\n",
    "                        path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_R.txt\")\n",
    "                        path_det_line_profiles_img = os.path.join(sample_dir, f\"{sample}_{en}_line_profile_angles.png\")\n",
    "                        \n",
    "                        if not all([os.path.isfile(path) for path in [path_det_img, path_OOP, path_IP, path_R, path_det_line_profiles_img]]):\n",
    "                            processed = False\n",
    "                            break\n",
    "                        \n",
    "                if processed:\n",
    "                    print(f\"Skipping processed dataset for sample {sample} at {en} energy.\")\n",
    "                    pbar1.total -= 1\n",
    "                    pbar1.update(0)\n",
    "                    continue\n",
    "                \n",
    "                dataset_maximum = None\n",
    "                dataset_minimum = None\n",
    "                with tqdm.notebook.tqdm(total=len(datasets[i][j]), position=2, leave=False, desc=\"Finding dataset maximum / minimum\") as pbar2:\n",
    "                    for k, fnames in enumerate(datasets[i][j]):\n",
    "                        fname = fnames[0]\n",
    "                        # Collect the metadata\n",
    "                        en_idx = fname.find('eV_')\n",
    "                        en2 = float(fname[en_idx-7:en_idx])\n",
    "                        ai_idx = fname.find(\"_ai\")\n",
    "                        ai = float(fname[ai_idx+3:ai_idx+7])\n",
    "                        assert en2==en\n",
    "                        \n",
    "                        if STITCH_MINMAX:\n",
    "                        \n",
    "                            # Update the geometry\n",
    "                            if timing:\n",
    "                                tcheck = datetime.datetime.now()\n",
    "                                SMI_waxs.alphai = ai\n",
    "                                tupdate = datetime.datetime.now()\n",
    "                                print(f\"Time to update geometry: {tupdate - tcheck}\")\n",
    "                                        \n",
    "                                \n",
    "                                tcheck = datetime.datetime.now()\n",
    "                                SMI_waxs.wav = en2wav(en)\n",
    "                                tupdate = datetime.datetime.now()\n",
    "                                print(f\"Time to update wavelength: {tupdate - tcheck}\")\n",
    "                                \n",
    "                                # Flatfield / masked normalized data\n",
    "                                tcheck = datetime.datetime.now()\n",
    "                                SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "                                tupdate = datetime.datetime.now()\n",
    "                                print(f\"Time to open data: {tupdate - tcheck}\")\n",
    "                                \n",
    "                                tcheck = datetime.datetime.now()\n",
    "                                apply_detector_mask(SMI_waxs)\n",
    "                                apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "                                tupdate = datetime.datetime.now()\n",
    "                                print(f\"Time to apply flatfield and mask: {tupdate - tcheck}\")\n",
    "                                \n",
    "                                energies_idx = global_energies.tolist().index(en) # for normalisation\n",
    "                                for img in SMI_waxs.imgs:\n",
    "                                    img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "                                    \n",
    "                                tcheck = datetime.datetime.now()\n",
    "                                SMI_waxs.stitching_data(interp_factor=3, flag_scale=False, timing=True, perpendicular=True)\n",
    "                                tupdate = datetime.datetime.now()\n",
    "                                print(f\"Time to stitch data: {tupdate - tcheck}\")\n",
    "                            \n",
    "                            else:\n",
    "                                # Unnecessary to set the correct geometry and wavelength for each image when finding min/max\n",
    "                                # SMI_waxs.alphai = ai\n",
    "                                # SMI_waxs.wav = en2wav(en)\n",
    "                                SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "                                apply_detector_mask(SMI_waxs)\n",
    "                                apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "                                \n",
    "                                energies_idx = global_energies.tolist().index(en) # for normalisation\n",
    "                                for img in SMI_waxs.imgs:\n",
    "                                    img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "                                SMI_waxs.stitching_data(interp_factor=3, flag_scale=False, perpendicular=True)\n",
    "                                \n",
    "                            # Collect the maximum and minimum values\n",
    "                            if dataset_maximum is None:\n",
    "                                dataset_maximum = np.max(SMI_waxs.img_st)\n",
    "                            else:\n",
    "                                dataset_maximum = np.max([dataset_maximum, np.max(SMI_waxs.img_st)])\n",
    "                                \n",
    "                            if dataset_minimum is None:\n",
    "                                dataset_minimum = np.min(SMI_waxs.img_st[SMI_waxs.img_st > 0])\n",
    "                            else:\n",
    "                                min_val = np.min(SMI_waxs.img_st[SMI_waxs.img_st > 0])\n",
    "                                dataset_minimum = np.min([dataset_minimum, min_val])\n",
    "                        else:\n",
    "                            # Open the file using pyFAI\n",
    "                            for file in fnames:\n",
    "                                raw_img = fabio.open(os.path.join(RAW_DIR, file)).data\n",
    "                                if dataset_maximum is None:\n",
    "                                    dataset_maximum = np.max(raw_img)\n",
    "                                else:\n",
    "                                    dataset_maximum = np.max([dataset_maximum, np.max(raw_img)])\n",
    "                                if dataset_minimum is None:\n",
    "                                    dataset_minimum = np.min(raw_img[raw_img > 0])\n",
    "                                else:\n",
    "                                    min_val = np.min(raw_img[raw_img > 0])\n",
    "                                    dataset_minimum = np.min([dataset_minimum, min_val])\n",
    "                        pbar2.update(1)\n",
    "                \n",
    "                cmap = plt.get_cmap('viridis')\n",
    "                norm = mplc.LogNorm(vmin=dataset_minimum, vmax=dataset_maximum)\n",
    "                \n",
    "                \n",
    "                # Gather the fluor data\n",
    "                fluor_data = np.zeros((len(datasets[i][j]), 3))\n",
    "                \n",
    "                # Create paths for fluor figure and data\n",
    "                path_flour_fig = os.path.join(sample_dir, f\"{sample}_{ai}_fluorescence.png\")\n",
    "                path_flour_data = os.path.join(sample_dir, f\"{sample}_{ai}_fluorescence_data.txt\")\n",
    "                \n",
    "                with tqdm.notebook.tqdm(total=len(datasets[i][j]), position=1, leave=False, desc=\"Sample dataset\") as pbar2:\n",
    "                    # For each file in the sample\n",
    "                    for k, fnames in enumerate(datasets[i][j]):\n",
    "                        fname = fnames[0]\n",
    "                        # Collect the metadata\n",
    "                        en_idx = fname.find('eV_')\n",
    "                        en2 = float(fname[en_idx-7:en_idx])\n",
    "                        ai_idx = fname.find(\"_ai\")\n",
    "                        ai = float(fname[ai_idx+3:ai_idx+7])\n",
    "                        assert en2 == en\n",
    "                        \n",
    "                        # Generate paths for the output files\n",
    "                        path_det_img = os.path.join(giwaxs_img_dir, f\"{sample}_giwaxs_{en:0.2f}eV_{ai:0.3f}deg.png\")\n",
    "                        path_OOP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_OOP.txt\")\n",
    "                        path_IP = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_IP.txt\")\n",
    "                        path_R = os.path.join(line_profiles_dir, f\"{sample}_line_profile_{en:0.2f}eV_{ai:0.3f}deg_R.txt\")\n",
    "                        path_det_line_profiles_img = os.path.join(sample_dir, f\"{sample}_{en}_line_profile_angles.png\")\n",
    "                        \n",
    "                        # Do not override the files if they \"all\" exist already. Override partial file sets though.\n",
    "                        if (not OVERRIDE \n",
    "                            and os.path.isfile(path_det_img) \n",
    "                            and os.path.isfile(path_OOP) \n",
    "                            and os.path.isfile(path_IP) \n",
    "                            and os.path.isfile(path_R)\n",
    "                            and os.path.isfile(path_flour_fig)\n",
    "                            and os.path.isfile(path_flour_data)\n",
    "                            and (k != 0 or os.path.isfile(path_det_line_profiles_img))):\n",
    "                            pbar2.total -= 1 # Reduce the total count\n",
    "                            continue\n",
    "                        \n",
    "                        # Update the geometry\n",
    "                        SMI_waxs.alphai = ai\n",
    "                        SMI_waxs.wav = en2wav(en)\n",
    "                        \n",
    "                        # flatfield / masked normalized data\n",
    "                        SMI_waxs.open_data(RAW_DIR, fnames)\n",
    "                        apply_detector_mask(SMI_waxs)\n",
    "                        apply_flatfield(SMI_waxs, flatfield, flat_percentile=FLATFIELD_PERCENTILE, img_percentile=SAMPLE_PERCENTILE)\n",
    "                        energies_idx = flux_energies.index(en) # for normalisation\n",
    "                        for img in SMI_waxs.imgs:\n",
    "                            img[:] = (img[:] / FLUX_NORM[energies_idx]).astype(np.int32)\n",
    "                        SMI_waxs.stitching_data(interp_factor=2, flag_scale=True, timing= True, perpendicular=True)\n",
    "                        \n",
    "                        # Setup a figure and open the file\n",
    "                        fig = generate_energy_video_figure(SMI_waxs, 1, dataset_maximum, en)\n",
    "                        fig.savefig(path_det_img)\n",
    "                        ax = fig.get_axes()[0]\n",
    "                        \n",
    "                        if k==0:\n",
    "                            # Plot the azimuthal and radial angles\n",
    "                            colors = ['r', 'orange', 'white'][::-1]\n",
    "                            for angle, width in zip([AZIMUTHAL_INPLANE, AZIMUTHAL_OUTOFPLANE, AZIMUTHAL_RADIAL], [AZIMUTHAL_WIDTH, AZIMUTHAL_WIDTH, RADIAL_WIDTH]):\n",
    "                                # Generate a set of x points to plot lines of.\n",
    "                                q_x = np.linspace(0, SMI_waxs.qp[-1], 100)\n",
    "                                # Calculate the x and y gradients for the lines\n",
    "                                m1 = np.tan(np.deg2rad(angle - width)) if angle - width != 90 else np.inf\n",
    "                                m2 = np.tan(np.deg2rad(angle + width)) if angle + width != 90 else np.inf\n",
    "                                # Calculate the x & y values for the lines\n",
    "                                q_x1 = q_x if m1 != np.inf else np.zeros(100)\n",
    "                                q_x2 = q_x if m2 != np.inf else np.zeros(100)\n",
    "                                y1 = m1 * q_x if m1 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                                y2 = m2 * q_x if m2 != np.inf else np.linspace(0, SMI_waxs.qz[-1], 100)\n",
    "                                # Plot the lines\n",
    "                                color = colors.pop()\n",
    "                                ax.plot(q_x1, y1, color=color, linestyle='-', label=f\"{angle} deg\")\n",
    "                                ax.plot(q_x2, y2, color=color, linestyle='-')\n",
    "                                # If gradient is inf, calculate an alternative fill between\n",
    "                                if m2 == np.inf:\n",
    "                                        ax.fill_betweenx(y1, q_x1, q_x2, color=color, alpha=0.1)\n",
    "                                else:\n",
    "                                        ax.fill_between(q_x, y1, y2, color=color, alpha=0.1)\n",
    "                            ax.set_xlim(*SMI_waxs.qp)\n",
    "                            ax.set_ylim(*SMI_waxs.qz)\n",
    "                            ax.legend()\n",
    "                            fig.savefig(path_det_line_profiles_img, dpi=300)\n",
    "                        plt.close(fig) # Save memory\n",
    "                        \n",
    "                        # Perform the radial/azimuthal averaging\n",
    "                        # In plane and out of plane\n",
    "                        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                azimuth_range=[90 - (AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH) , 90 - (AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                npt = NPOINTS_RADIAL_AVE)\n",
    "                        q0_IP, I0_IP, I0_IP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                azimuth_range=[90 - (AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH), 90 - (AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                npt = NPOINTS_RADIAL_AVE)\n",
    "                        q0_OOP, I0_OOP, I0_OOP_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                        # Repeat IP and OOP for the consistency checking\n",
    "                        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                azimuth_range=[-90+(AZIMUTHAL_INPLANE - AZIMUTHAL_WIDTH), -90+(AZIMUTHAL_INPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                npt = NPOINTS_RADIAL_AVE)\n",
    "                        q0_IP2, I0_IP2, I0_IP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                azimuth_range=[-90+(AZIMUTHAL_OUTOFPLANE - AZIMUTHAL_WIDTH) , -90+(AZIMUTHAL_OUTOFPLANE + AZIMUTHAL_WIDTH)], \n",
    "                                                npt = NPOINTS_RADIAL_AVE)\n",
    "                        q0_OOP2, I0_OOP2, I0_OOP2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                        # Radial averaging\n",
    "                        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                azimuth_range=[90-(AZIMUTHAL_RADIAL - RADIAL_WIDTH), 90-(AZIMUTHAL_RADIAL + RADIAL_WIDTH)], \n",
    "                                                npt = NPOINTS_RADIAL_AVE)\n",
    "                        q0_R, I0_R, I0_R_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "\n",
    "                        # Repeat radial averaging for consistency checking\n",
    "                        SMI_waxs.radial_averaging(radial_range = [0, 8], \n",
    "                                                azimuth_range=[-90+(AZIMUTHAL_RADIAL - RADIAL_WIDTH), -90+(AZIMUTHAL_RADIAL + RADIAL_WIDTH)], \n",
    "                                                npt = NPOINTS_RADIAL_AVE)\n",
    "                        q0_R2, I0_R2, I0_R2_err = SMI_waxs.q_rad, SMI_waxs.I_rad, SMI_waxs.I_rad_err\n",
    "                        \n",
    "                        # Save the line profiles \n",
    "                        header = (\"Main Data\\t\\tMirror-Y axis Data\\t\\n\" \n",
    "                                + \"q (Å^-1)\\tI (a.u.)\\tI_err (a.u.)\\tq (Å^-1)\\tI (a.u.)\\tI_err (a.u.)\\n\")\n",
    "                        delim = \"\\t\"\n",
    "                        kwargs = {\"header\": header, \"delimiter\": delim}\n",
    "                        np.savetxt(path_OOP, np.array([q0_OOP, I0_OOP, I0_OOP_err, q0_OOP2, I0_OOP2, I0_OOP2_err]).T, **kwargs)\n",
    "                        np.savetxt(path_IP, np.array([q0_IP, I0_IP, I0_IP_err, q0_IP2, I0_IP2, I0_IP2_err]).T, **kwargs)\n",
    "                        np.savetxt(path_R, np.array([q0_R, I0_R, I0_R_err, q0_R2, I0_R2, I0_R2_err]).T, **kwargs)\n",
    "                        \n",
    "                        # Calculate the Fluorescence data\n",
    "                        mask = (q0_R > FLUOR_RANGE[0]) & (q0_R < FLUOR_RANGE[1])\n",
    "                        fluor_data[k, 0] = en\n",
    "                        fluor_data[k, 1] = np.trapezoid(I0_R[mask], q0_R[mask])\n",
    "                        fluor_data[k, 2] = np.trapezoid(I0_R_err[mask], q0_R[mask])\n",
    "                        \n",
    "                        \n",
    "                        pbar2.update(1)\n",
    "                        plt.close(\"all\")\n",
    "                        # break # Only do the first file for now.\n",
    "                \n",
    "                # Save and plot the Fluroescence data\n",
    "                idx = np.argsort(fluor_data[:,0]) #sort by energy\n",
    "                fluor_data = fluor_data[idx]\n",
    "                if not os.path.isfile(path_flour_data) or OVERRIDE:\n",
    "                    with open(path_flour_data, \"w\") as f:\n",
    "                        f.write(\"Energy (eV)\\tFluorescence (a.u.)\\tFluorescence Error (a.u.)\\n\")\n",
    "                        np.savetxt(f, fluor_data, delimiter=\"\\t\")\n",
    "                \n",
    "                if not os.path.isfile(path_flour_fig) or OVERRIDE:\n",
    "                    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "                    ax.plot(fluor_data[:,0], fluor_data[:,1], label=\"Fluorescence\")\n",
    "                    ax.fill_between(fluor_data[:,0], fluor_data[:,1] - fluor_data[:,2], fluor_data[:,1] + fluor_data[:,2], alpha=0.5)\n",
    "                    ax.set_xlabel(\"Energy (eV)\")\n",
    "                    ax.set_ylabel(\"Fluorescence (a.u.)\")\n",
    "                    ax.set_title(f\"{sample} Fluorescence\")\n",
    "                    fig.tight_layout()\n",
    "                    fig.savefig(path_flour_fig)\n",
    "                    plt.close(fig)\n",
    "                pbar1.update(1)\n",
    "        pbar.update(1)\n",
    "        \n",
    "# Turn back on interactive plotting\n",
    "plt.ion()\n",
    "matplotlib.interactive(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn back on interactive plotting\n",
    "plt.ion()\n",
    "matplotlib.interactive(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
